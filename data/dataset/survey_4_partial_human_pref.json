{

    "Lecture Notes on Neural Information Retrieval": {
        "introduction": {
            "content": "Text Information Retrieval (IR) systems focus on retrieving text documents able to ful- fill the information needs of their users, typically expressed as textual queries. Over the years, this inherently vague description has been formalised and characterised by the specific nature of documents, information needs, and users. At the core of the for- malisation lies the concept of relevance of a document with respect to a query, and how to estimate their relevance. Over the years, many different ranking models have been proposed to estimate the relevance of documents in response to a query. These mod- els depend on the information provided by the queries and the documents, that are exploited to derive “relevance signals”. Many ranking models have been developed over the years, ranging from Boolean models to probabilistic and statistical language models. These “bag of words” models leverage the presence or the number of occur- rences of query terms in the documents to infer their relevance to a query, exploiting hand-crafted functions to combine these occurrences such as BM25. With the rise of the Web and social platforms, more sources of relevance information about documents have been identified. Machine learning methods have been proved effective to deal with this abundance of relevance signals, and their application to rank the documents in order of relevance estimates w.r.t. a query has given birth to many learning-to-rank (LTR) models. Relevance signals are input features in LTR models, and they are often designed by hand, a time-consuming process. Motivated by their breakthroughs in many computer vision and natural language processing tasks, neural networks repre- sent the current state-of-the-art approach in ranking documents w.r.t. query relevance. Neural Information Retrieval focuses on retrieving text documents able to fulfill the information needs of their users exploiting deep neural networks. In neural IR, neural networks are typically used in two different ways: to learn the ranking functions com- bining the relevance signals to produce an ordering of documents, and to learn the abstract representations of documents and queries to capture their relevance informa- tion. In the following, we provide an introduction to the recent approaches in neural IR. Since the research in the field is rapidly evolving, we do not pretend to cover every single aspect of neural IR, but to provide a principled introduction to the main ideas and existing systems in the field. When available, we provide links to relevant and more detailed surveys. Here is a quick overview over what the sections are about. Section 1 provides a short depiction of the different representations for text adopted in IR, from the classical BOW encodings to learning-to-rank features to word embeddings. Section 2 presents the main neural architectures for computing a joint representation of query and document pairs for relevance ranking. Section 3 focuses on the neural architectures specif- ically tailored for learning abstract complex representations of query and documents texts independently. Section 4 overviews the deployment schemes adopted in neural IR systems, together with an overview of the most common dense retrieval indexes supporting exact and approximate nearest neighbour search. Section 5 discusses the current approaches in learned sparse retrieval, dealing with the learning of low di- mensional representations of documents amenable to be stored in an inverted index or similar data structures. Finally, Section 6 draws concluding remarks."
        },
        "Text Representations for Ranking": {
            "content": "According to the Probability Ranking Principle [Robertson 1977], under certain as-\nsumptions, for a given user’s query, documents in a collection should be ranked in\norder of the (decreasing) probability of relevance w.r.t. the query, to maximise the\noverall effectiveness of a retrieval system for the user. The task of ad-hoc ranking is, for\neach query, to compute an ordering of the documents equivalent or most similar to\nthe optimal ordering based on the probability of relevance. It is common to limit the\ndocuments to be ordered to just the top k documents in the optimal ordering.\nLet D denote a collection of (text) documents, and Q denote a log of (text) queries.\nQueries and documents share the same vocabulary V of terms. A ranking function,\nalso known as scoring function, s : Q×D → R computes a real-valued score for the\ndocuments in the collection D w.r.t. the queries in the log Q. Given a query q and a\ndocument d, we call the value s(q, d) relevance score of the document w.r.t. the query.\nFor a given query, the scores of the documents in the collection can be used to induce\nan ordering of the documents, in reverse value of score. The closer this induced or-\ndering is to the optimal ordering, the more effective an IR system based on the scoring\nfunction is.\nWithout loss of generality, the scoring function s(q, d) can be further decomposed as:\ns(q, d) = f (φ(q), ψ(d), η(q, d)), (1)\nwhere φ : Q → V1, ψ : D → V2, and η : Q×D → V3 are three representation func-\ntions, mapping queries, documents, and query-document pairs into the latent repre-\nsentation spaces V1, V2, and V3, respectively [Guo et al. 2020]. These functions build\nabstract mathematical representations of the text sequences of documents and queries\namenable for computations. The elements of these vectors represent the features used\nto describe the corresponding objects, and the aggregation function f : V1 ×V2 ×V3 →\nR computes the relevance score of the document representation w.r.t. the query repre-\nsentation.\nThe representation functions φ, ψ and η, and the aggregation function f can be de-\nsigned by hand, leveraging some axioms or heuristics, or computed through machine\nlearning algorithms. In the following, we will overview the representation functions\nadopted in classical IR (Section 1.1), in LTR scenarios (Section 1.2) and the recently\nproposed word embeddings (Section 1.3).",
            "subsections": {
                "BOW Encodings": {
                    "content": "In classical IR, both representation and aggregation functions are designed manually,\nincorporating some lexical statistics such as number of occurrences of terms in a doc-\nument or in the whole collection. Classical IR ranking models, e.g., vector space mod-\nels [Salton et al. 1975], probabilistic models [Robertson and Zaragoza 2009] and sta-\ntistical language models [Ponte and Croft 1998], are based on the bag of words (BOW)\nmodel, where queries and documents are represented as a set of terms from the vo-\ncabulary V together with the number of occurrences of the corresponding tokens in\nthe text. More formally, queries and documents are represented as vectors φ(q) and\nψ(d) in N|V|, called BOW encodings, where the i-th component of both representations\nencodes the number of occurrences of the term wi ∈ V in the corresponding text.\nThe query-document representation function η is not present in these ranking func-\ntions. The aggregation function f over these representations is an explicit formula\ntaking into account the components of the query and document representations, i.e.,\nthe in-query and in-document term frequencies, together with other document nor-\nmalisation operations. These representations are referred to as sparse representations,\nsince most of their components are equal to 0 because they correspond to tokens not\nappearing in the query/document. Sparse representations can be trivially computed\nand efficiently stored in specialised data structures called inverted indexes, which rep-\nresent the backbone of commercial Web search engine [Cambazoglu and Baeza-Yates\n2015]; see [B  ̈uttcher et al. 2010, Manning et al. 2008, Tonellotto et al. 2018] for more\ndetails on inverted indexes and classical IR ranking models."
                },
                "Word Embeddings":{
                    "content": "Both BOW encodings and LTR features are widely adopted in commercial search en-\ngines, but they suffer from several limitations. On the one hand, semantically-related\nterms end up having completely different BOW encodings. Although the two terms\ncatalogue and directory can be considered synonyms, their BOW encodings are com-\npletely different, with the single 1 appearing in different components. Similarly, two\ndifferent documents on a same topic can end up having two unrelated BOW encod-\nings. On the other hand, LTR features create text representations by hand via feature\nengineering, with heterogeneous components and no explicit concept of similarity.\nIn the 1950s, many linguists formulated the distributional hypothesis: words that occur\nin the same contexts tend to have similar meanings [Harris 1954]. According to this\nhypothesis, the meaning of words can be inferred by their usage together with other\nwords in existing texts. Hence, by leveraging the large text collections available, it is\npossible to learn useful representations of terms, and devise new methods to use these\nrepresentations to build up more complex representations for queries and documents.\nThese learned representations are vectors in Rn, with n |V|, called distributional rep-\nresentations or word embeddings. The number of dimensions n ranges approximatively\nfrom 50 to 1000 components, instead of the vocabulary size |V|. Moreover, the com-\nponents of word embeddings are rarely 0: they are real numbers, and can also have\nnegative values. Hence, word embeddings are also referred to as dense representations.\nAmong the different techniques to compute these representations, there are algorithms\nto compute global representations of the words, i.e., a single fixed embedding for each\nterm in the vocabulary, called static word embeddings, and algorithms to compute local\nrepresentations of the terms, which depend on the other tokens used together with\na given term, i.e., its context, called contextualised word embeddings. Static word em-\nbeddings used in neural IR are learned from real-world text with no explicit training\nlabels: the text itself is used in a self-supervised fashion to compute word represen-\ntations. There are different kinds of static word embeddings, for different languages,\nsuch as word2vec [Mikolov et al. 2013], fasttext [Joulin et al. 2017] and GloVe [Pennington\net al. 2014]. Static word embeddings map terms with multiple senses into an average\nor most common sense representation based on the training data used to compute the\nvectors; each term in the vocabulary is associated with a single vector. Contextualised\nword embeddings map tokens used in a particular context to a specific vector; each\nterm in the vocabulary is associated with a different vector every time it appears in\na document, depending on the surrounding tokens. The most popular contextualised\nword embeddings are learned with deep neural networks such as the Bidirectional En-\ncoder Representations from Transformers (BERT) [Devlin et al. 2019], the Robustly Op-\ntimized BERT Approach (RoBERTa) [Liu et al. 2019], and the Generative Pre-Training\nmodels (GPT) [Radford and Narasimhan 2018].\nIn neural IR, word embeddings are used to compute the representation functions φ,\nψ and η, and the aggregation function f through (deep) neural networks. Depend-\ning on the assumptions over the representation functions, the neural ranking models\ncan be classified in interaction-focused models and representation-focused models. In\ninteraction-focused models, the query-document representation function η(q, d), tak-\ning into account the interaction between the query and document contents, is explicitly\nconstructed and used as input to a deep neural network, or it is implicitly generated\nand directly used by a deep neural network. In representation-focused models, the\nquery-document representation function η(q, d) is not present; query and document\nrepresentations φ(q) and ψ(d) are computed independently by deep neural networks\nIn the following, we discuss the main interaction-focused models for ad-hoc ranking\n(Section 2) and representation-focused models for queries and documents (Section 3)."
                }
            }       
        }

    }
}
