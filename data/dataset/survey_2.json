{
    "A survey of multimodal deep generative models":{
        "Introduction":{
            "content": ""
        },
        "Multimodal generative models":{
            "content": ""
        },
        "Multimodal deep generative models":{
            "content": ""
        },
        "Coordinated model":{
            "content": ""
        },
        "Joint Model":{
            "content": ""
        },
        "Benchmark datasets and application":{
            "content": "Multimodal deep generative models have no multimodal benchmark that is always used like\nzero-shot learning. The most commonly used are image datasets such as MNIST [46], FashionMNIST [115], and the street view house number (SVHN) [61], which are often used as toy problems\nfor deep learning. However, since these datasets are not multimodal data, many studies turned\nthem into multimodal settings in various ways. For example, the image and label are consid-\nered different modalities [91, 112, 113, 118], and the image with different noise is considered\nas multimodal data [89, 107]. Another common practice is to pair multiple datasets that share\nthe same label domain (i.e., number of classes) and consider them as multimodal data, such as\nMNIST and FashionMNIST [39, 96, 118] or MNIST and SVHN [15, 78, 79]. In addition, some\nstudies have created multimodal data by adding information such as voice and text to these\ndatasets [33, 88].\nSeveral studies have experimented with a true multimodal setting rather than the toy prob-\nlem described above. A common multimodal dataset used in many multimodal deep generative\nmodels [33, 88, 88, 89, 91, 104, 118] is the CelebA dataset [52], which contains face images\nand their attributes. In CADA-VAE [76], the authors used the Caltech-UCSD Birds (CUB)-200\ndataset [111], which contains bird images and their attributes, as a benchmark for zero-shot\nlearning. Shi et al. [78, 79] used the CUB-200-2011 dataset [106], which is an extended version\nof the CUB-200 dataset and includes fine-grained natural language descriptions. Jo et al. [39]\nworked on hand pose estimation using RGB images and keypoints of each hand in the Rendered\nHand pose dataset [126] as multimodal data. Yin et al. [119] trained their multimodal deep\ngenerative model using the handwriting motion and the corresponding handwriting image in the\nUJI Char Pen dataset as different modalities to confirm that cross-modal generation is possible.\nTsai et al. [99] conducted experiments on real-world time-series multimodal datasets such as\nCMU-MOSI [120], which includes three modalities: language, acoustic, and visual.\nMoreover, beyond the multimodal deep generative model defined in this study, i.e., in a setting\nwhere not all modalities are inferred and generated, various multimodal tasks are performed\nwith VAEs, such as audio-visual speech enhancement [73, 74] and the acquisition of the joint\nrepresentation of depth and RGB images [11].\nThere have also been several studies on the application of multimodal deep generative models\n11\nto robots. Zambelli et al. [121] trained a JVAE using five modalities from an iCub robot [55]\ninteracting with a piano keyboard as input, including joint positions and vision, to reconstruct\nmissing modalities and predict their own sensorimotor states and others’ visual trajectories.\nPark et al. [69] introduced a LSTM-based JVAE to detect anomalous feeding executions given\n17 sensory signals from five types of sensors on a PR2 robot. Meo et al. [54] proposed considering\nthe joint angles in a simulated robot arm and the images provided by the camera as different\nmodalities, and using JVAE to learn them and control the robot based on the active inference.\nKorthals et al. [44] argued that the representation of the multimodal deep generative model\ncan be used as a state as well as a reward in deep reinforcement learning, and showed that the\nproposed model is effective in multi-agent reinforcement learning with multiple robots."
        },
        "Future challenges":{
            "content": "In this section we discuss future research directions for multimodal deep generative models.\nThe studies of multimodal deep generative models described so far have been validated mainly\nin two, or at most three, modalities. However, the number of modalities in the real world is far\ngreater, and more modality information has been used in previous studies of cognitive architecture based on probabilistic generative models. As described in Section 2.5, Nakamura et al. [58]\nused visual, tactile, auditory, and word information obtained from the robot.\nMoreover, the greater the number and variety of modalities, the more difficult training the entire model end-to-end will be. Therefore, it is important to learn modules that deal with different\nmodalities separately and integrate their inference. Coordinated models often use these two-step\napproaches; however, as mentioned earlier, they cannot perform inference on shared representations from arbitrary sets of modalities. Recently, Symbol Emergence in Robotics tool KIT\n(SERKET) [60] and Neuro-SERKET [93] have been proposed as a frameworks for integration in\nprobabilistic models that deal with multimodal information. SERKET provides a protocol that\ndivides inference from different modalities into the inference of modules of individual modalities\nand their communication, i.e., message-passing. Neuro-SERKET, an extension of the SERKET\nframework for including neural networks, can integrate modules of different modalities that perform different inference procedures, such as Gibbs sampling and variational inference. Another\npossible approach is to refer to the idea of global workspace (GW) theory in cognitive science [4],\nin which interactions between different modules are realized by a shared space that can be modified by any module and broadcast to all modules. Goyal et al. [20] proposed an attention-based\nGW architecture for communicating positions and modules in Transformer [103] and slot-based\nmodular architectures [21]. Such idea might also be applied to the integration of modules of\ndifferent modalities.\nFurthermore, multimodal deep generative models can be applied to the domain of world models [22], i.e., model-based reinforcement learning with self-supervised learning. Many studies on\nworld models have used only images as input modalities [22–24], but we humans are building\nmore reliable models in our brains of how the world is organized from many different types\nof information. Recently, Taniguchi et al. [94] proposed implementing prediction and decision\nmaking from multimodal information in the framework of generative models based on human\ncognitive systems.\nFrom the viewpoint of robot research, as mentioned in Section 6, some studies used multimodal\ndeep generative models, but they are not yet mainstream and their latest methods are rarely\nused. It is inevitable for robots to deal with multiple modalities and the ability of multimodal\ndeep generative models to obtain integrated representations from them, to deal with missing\nmodalities, and to transform between different modalities should help robots make better decisions in the real world. However, there are several challenges in applying multimodal deep\ngenerative models to robots operating in the real world. For example, in order for robots with\nmultimodal deep generative models to be able to generalize properly in different environments,\n12\nwe need to acquire a large amount of training data for all environments, which is difficult to do\nin practice. To deal with this difficulty, it might be important to use techniques such as transfer\nlearning and continuous learning [49, 95], given that humans have the ability to act in unknown\nenvironments based on their memories of other environments and the ability to continuously\nlearn new things from the past. Furthermore, in terms of model implementation, multimodal\ndeep generative models might become larger and more complex due to the need to train large\nenvironments based on a large number of multimodal information, which makes it difficult to\nmaintain and handle these implementations1. Probabilistic modeling languages, such as Pyro [7],\nand deep generative modeling libraries, such as Pixyz [90], might help to address this difficulty.\nWe hope that multimodal deep generative models will be widely used in robot control in the\nfuture."
        },
        "Conclusion":{
            "content": "In this paper, we surveyed multimodal deep generative models in two categories: coordinated\nmodels and joint models. We classified the coordinated models into two criteria according to\nthe how they define the closeness between inference distributions. For the joint models, we\nsummarized the study with three important challenges. In particular, for the first challenge,\nthe treatment of missing modalities, we described two approaches: the introduction of surrogate\nunimodal inference and the aggregation of unimodal inference. In addition, we summarized\nthe benchmark datasets and applications of multimodal deep generative models and discussed\nfuture directions. We hope that this paper will be of help for future research on multimodal deep\ngenerative models."
        }
    }
}