{

    "Lecture Notes on Neural Information Retrieval": {
        "introduction": {
            "content": "Text Information Retrieval (IR) systems focus on retrieving text documents able to ful- fill the information needs of their users, typically expressed as textual queries. Over the years, this inherently vague description has been formalised and characterised by the specific nature of documents, information needs, and users. At the core of the for- malisation lies the concept of relevance of a document with respect to a query, and how to estimate their relevance. Over the years, many different ranking models have been proposed to estimate the relevance of documents in response to a query. These mod- els depend on the information provided by the queries and the documents, that are exploited to derive “relevance signals”. Many ranking models have been developed over the years, ranging from Boolean models to probabilistic and statistical language models. These “bag of words” models leverage the presence or the number of occur- rences of query terms in the documents to infer their relevance to a query, exploiting hand-crafted functions to combine these occurrences such as BM25. With the rise of the Web and social platforms, more sources of relevance information about documents have been identified. Machine learning methods have been proved effective to deal with this abundance of relevance signals, and their application to rank the documents in order of relevance estimates w.r.t. a query has given birth to many learning-to-rank (LTR) models. Relevance signals are input features in LTR models, and they are often designed by hand, a time-consuming process. Motivated by their breakthroughs in many computer vision and natural language processing tasks, neural networks repre- sent the current state-of-the-art approach in ranking documents w.r.t. query relevance. Neural Information Retrieval focuses on retrieving text documents able to fulfill the information needs of their users exploiting deep neural networks. In neural IR, neural networks are typically used in two different ways: to learn the ranking functions com- bining the relevance signals to produce an ordering of documents, and to learn the abstract representations of documents and queries to capture their relevance informa- tion. In the following, we provide an introduction to the recent approaches in neural IR. Since the research in the field is rapidly evolving, we do not pretend to cover every single aspect of neural IR, but to provide a principled introduction to the main ideas and existing systems in the field. When available, we provide links to relevant and more detailed surveys. Here is a quick overview over what the sections are about. Section 1 provides a short depiction of the different representations for text adopted in IR, from the classical BOW encodings to learning-to-rank features to word embeddings. Section 2 presents the main neural architectures for computing a joint representation of query and document pairs for relevance ranking. Section 3 focuses on the neural architectures specif- ically tailored for learning abstract complex representations of query and documents texts independently. Section 4 overviews the deployment schemes adopted in neural IR systems, together with an overview of the most common dense retrieval indexes supporting exact and approximate nearest neighbour search. Section 5 discusses the current approaches in learned sparse retrieval, dealing with the learning of low di- mensional representations of documents amenable to be stored in an inverted index or similar data structures. Finally, Section 6 draws concluding remarks."
        },
        "Text Representations for Ranking": {
            "content": "According to the Probability Ranking Principle [Robertson 1977], under certain assumptions, for a given user’s query, documents in a collection should be ranked in\norder of the (decreasing) probability of relevance w.r.t. the query, to maximise the\noverall effectiveness of a retrieval system for the user. The task of ad-hoc ranking is, for\neach query, to compute an ordering of the documents equivalent or most similar to\nthe optimal ordering based on the probability of relevance. It is common to limit the\ndocuments to be ordered to just the top k documents in the optimal ordering.\nLet D denote a collection of (text) documents, and Q denote a log of (text) queries.\nQueries and documents share the same vocabulary V of terms. A ranking function,\nalso known as scoring function, s : Q × D → R computes a real-valued score for the\ndocuments in the collection D w.r.t. the queries in the log Q. Given a query q and a\ndocument d, we call the value s(q, d) relevance score of the document w.r.t. the query.\nFor a given query, the scores of the documents in the collection can be used to induce\nan ordering of the documents, in reverse value of score. The closer this induced ordering is to the optimal ordering, the more effective an IR system based on the scoring\nfunction is.\nWithout loss of generality, the scoring function s(q, d) can be further decomposed as:\ns(q, d) = f (φ(q), ψ(d), η(q, d)), (1)\nwhere φ : Q → V1, ψ : D → V2, and η : Q × D → V3 are three representation functions, mapping queries, documents, and query-document pairs into the latent representation spaces V1, V2, and V3, respectively [Guo et al. 2020]. These functions build\nabstract mathematical representations of the text sequences of documents and queries\namenable for computations. The elements of these vectors represent the features used\nto describe the corresponding objects, and the aggregation function f : V1 × V2 × V3 →\nR computes the relevance score of the document representation w.r.t. the query representation.\nThe representation functions φ, ψ and η, and the aggregation function f can be designed by hand, leveraging some axioms or heuristics, or computed through machine\nlearning algorithms. In the following, we will overview the representation functions\nadopted in classical IR (Section 1.1), in LTR scenarios (Section 1.2) and the recently\nproposed word embeddings (Section 1.3).\n1.1 BOW Encodings\nIn classical IR, both representation and aggregation functions are designed manually,\nincorporating some lexical statistics such as number of occurrences of terms in a document or in the whole collection. Classical IR ranking models, e.g., vector space models [Salton et al. 1975], probabilistic models [Robertson and Zaragoza 2009] and statistical language models [Ponte and Croft 1998], are based on the bag of words (BOW)\nmodel, where queries and documents are represented as a set of terms from the vocabulary V together with the number of occurrences of the corresponding tokens in\nthe text. More formally, queries and documents are represented as vectors φ(q) and\nψ(d) in N|V|, called BOW encodings, where the i-th component of both representations\nencodes the number of occurrences of the term wi ∈ V in the corresponding text.\nThe query-document representation function η is not present in these ranking functions. The aggregation function f over these representations is an explicit formula\ntaking into account the components of the query and document representations, i.e.,\nthe in-query and in-document term frequencies, together with other document normalisation operations. These representations are referred to as sparse representations,\nsince most of their components are equal to 0 because they correspond to tokens not\nappearing in the query/document. Sparse representations can be trivially computed\nand efficiently stored in specialised data structures called inverted indexes, which represent the backbone of commercial Web search engine [Cambazoglu and Baeza-Yates\n2015]; see [B  ̈uttcher et al. 2010, Manning et al. 2008, Tonellotto et al. 2018] for more\ndetails on inverted indexes and classical IR ranking models.\n1.2 LTR Features\nWith the advent of the Web, new sources of relevance information about the documents have been made available. The importance of a Web page, e.g., PageRank,\nadditional document statistics, e.g., term frequencies in the title or anchors text, and\nsearch engine interactions, e.g., clicks, can be exploited as relevance signals. Moreover,\ncollaborative and social platforms such as Wikipedia, Twitter and Facebook represent\nnew sources of relevance signals. These relevance signals have been exploited to build\nricher query and document representations in LTR. The relevance signals extracted\nfrom queries and/or documents are called features. There are various classes of these\nfeatures [Bendersky et al. 2011, Macdonald et al. 2012], such as:\n• query-only features, i.e., components of φ(q): query features with the same value\nfor each document, such as query type, query length, and query performance\npredictors;\n• query-independent features, i.e., components of ψ(d): document features with the\nsame value for each query, such as importance score, URL length, and spam\nscore;\n• query-dependent features, i.e., components of η(q, d): document features that depend on the query, such as different term weighting models on different fields.\nIn LTR, the representation functions are hand-crafted: exploiting the relevance signals\nfrom heterogeneous information sources, the different components of query and document representations are computed with feature-specific algorithms. Hence, the representations φ(q), ψ(d), and η(q, d) are elements of vector spaces over R, but whose dimensions depend on the number of hand-crafted query-only, query-independent, and\nquery-dependent features, respectively. Moreover the different components of these\nvectors are heterogeneous, and do not carry any specific semantic meaning. Using\nthese representations, in LTR the aggregation function f is machine-learned, for example using logistic regression [Gey 1994], gradient-boosted regression trees [Burges\n2010] or neural networks [Burges et al. 2005]; see [Liu 2009] for a detailed survey.\n1.3 Word Embeddings\nBoth BOW encodings and LTR features are widely adopted in commercial search engines, but they suffer from several limitations. On the one hand, semantically-related\nterms end up having completely different BOW encodings. Although the two terms\ncatalogue and directory can be considered synonyms, their BOW encodings are completely different, with the single 1 appearing in different components. Similarly, two\ndifferent documents on a same topic can end up having two unrelated BOW encodings. On the other hand, LTR features create text representations by hand via feature\nengineering, with heterogeneous components and no explicit concept of similarity.\nIn the 1950s, many linguists formulated the distributional hypothesis: words that occur\nin the same contexts tend to have similar meanings [Harris 1954]. According to this\nhypothesis, the meaning of words can be inferred by their usage together with other\nwords in existing texts. Hence, by leveraging the large text collections available, it is\npossible to learn useful representations of terms, and devise new methods to use these\nrepresentations to build up more complex representations for queries and documents.\nThese learned representations are vectors in Rn, with n |V|, called distributional representations or word embeddings. The number of dimensions n ranges approximatively\nfrom 50 to 1000 components, instead of the vocabulary size |V|. Moreover, the components of word embeddings are rarely 0: they are real numbers, and can also have\nnegative values. Hence, word embeddings are also referred to as dense representations.\nAmong the different techniques to compute these representations, there are algorithms\nto compute global representations of the words, i.e., a single fixed embedding for each\nterm in the vocabulary, called static word embeddings, and algorithms to compute local\nrepresentations of the terms, which depend on the other tokens used together with\na given term, i.e., its context, called contextualised word embeddings. Static word embeddings used in neural IR are learned from real-world text with no explicit training\nlabels: the text itself is used in a self-supervised fashion to compute word representations. There are different kinds of static word embeddings, for different languages,\nsuch as word2vec [Mikolov et al. 2013], fasttext [Joulin et al. 2017] and GloVe [Pennington\net al. 2014]. Static word embeddings map terms with multiple senses into an average\nor most common sense representation based on the training data used to compute the\nvectors; each term in the vocabulary is associated with a single vector. Contextualised\nword embeddings map tokens used in a particular context to a specific vector; each\nterm in the vocabulary is associated with a different vector every time it appears in\na document, depending on the surrounding tokens. The most popular contextualised\nword embeddings are learned with deep neural networks such as the Bidirectional Encoder Representations from Transformers (BERT) [Devlin et al. 2019], the Robustly Optimized BERT Approach (RoBERTa) [Liu et al. 2019], and the Generative Pre-Training\nmodels (GPT) [Radford and Narasimhan 2018].\nIn neural IR, word embeddings are used to compute the representation functions φ,\nψ and η, and the aggregation function f through (deep) neural networks. Depending on the assumptions over the representation functions, the neural ranking models\ncan be classified in interaction-focused models and representation-focused models. In\ninteraction-focused models, the query-document representation function η(q, d), taking into account the interaction between the query and document contents, is explicitly\nconstructed and used as input to a deep neural network, or it is implicitly generated\nand directly used by a deep neural network. In representation-focused models, the\nquery-document representation function η(q, d) is not present; query and document\nrepresentations φ(q) and ψ(d) are computed independently by deep neural networks\nIn the following, we discuss the main interaction-focused models for ad-hoc ranking\n(Section 2) and representation-focused models for queries and documents (Section 3)."
        },
        "Interaction-focused Systems" : {
            "content": "The interaction-focused systems used in neural IR model the word and n-gram relationships across a query and a document using deep neural networks. These systems receive as input both a query q and a document d, and output a query-document\nrepresentation η(q, d). Among others, two neural network architectures have been\ninvestigated to build a representation of these relationships: convolutional neural networks and transformers. Convolutional neural networks represent one of the first\napproaches in building joint representations of queries and documents, as discussed\nin Section 2.1. Transformers represent the major turning point in neural IR, as their\napplication to textual inputs gave birth to pre-trained language models, presented\nin Section 2.2. In neural IR, pre-trained language models are used to compute query-document representations, and the two main transformer models used for this task are\nBERT and T5 illustrated in Sections 2.3 and 2.4, respectively. Section 2.5 describes how\npre-trained language models are fine-tuned to compute effective query-document representations, and Section 2.6 briefly discusses how pre-trained language models can\ndeal with long documents.\n2.1 Convolutional Neural Networks\nA convolutional neural network is a family of neural networks designed to capture\nlocal patterns in structured inputs, such as images and texts [LeCun and Bengio 1998].\nThe core component of a convolutional neural network is the convolution layer, used\nin conjunction with feed forward and pooling layers. A convolutional layer can be\nseen as a small linear filter, sliding over the input and looking for proximity patterns.\nSeveral neural models employ convolutional neural networks over the interactions\nbetween queries and documents to produce relevance scores. Typically, in these models, the word embeddings of the query and document tokens are aggregated into an\ninteraction matrix, on top of which convolutional neural networks are used to learn\nhierarchical proximity patterns such as unigrams, bigrams and so on. Then, the final\ntop-level proximity patterns are fed into a feed forward neural network to produce\nthe relevance score s(q, d) between the query q and the document d, as illustrated in\nFigure 2.ψ1 ψn\nWord embeddings\nd\n𝜙1\n𝜙m\nWord embeddings\nq η(q,d)\nConvolutional \nneural \nnetworks\nFeed forward \nneural \nnetworks\ns(q,d)\nProximity\nPatterns\nInteraction\nMatrix\nRelevance\nScore\nFigure 2: Scheme of an interaction-focused model based on convolutional neural networks.\nThe query q and the document d are tokenised into m and n tokens, respectively, and\neach token is mapped to a corresponding static word embedding. The interaction\nmatrix η(q, d) ∈ Rm×n is composed of the cosine similarities between a query token\nembedding and a document token embedding.\nOne of the first neural models leveraging the interaction matrix is the Deep Relevance\nMatching Model (DRMM) [Guo et al. 2016]). In DRMM, the cosine similarities of every\nquery token w.r.t. the document tokens are converted into a discrete distribution using\nhard bucketing, i.e., into a query token histogram. Then the histogram of each query token is provided as input to a feed forward neural network to compute the final query\ntoken-document relevance score. These relevance scores are then aggregated through\nan IDF-based weighted sum across the different query terms. Instead of using hard\nbucketing, the Kernel-based Neural Ranking Model (KNRM) [Xiong et al. 2017] proposes to use Gaussian kernels to smoothly distribute the contribution of each cosine\nsimilarity across different buckets before providing the histograms with soft bucketing\nto the feed forward neural networks.\nBoth DRMM and KNRM exploit the interaction matrix, but they do not incorporate\nany convolutional layer. In the Convolutional KNRM model (ConvKNRM) [Dai et al.\n2018], the query and document embeddings are first independently processed through\nk convolutional neural networks, to build unigam, bigram, up to k-gram embeddings.\nThese convolutions allow to build word embeddings taking into account multiple\nclose words at the same time. Then, k2 cosine similarity matrices are built, between\neach combination of query and document n-gram embeddings, and these matrices\nare processed with KNRM. In the Position-Aware Convolutional Recurrent Relevant\nmodel (PACRR) [Hui et al. 2017], the interaction matrix is processed through several\nconvolutional and pooling layers to take into account words proximity. Convolutional\nlayers are used also in other similar neural models [Fan et al. 2018b, Hu et al. 2014, Hui\net al. 2018, Pang et al. 2016, 2017].\n2.2 Pre-trained Language Models\nStatic word embeddings map words with multiple senses into an average or most\ncommon-sense representation based on the training data used to compute the vectors.\nThe vector of a word does not change with the other words used in a sentence around\nit. The transformer is a neural network designed to explicitly take into account the\ncontext of arbitrary long sequences of text, thanks to a special neural layer called selfattention, used in conjunction with feed forward and linear layers. The self-attention\nlayer maps input sequences to output sequences of the same length. When computing the i-th output element, the layer can access all the n input elements (bidirectional\nself-attention) or only the first i input elements (causal self-attention). A self-attention\nlayer allows the network to take into account the relationships among different elements in the same input. When the input elements are tokens of a given text, a selfattention layer computes token representations that take into account their context,\ni.e., the surrounding words. In doing so, the transformer computes contextualised word\nembeddings, where the representation of each input token is conditioned by the whole\ninput text.\nTransformers have been successfully applied to different natual language processing\ntasks, such as machine translation, summarisation, question answering and so on. All\nthese tasks are special instances of a more general task, i.e., transforming an input\ntext sequence to some output text sequence. The sequence-to-sequence model has been\ndesigned to address this general task. The sequence-to-sequence neural network is\ncomposed of two parts: an encoder model, which receives an input sequence and builds\na contextualised representation of each input element, and a decoder model, which\nuses these contextualised representations to generate a task-specific output sequence.\nBoth models are composed of several stacked transformers. The transformers in the\nencoder employ bidirectional self-attention layers on the input sequence or the output\nsequence of the previous transformer. The transformers in the decoder employ causal\nself-attention on the previous decoder transformer’s output and bidirectional crossattention of the output of the final encoder transformer’s output.\nIn neural IR, two specific instances of the sequence-to-sequence models have been\nstudied: encoder-only models and encoder-decoder models. Encoder-only models receive as input all the tokens of a given input sentence, and they compute an output\ncontextualised word embedding for each token in the input sentence. Representatives\nof this family of models include BERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019],\nand DistilBERT [Sanh et al. 2019]. Encoder-decoder models generate new output sentences depending on the given input sentence. The encoder model receives as input all\nthe tokens of a given sequence and builds a contextualised representation, and the decoder model sequentially accesses these embeddings to generate new output tokens,\none token at a time. Representatives of this family of models include BART [Lewis\net al. 2020] and T5 [Raffel et al. 2020].\nSequence-to-sequence models can be trained as language models, by projecting with a\nlinear layer every output embedding to a given vocabulary and computing the tokens\nprobabilities with a softmax operation. The softmax operation is a function σ : Rk →\n[0, 1]k that takes as input k > 1 real values z1, z2, . . . , zk and transforms each input zi as\nfollows:\nσ(zi ) = ezi\n∑kj=1 ezj (2)\nThe softmax operation normalises the input values into a probability distribution. In\nthe context of deep learning, the inputs of a softmax operation are usually called logits,\nand they represent the raw predictions generated by a multi-class classification model,\nturned into a probability distribution over the classes by the softmax operation.\nDepending on the training objective, a sequence-to-sequence model can be trained\nas a masked language model (MLM), as for BERT, or a casual language model (CLM), as\nfor T5. MLM training focuses on learning to predict missing tokens in a sequence\ngiven the surrounding tokens; CLM training focuses on predicting the next token in\nan output sequence given the preceding tokens in the input sequence. In both cases,\nit is commonplace to train these models using massive text data to obtain pre-trained\nlanguage models. In doing so, we allow the model to learn general-purpose knowledge\nabout a language that can be adapted afterwards to a more specific downstream task. In\nthis transfer learning approach, a pre-trained language model is used as initial model\nto fine-tune it on a domain-specific, smaller training dataset for the downstream target\ntask. In other words, fine-tuning is the procedure to update the parameters of a pretrained language model for the domain data and target task.\nAs illustrated in Figure 3, pre-training typically requires a huge general-purpose training corpus, such as Wikipedia or Common Crawl web pages, expensive computation resources and long training times, spanning several days or weeks. On the other\nside, fine-tuning requires a small domain-specific corpus focused on the downstream\ntask, affordable computational resources and few hours or days of additional training.\nSpecial cases of fine-tuning are few-shot learning, where the domain-specific corpus is\ncomposed of a very limited number of training data, and zero-shot learning, where a\npre-trained language model is used on a downstream task that it was not fine-tuned\non.\nIn neural IR, the interaction-focused systems that use pre-trained language models are\ncalled cross-encoder models, as they receive as input a pair (q, d) of query and document\ntexts. Depending on the type of sequence-to-sequence model, different cross-encoders\nare fine-tuned in different ways, but, in general, they aim at computing a relevance\nscore s(q, d) to rank documents w.r.t. a given query. In the following, we illustrate\nthe most common cross-encoders leveraging both encoder-only models (Sec. 2.3) and\nencoder-decoder models (Sec. 2.4).\n2.3 Ranking with Encoder-only Models\nThe most widely adopted transformer architecture in neural IR is BERT, an encoderonly model. Its input text is tokenised using the WordPiece sub-word tokeniser [Wu\net al. 2016]. The vocabulary V of this tokeniser is composed of 30, 522 terms, where the\nuncommon/rare words, e.g., goldfish, are splitted up in sub-words, e.g., gold## and\n##fish. The first input token of BERT is always the special [CLS] token, that stands\nfor “classification”. BERT accepts as input other special tokens, such as [SEP], that\ndenotes the end of a text provided as input or to separate two different texts provided\nas a single input. BERT accepts as input at most 512 tokens, and produces an output\nembedding in R` for each input token. The most commonly adopted BERT version\nis BERT base, which stacks 12 transformer layers, and whose output representation\nspace has ` = 768 dimensions.\nNogueira and Cho [2019] and MacAvaney et al. [2019] illustrated how to fine-tune\nBERT as a cross-encoder1, in two slightly different ways. Given a query-document\npair, both texts are tokenised into token sequences q1, . . . , qm and d1, . . . , dn. Then,\nthe tokens are concatenated with BERT special tokens to form the following input\nconfiguration:\n[CLS] q1 ···qm [SEP] d1 ···dn [SEP]\nthat will be used as BERT input. In doing so, the self-attention layers in the BERT encoders are able to take into account the semantic interactions among the query tokens\nand the document tokens. The output embedding η[CLS] ∈ R`, corresponding to the\ninput [CLS] token, serves as a contextual representation of the query-document pair\nas a whole.\nNogueira et al. [2019a] fine-tune BERT on a binary classification task to compute the\nquery-document relevance score, as illustrated in Figure 4. To produce the relevance\nscore s(q, d), the query and the document are processed by BERT to generate the output embedding η[CLS] ∈R`, that is multiplied by a learned set of classification weights\nW2 ∈R2×` to produce two real scores z0 and z1, and then through a softmax operation\nto transform the scores into a probability distribution p0 and p1 over the non-relevant\nand relevant classes. The probability corresponding to the relevant class, conventionally assigned to label 1, i.e., p1, is the final relevance score.\nMacAvaney et al. [2019] fine-tune BERT by projecting the output embedding η[CLS] ∈\nR` through the learned matrix W1 ∈R1×` into a single real value z, that represents the\nfinal relevance score.\nη[CLS] = BERT(q, d)\n[z0, z1] = W2η[CLS] or z = W1η[CLS]\n[p0, p1] = softmax([z0, z1])\ns(q, d) = p1 or s(q, d) = z\nIn Section 2.5 we illustrate how a BERT-based cross-encoder is typically fine-tuned for\nad-hoc ranking.\n2.4 Ranking with Encoder-decoder Models\nInstead of using an encoder-only transformer model to compute the latent representation of a query-document pair and to convert it into a relevance score, it is possible also\nto use an encoder-decoder model [Raffel et al. 2020] with prompt learning, by converting\nthe relevance score computation task into a cloze test, i.e., a fill-in-the-blank problem.\nPrompting has been successfully adopted in article summarisation tasks [Radford et al.\n2019] and knowledge base completion tasks [Petroni et al. 2019].\nIn prompt learning, the input texts are reshaped as a natural language template, and\nthe downstream task is reshaped as a cloze-like task. For example, in topic classification, assuming we need to classify the sentence text into two classes c0 and c1, the\ninput template can be:\nInput: text Class: [OUT]\nAmong the vocabulary terms, two label terms w0 and w1 are selected to correspond to\nthe classes c0 and c1, respectively. The probability to assign the input text to a class\ncan be transferred into the probability that the input token [OUT] is assigned to the\ncorresponding label token:\np(c0|text) = p([OUT] = w0|Input: text Class: [OUT])\np(c1|text) = p([OUT] = w1|Input: text Class: [OUT])\nNogueira et al. [2020] proposed a prompt learning approach for relevance ranking\nusing a T5 model2, as illustrated in Figure 5. The query and the document texts q and\nd are concatenated to form the following input template:\nQuery: q Document: d Relevant: [OUT]\nAn encoder-decoder model is fine-tuned with a downstream task taking as input this\ninput configuration, and generating an output sequence whose last token is equal to\nTrue or False, depending on whether the document d is relevant or non-relevant to\nthe query q.\nThe query-document relevance score is computed by normalising only the False and\nTrue output probabilities, computed over the whole vocabulary, with a softmax operation.\nTo produce the relevance score s(q, d), the query and the document are processed by\nT5 to generate the output embedding η[OUT] ∈R`, that is projected by a learned set of\nclassification weights WV ∈R|V|×` over the vocabulary V. The outputs zFalse and zTrue\ncorresponding to the False and True terms, respectively, are transformed into a probability distribution with a softmax operation, to yield the required predictions pFalse and\npTrue over the “non-relevant” and “relevant” classes. The prediction corresponding to\n2Nogueira et al. [2020] call it monoT5.\nthe relevant class, i.e., pTrue, is the final relevance score.\nη[OUT] = T5(q, d)\n[. . . , zFalse, . . . , zTrue, . . .]T = WV η[OUT]\n[pFalse, pTrue] = softmax([zFalse, zTrue])\ns(q, d) = pTrue\n(4)\nIn the next section we discuss how a T5-based cross-encoder is typically fine-tuned for\nad-hoc ranking.\n2.5 Fine-tuning Interaction-focused Systems\nAs discussed in Section 2.2, the pre-trained language models adopted in IR require\na fine-tuning of the model on a specific downstream task. Given an input querydocument pair (q, d) a neural IR model M(θ), parametrised by θ, computes sθ (q, d) ∈\nR, the score of the document d w.r.t. the query q. We are supposed to predict y ∈\n{+, −}from (q, d) ∈Q×D, where −stands for non-relevant and + for relevant.\nThis problem can be framed as a binary classification problem. We perform the classification by assuming a joint distribution p over {+, −}×Q×D from which we\ncan sample correct pairs (+, q, d) ≡ (q, d+) and (−, q, d) ≡ (q, d−). Using sampled\ncorrect pairs we learn a score function sθ (q, d) as an instance of a metric learning problem [Xing et al. 2002]: the score function must assign a high score to a relevant document and a low score to a non-relevant document, as in Eq. (3) and Eq. (4). Then we\nfind θ∗ that minimises the (binary) cross entropy lCE between the conditional probability p(y|q, d) and the model probability pθ (y|q, d):\nθ∗ = arg minθ E\n[\nlCE (y, q, d)\n]\n(5)\nwhere the expectation is computed over (y, q, d) ∼ p, and the cross entropy is computed as:\nlCE\n(y, (q, d)) =\n\n\n−log (sθ (q, d)) if y = +\n−log (1 −sθ (q, d)) if y = − (6)\nTypically, a dataset T available for fine-tuning pre-trained language models for relevance scoring is composed of a list of triples (q, d+, d−), where q is a query, d+ is a\nrelevant document for the query, and d− is a non-relevant document for the query. In\nthis case, the expected cross entropy is approximated by the sum of the cross entropies\ncomputed for each triple:\nE\n[\nlCE\n(y, (q, d))]\n≈ 1\n2|T| ∑(q,d+,d−)∈T\n(\n−log (sθ (q, d+))−log (1 −sθ (q, d−)))\n(7)\nIn doing so, we do not take into account documents in the collection that are not explicitly labeled as relevant or non-relevant. This approach is limited to take into account positive and negative triples in a pairwise independent fashion. In Section 3.3\nwe will discuss a different fine-tuning approach commonly used for representationfocused systems, taking into account multiple non-relevant documents per relevant\ndocument.\n2.6 Dealing with long texts\nBERT and T5 models have an input size limited to 512 tokens, including the special\nones. When dealing with long documents, that cannot be fed completely into a transformer model, we need to split them into smaller texts in a procedure referred to as\npassaging. Dai and Callan [2019a] propose to split a long document into overlapping\nshorter passages, to be processed independently together with the same query by a\ncross-encoder. During training, if a long document is relevant, all its passages are\nrelevant, and vice-versa. Then, the relevance scores for each composing passage are\naggregated back to a single score for the long document. In this scenario, the common\naggregation functions are FirstP, i.e., the document score is the score of the first passage, MaxP, i.e., the document score is the highest score across all passages, and SumP,\ni.e., the document score is the sum of the scores of its passages. Alternatively, Li et al.\n[2020] generate the [CLS] output embedding for each passage to compute a querypassage representation for each passage. Then, the different passage embeddings are\naggregated together to compute a final relevance score for the whole document using\nfeed forward neural networks, convolutional neural networks or simple transformer\narchitectures"
        },
        "Representation-focused Systems": {
            "content":"The representation-focused systems build up independent query and document rep-\nresentations, in such a way that document representations can be pre-computed and\nstored in advance. During query processing, only the query representation is com-\nputed, and the top documents are searched through the stored document representa-\ntions. In doing so, representation-based systems are able to identify the relevant docu-\nments among all documents in a collection, rather than just among a query-dependent\nsample; these systems represent a new type of retrieval approaches called dense re-\ntrieval systems. Thus far, two different families of representations have emerged in\ndense retrieval. In single representations systems, queries and documents are repre-\nsented with a single embedding, as discussed in Section 3.1. In multiple representations\nsystems, queries and/or documents are represented with more than a single embed-\nding, as discussed in Section 3.2. Section 3.3 discusses how representation-focused\nsystems are fine-tuned, exploiting noise-contrastive estimation.\n3.1 Single Representations\nIn interaction-focused systems discussed in Section 2, the query and document texts\nare concatenated together before processing with sequence-to-sequence models, yield-\ning rich interactions between the query context and the document context, as every\nword in the document can attend to every word in the query, and vice-versa. At query\nprocessing time, every document must be concatenated with the query and must be\nprocessed with a forward pass of the whole sequence-to-sequence model. Even if\nsome techniques such as the pre-computation of some internal representations have\nbeen proposed [MacAvaney et al. 2020b], interaction-focused systems cannot scale to a\nlarge amount of documents. In fact, on a standard CPU, the processing of a query over\nthe whole document collection exploiting an inverted index requires a few millisec-\nonds [Tonellotto et al. 2018], while computing the relevance score of a single query-\ndocument pair with a transformer model may requires a few seconds [MacAvaney\net al. 2019].\nInstead of leveraging sequence-to-sequence models to compute a semantically richer\nbut computationally expensive interaction representation η(q, d), representation-focused\nsystems employ encoder-only models to independently compute query representa-\ntions φ(q) and document representations ψ(d) in the same latent vector space [Ur-\nbanek et al. 2019], as illustrated in Figure 6. Next, the relevance score between the\nrepresentations is computed via an aggregation function f between these representa-\ntions:\nφ(q) = [φ[CLS], φ1, . . . , φ|q|] = Encoder(q)\nψ(d) = [ψ[CLS], ψ1, . . . , ψ|d|] = Encoder(d)\ns(q, d) = f (φ(q), ψ(d))\n(8)\nIn neural IR, the representation functions φ and ψ are computed through fine-tuned\nencoder-only sequence-to-sequence models such as BERT. The same neural model is\nused to compute both the query and the document representations, so the model is\nalso called dual encoder or bi-encoder [Bromley et al. 1993]. A bi-encoder maps queries\nand documents in the same vector space R`, in such a way that the representations\ncan be mathematically manipulated. Usually, the output embedding corresponding to\nthe [CLS] token is assumed to be the representation of a given input text. Using these\nsingle representations, the score aggregation function is the dot product:\ns(q, d) = φ[CLS] ·ψ[CLS] (9)\nDifferent single-representation systems have been proposed: DPR [Karpukhin et al.\n2020], ANCE [Xiong et al. 2021], and STAR [Zhan et al. 2021b] being the most widely\nadopted. The main difference among these systems is how the fine-tuning of the BERT\nmodel is carried out, as discussed in Section 3.3.\n3.2 Multiple Representations\nUp so far, we considered representation-focused systems in which queries and doc-\numents are represented through a single embedding in the latent vector space. This\nsingle representation is assumed to incorporate the meaning of an entire text within\nthat single embedding.\nIn contrast, multiple representation systems such as poly-encoders [Humeau et al. 2019],\nME-BERT [Luan et al. 2021], ColBERT [Khattab and Zaharia 2020] and COIL [Gao et al.\n2021] exploit more than a single embedding to represent a given text, which may allow\na richer semantic representation of the content.\nInstead of using just the first output embedding ψ[CLS] = ψ0 to encode a document d,\npoly-encoders [Humeau et al. 2019] exploit the first m output embeddings ψ0, ψ1, . . . , ψm−1.\nA query q is still represented with the single embedding φ[CLS] = φ0, while we need to\naggregate the m output document embeddings into a single representation ψ∗ to com-\npute the final relevance score using the dot product with the output query embedding.\nTo do so, poly-encoders first compute the m similarities s0, . . . , sm−1 between the query\nembedding and the first m document embedding using the dot product. These similar-\nities are transformed into normalised weights v0, . . . , vm−1 using a softmax operation,\nand the weighted output embeddings are summed up to compute the final document\nembedding ψ∗ used to compute the relevance score:\n[φ0, φ1, . . .] = Encoder(q)\n[ψ0, ψ1, . . .] = Encoder(d)\n[s0, s1, . . . , sm−1] = [φ0 ·ψ0, φ0 ·ψ1, . . . , φ0 ·ψm−1]\n[v0, v1, . . . , vm−1] = softmax([s0, s1, . . . , sm−1])\nψ∗ =\nm−1\n∑i=0\nvi ψi\ns(q, d) = φ0 ·ψ∗\n(10)\nSimilarly to poly-encoders, ME-BERT [Luan et al. 2021] exploits the first m output\nembeddings to represent a document d (including the [CLS] embedding), but uses a\ndifferent strategy to compute the relevance score s(q, d) w.r.t. a query q. ME-BERT\ncomputes the similarity between the query embedding and the first m document em-\nbedding using the dot product, and the maximum similarity, also called maximum\ninner product, represents the relevance score:\n[φ0, φ1, . . .] = Encoder(q)\n[ψ0, ψ1, . . .] = Encoder(d)\ns(q, d) = maxi=0,...,m−1 φ0 ·ψi\n(11)\nThis relevance scoring function, called max similarity or maxsim, allows us to exploit ef-\nficient implementations of maximum inner product search systems, discussed in Sec-\ntion 4. On the contrary, the relevance scoring function in Eq. (10), based on a softmax\noperation, does not permit to decompose the relevance scoring to a maximum compu-\ntation over dot products.\nDifferently from poly-encoders and ME-BERT, ColBERT [Khattab and Zaharia 2020]\ndoes not limit to m the number of embeddings used to represent a document. In-\nstead, it uses all the 1 + |d| output embeddings to represent a document, i.e., one\noutput embedding per document token, including the [CLS] special token. More-\nover, also a query q is represented with multiple 1 + |q|output embeddings, i.e., one\noutput embedding per query token, including the [CLS] special token. As in other\nrepresentation-focused systems, query token embeddings are computed at query pro-\ncessing time; queries may also be augmented with additional masked tokens to provide\n“a soft, differentiable mechanism for learning to expand queries with new terms or to re-weigh\nexisting terms based on their importance for matching the query” [Khattab and Zaharia\n2020]. In current practice, queries are augmented up to 32 query token embeddings.\nWithout loss of generality, query and documents embeddings can be projected in a\nsmaller latent vector space through a learned weight matrix W ∈R`′×`, with `′ < `.\nSince there are multiple query embeddings, ColBERT exploits a modified version of\nthe relevance scoring function in Eq (11), where every query embedding contributes\nto the final relevance score by summing up its maximum dot product value w.r.t. every\ndocument embeddings:\n[φ0, φ1, . . .] = Encoder(q)\n[ψ0, ψ1, . . .] = Encoder(d)\ns(q, d) =\n|q|\n∑i=0\nmaxj=0,...,|d|φi ·ψj\n(12)\nColBERT’s late interaction scoring in Eq. (12), also called sum maxsim, performs an\nall-to-all computation: each query embedding, including the masked tokens’ embed-\ndings, is dot-multiplied with every document embedding, and then the maximum\ncomputed dot products for each query embedding are summed up. In doing so, a\nquery term can contribute to the final scoring by (maximally) matching a different lex-\nical token. A different approach is proposed by the COIL system [Gao et al. 2021]. In\nCOIL, the query and document [CLS] embeddings are linearly projected with a learned\nmatrix WC ∈ R`×`. The embeddings corresponding to normal query and document\ntokens are projected into a smaller vector space with dimension `′ < `, using another\nlearned matrix WT ∈R`′×`. Typical values for `′ range from 8 to 32.\nThe query-document relevance score is the sum of two components. The first compo-\nnent is the dot product of the projected query and document [CLS] embeddings, and\nthe second component is the sum of sub-components, one per query token. Each sub\ncomponent is the maximum inner product between a query token and the document\nembeddings for the same token:\n[φ0, φ1, . . .] = Encoder(q)\n[ψ0, ψ1, . . .] = Encoder(d)\n[φ′0, φ′1, . . .] = [WC φ0, WT φ1, . . .]\n[ψ′0, ψ′1, . . .] = [WC ψ0, WT ψ1, . . .]\ns(q, d) = φ′0 ·ψ′0 + ∑ti ∈q\nmaxtj ∈d, tj =ti\nφ′i ·ψ′j\n(13)\nThe COIL’s scoring function, based on lexical matching between query and document\ntokens, allows us to pre-compute the projected document embeddings and, for each\nterm in the vocabulary, to concatenate together the embeddings in the same document\nand in the whole collection, organising them in posting lists of embeddings, including\na special posting list for the [CLS] token and its document embeddings. This organ-\nisation permits the efficient processing of posting lists at query time with optimised\nlinear algebra libraries such as BLAS [Blackford et al. 2002]. Note that the projected\nquery embeddings are still computed at query processing time.\n3.3 Fine-tuning Representation-focused Systems\nThe fine-tuning of a bi-encoder corresponds to learning an appropriate inner-product\nfunction suitable for the ad-hoc ranking task, i.e., for relevance scoring. As in Sec-\ntion 2.5, we have a neural IR model M(θ), parametrised by θ, that computes a score\nsθ (q, d) for a document d w.r.t. a query q. We now frame the learning problem as\na probability estimation problem. To this end, we turn the scoring function into a\nproper conditional distribution by using a softmax operation:\npθ (d|q) = exp (sθ (q, d))\n∑d′∈D exp (sθ (q, d′)) (14)\nwhere pθ (d|q) represents the posterior probability of the document being relevant\ngiven the query. We assume to have a joint distribution p over D×Q, and we want to\nfind the parameters θ∗ that minimise the cross entropy lCE between the actual proba-\nbility p(d|q) and the model probability pθ (d|q):\nθ∗ = arg minθ E\n[\nlCE (d, q)\n]\n= arg minθ E\n[\n−log (pθ (d|q))]\n(15)\nwhere the expectation is computed over (d, q) ∼ p. If the scoring function sθ (q, d) is\nexpressive enough, then, for some θ, we have p(d|q) = pθ (d|q).\nThe cross entropy loss in Eq. (15) is difficult to optimise, since the number of doc-\numents in D is large, and then the denominator in Eq. (14), also known as partition\nfunction, is expensive to compute. In noise contrastive estimation we choose an artificial\nnoise distribution g over Dof negative samples and maximise the likelihood of pθ (d|q)\ncontrasting g(d). Given k ≥ 2 documents Dk = {d1, . . . , dk }, for each of them we\ndefine the following conditional distribution:\nˆpθ (di |q, Dk ) = exp (sθ (q, di ))\n∑d′∈Dk exp (sθ (q, d′)) (16)\nwhich is significantly cheaper to compute that Eq. (14) if k << |D|. Now, we want\nto find the parameters θ† that minimise the noise contrastive estimation loss lNCE,\ndefined as:\nθ† = arg minθ E\n[\nlNCE (Dk, q)\n]\n= arg minθ E\n[\n−log (ˆpθ (d1|q, Dk ))]\n(17)\nwhere the expectation is computed over (d1, q) ∼ p and di ∼ g for i = 2, . . . , k.\nThe end goal of this fine-tuning is to learn a latent vector space for query and docu-\nment representations where a query and its relevant document(s) are closer, w.r.t. the\ndot product, than the query and its non-relevant documents [Karpukhin et al. 2020];\nthis fine-tuning approach is also called contrastive learning [Huang et al. 2013].\nNegative samples are drawn from the noise distribution g over D. In the following,\nwe list some negative sampling strategies adopted in neural IR.\n• Random sampling: any random document from the corpus is considered non-\nrelevant, with equal probability, i.e., q(d) = 1/|D|. Any number of negative doc-\numents can be sampled. Intuitively, it is reasonable to expect that a randomly-\nsampled document will obtain a relevance score definitely smaller than the rel-\nevance score of a positive document, with a corresponding loss value close to 0.\nNegative documents with near zero loss contribute little to the training conver-\ngence to identify the parameters θ† [Johnson and Guestrin 2018, Katharopoulos\nand Fleuret 2018].\n• In-batch sampling: during training, the queries to compute the loss can be ran-\ndomly aggregated into batches of size b, for faster training. For each query in\na given batch, the positive passages for the other b −1 queries are considered\nas negative passages for the query [Gillick et al. 2019]. This sampling approach\nsuffers from the same near zero loss problem as random sampling [Xiong et al.\n2021], but the sampling prodedure is faster.\n• Hard negative sampling: negative documents can be generated exploiting a clas-\nsical or trained retrieval system. Each query is given as input to the retrieval\nsystem, the top documents retrieved, and the documents not corresponding to\nthe positive ones are treated as negatives. Note that in this case we are assuming\na conditional noise distribution p(d|q, d1), since we assume to know the relevant\ndocument d1 for the query. In doing so, high-ranking documents are prioritised\nw.r.t. low-ranking documents, that do not impact on the user experience and do\nnot contribute to the loss. The retrieval system used to mine the negative docu-\nments can exploit BM25 relevance model, as in DPR [Karpukhin et al. 2020], the\ncurrently neural model under training, as in ANCE [Xiong et al. 2021], or another\nfine-tuned neural model, as in STAR [Zhan et al. 2021b]."
        },
        "Retrieval Architectures and Vector Search": {

            "content":"This section illustrates how the neural IR systems discussed so far are actually deployed in end-to-end systems. Section 4.1 discusses how cross-encoders and bi-encoders\nare deployed in ranking architectures. Since dense retrieval systems pre-computed the\ndocument embeddings, many actual systems focus on storing and searching through\ndocument embeddings. Section 4.2 formally defines the search problems in vector\nspaces and the embedding index, while Sections 4.3, 4.4, and 4.5 illustrate different\nsolutions for efficiently storing and searching vectors. Section 4.6 discusses some optimisations for embedding indexes specifically tailored for dense retrieval systems.\n4.1 Retrieval architectures\nPre-trained language models successfully improve the effectiveness of IR systems in\nthe ad-hoc ranking task, but they are computationally very expensive. Due to these\ncomputational costs, the interaction-focused systems are not applied directly on the\ndocument collection, i.e., to rank all documents matching a query. They are deployed\nin a pipelined architecture (Figure 7) by conducting first a preliminary ranking stage to\nretrieve a limited number of candidates, typically 1000 documents, before re-ranking\nthem with a more expensive neural re-ranking system, such as cross-encoders described in Section 2.\nThe most important benefit of bi-encoders discussed in Section 3 is the possibility to\npre-compute and cache the representations of a large corpus of documents with the\nlearned document representation encoder ψ(d). At query processing time, the learned\nquery representation encoder must compute only the query representation φ(q), then\nthe documents are ranked according to the inner product of their representation with\nthe query embedding, and the top k documents whose embeddings have the largest\ninner product w.r.t. the query embedding are returned to the user (Figure 8).\n4.2 MIP and NN Search Problems\nThe pre-computed document embeddings are stored in a special data structure called\nindex. In its simplest form, this index must store the document embeddings and provide a search algorithm that, given a query embedding, efficiently finds the document\nembedding with the largest dot product, or, more in general, with the maximum inner\nproduct.\nFormally, let φ ∈R` denote a query embedding, and let Ψ= {ψ1, . . . , ψn }denote a set\nof n document embeddings, with ψi ∈ R` for i = 1, . . . , n. The goal of the maximum\ninner product (MIP) search is to find the document embedding ψ∗ ∈ X such that\nψ∗ = arg maxψ∈Ψ〈φ, ψ〉 (18)\nA data structure designed to store Ψis called embedding index. The na ̈ıve embedding\nindex is the flat index, which stores the document embeddings in Ψexplicitly and performs an exhaustive search to identify ψ∗. Its complexity is O(n`) both in space and\ntime, so it is particularly inefficient for large n or ` values.\nA common approach to improve the space and time efficiency of the flat index is to\nconvert the maximum inner product search into a nearest neighbour (NN) search, whose\ngoal is to find the document embedding ψ† such that\nψ† = arg minψ∈Ψ‖φ −ψ‖ (19)\nMany efficient index data structures exist for NN search. To leverage them with embedding indexes, MIP search between embeddings must be adapted to use the Euclidean distance and NN search. This is possible by applying the following transformation from R` to R`+1 [Bachrach et al. 2014, Neyshabur and Srebro 2015]:\nˆφ =\n[\nφ/‖φ‖\n0\n]\n, ˆψ =\n[\nψ/M√1 −‖ψ‖2/M2\n]\n, (20)\nwhere M = maxψ∈Ψ‖ψ‖. By using this transformation, the MIP search solution ψ∗\ncoincides with the NN search solution ˆψ†. In fact, we have:\nmin ‖ˆφ − ˆψ‖2 = min (‖ˆφ‖2 + ‖ˆψ‖2 −2〈ˆφ, ˆψ〉) = min (2 −2〈φ, ψ/M〉) = max〈φ, ψ〉.\nHence, hereinafter we consider the MIP search for ranking with a dense retriever as\na NN search based on the Euclidean distance among the transformed embeddings ˆφ\nand ˆψ in R`+1. To simplify the notation, from now on we drop the hat symbol from\nthe embeddings, i.e., ˆφ → φ and ˆψ → ψ, and we consider ` + 1 as the new dimension\n`, i.e., ` + 1 →`.\nThe index data structures for exact NN search in low dimensional spaces have been\nvery successful, but they are not efficient with high dimensional data, as in our case,\ndue to the curse of dimensionality. It is natural to make a compromise between search\naccuracy and search speed, and the most recent search methods have shifted to approximate nearest neighbor (ANN) search.\nThe ANN search approaches commonly used in dense retrieval can be categorised\ninto three families: locality sensitive hashing approaches, quantisation approaches,\nand graph approaches.\n4.3 Locality sensitive hashing approaches\nLocality sensitive hashing (LSH) [Indyk and Motwani 1998] is based on the simple idea\nthat, if two embeddings are close together, then after a “projection”, using an hash\nfunction, these two embeddings will remain close together. This requires that:\n• for any two embeddings ψ1 and ψ2 that are close to each other, there is a high\nprobability p1 that they fall into the same hash bucket;\n• for any two embeddings ψ1 and ψ2 that are far apart, there is a low probability\np2 < p1 that they fall into the same hash bucket.\nThe actual problem to solve is to design a family of LSH functions fulfilling these\nrequirements. LSH functions have been designed for many distance metrics. For\nthe euclidean distance, a popular LSH function h(ψ) is the random projection [Datar\net al. 2004].A set of random projections defines a family of hash functions Hthat can\nbe used to build a data structure for ANN search. First, we sample m hash functions h1(ψ), . . . , hm (ψ) independently and uniformly at random from H, and we define the function family G = {g : R` → Zm }, where g(ψ) = (h1(ψ), . . . , hm (ψ)),\ni.e., g is the concatenation of m hash functions from H. Then, we sample r functions\ng1(ψ), . . . , gr (ψ) independently and uniformly at random from G, and each function\ngi is used to build a hash table Hi .\nGiven the set of document embeddings Ψand selected the values of the parameters\nr and m, an LSH index is composed of r hash tables, each containing m concatenated\nrandom projections. For each ψ ∈ Ψ, ψ is inserted in the gi (ψ) bucket for each hash\ntable Hi , for i = 1, . . . , r. At query processing time, given a query embedding, we first\ngenerate a candidate set of document embeddings by taking the union of the contents\nof all r buckets in the r hash tables the query is hashed to. The final NN document\nembedding is computed performing an exhaustive exact search within the candidate\nset.\nThe main drawback of the LSH index is that it may require a large number of hash tables to cover most nearest neighbors, and it requires to store the original embeddings\nto perform the exhaustive exact search. Although some optimisations have been proposed [Lv et al. 2007], the space consumption may be prohibitive with very large data\nsets.\n4.4 Vector quantisation approaches\nInstead of random partitioning the input space Ψas in LSH, the input space can be\npartitioned according to the data distribution. By using the k-means clustering algorithm on Ψwe can compute k centroids μ1, . . . , μk, with μi ∈R` for i = 1, . . . , k that can\nbe used to partition the input space Ψ. The set M = {μ1, . . . , μk }is called a codebook.\nGiven a codebook M, a vector quantiser q : R` → R` maps a vector ψ to its closest\ncentroid:\nq(ψ) = arg minμ∈M ‖ψ −μ‖ (21)\nGiven a codebook M, an IVF (Inverted File) index built over M and Ψstores the set\nof document embeddings Ψin k partitions or inverted lists L1, . . . , Lk, where Li = {ψ ∈\nΨ: q(ψ) = μi }. At query processing time, we specify to search for the NN document\nembeddings in p > 0 partitions. If p = k, the search is exhaustive, but if p < k, the\nsearch is carried out in the partitions whose centroid is closer to the query embedding.\nIn doing so the search is not guaranteed to be exact, but the search time can be sensibly\nreduced. In fact, an IVF index does not improve the space consumption, since it still\nneeds to store all document embeddings, but it can reduce the search time depending\non the number of partitions processed for each query.\nA major limitation of IVF indexes is that they can require a large number of centroids [Gersho and Gray 1992]. To address this limitation, product quantisation [J ́egou\net al. 2011] divides each vector ψ ∈ Ψinto m sub-vectors ψ = [ψ1|ψ2|···|ψm ]. Each\nsub-vector ψj ∈R`/m with j = 1, . . . , m is quantised independently using its own subvector quantiser qj. Each vector quantiser qj has its own codebook Mj = {μj,1, . . . , μj,k }.\nGiven the codebooks M1, . . . , Mm, a product quantiser pq : R` → R` maps a vector ψ\ninto the concatenation of the centroids of its sub-vector quantisers:\npq(ψ) = [q1(ψ1)|q2(ψ2)|···|qm (ψm )] = [μ1,i1 |μ2,i2 |. . . |μm,im ] (22)\nNote that a product quantiser can output any of the km centroid combinations in M1 ×\n. . . ×Mm.\nA PQ (Product Quantization) index stores, for each embedding ψ ∈ Ψ, its encoding\ni1, . . . , im, that requires m log k bits of storage. At query processing time, the document\nembeddings are processed exhaustively. However the distance computation between\na query embedding φ and a document embedding ψ is carried out using the product\nquantisation of the document embedding pq(x):\n‖ψ −φ‖2 ≈‖pq(ψ) −φ‖2 =\nm\n∑j=1\n‖qj (ψj ) −φj ‖2 (23)\nTo implement this computation, m lookup tables are computed, one per sub-vector\nquantiser: the j-th table is composed of the squared distances between the j-th subvector of φ, and the centroids of Mj. These tables can be used to quickly compute the\nsums in Eq. (23) for each document embedding.\nANN search on a PQ index is fast, requiring only m additions, and memory efficient,\nbut it is still exhaustive. To avoid it, an IVFPQ index exploits inverted files and product\nquantisation jointly. Firstly, a coarse quantiser partitions the input dataset into inverted\nlists, for a rapid access to small portions of the input data. In a given inverted list, the\ndifference between each input data and the list centroid, i.e., the input residual, is encoded with a product quantiser. In doing so, the exhaustive ANN search can be carried\nout only in a limited number of the partitions computed by the coarse quantiser.\n4.5 Graph approaches\nThe distances between vectors in a dataset can be efficiently stored in a graph-based\ndata structure called kNN graph. In a kNN graph G = (V, E), each input data ψ ∈ Ψ\nis represented as a node v ∈ V, and, for its k nearest neighbours, a corresponding\nedge is added in E. The computation in an exact kNN graph requires O(n2) similarity computation, but many approximate variants are available [Dong et al. 2011]. To\nsearch for an approximate nearest neighbour to an element φ using a kNN graph, a\ngreedy heuristic search is used. Starting from a predefined entry node, the graph is visited one node at a time, keeping on finding the closest node to φ among the unvisited\nneighbour nodes. The search terminates when there is no improvement in the current\nNN candidate. In practice, several entry nodes are used together with a search budget to avoid local optima. For a large number of nodes, the greedy heuristic search\non the kNN graph becomes inefficient, due to the long paths potentially required to\nconnect two nodes. Instead of storing only short-range edges, i.e., edges connecting\ntwo close nodes, the kNN graph can be enriched with randomly generated long-range\nedges, i.e., edges connecting two randomly-selected nodes. This kind of kNN graph is\na navigable small world (NSW) graph [Malkov et al. 2013], for which the greedy search\nheuristic is theoretically and empirically efficient [Kleinberg 2000].\nA hierarchical NSW (HNSW) index stores the input data into multiple NSW graphs.\nThe bottom layer graph contains a node for each input element, while the number of\nnodes in the other graphs decreases exponentially at each layer. The search procedure\nfor approximate NN vectors starts with the top layer graph. At each layer, the greedy\nheuristic searches for the closest node, then the next layer is searched, starting from the\nnode corresponding to the closest node identified in the preceding graph. At the bottom layer, the greedy heuristic searches for the k closest nodes to be returned [Malkov\nand Yashunin 2020].\n4.6 Optimisations\nImplementations of the embedding indexes presented in the previou sections are available in many open-source production-ready search engines such as Lucene3 and Vespa4.\nIn the IR research community, FAISS is the most widely adopted framework for embedding indexes [Johnson et al. 2021]. Among others, FAISS includes implementations\nof flat, LSH, IVF, PQ, IVFPQ and HNSW indexes.\nSingle representation systems such as DPR [Karpukhin et al. 2020], ANCE [Xiong et al.\n2021], and STAR [Zhan et al. 2021b] use flat indexes. In these cases, it is unfeasible to\nadopt product quantisation indexes due to their negative impact on IR-specific metrics, mainly caused by the separation between the document encoding and embedding\ncompression phases. To overcome this limitation, several recent techniques such as Poemm [Zhang et al. 2021], JPQ [Zhan et al. 2021a] and RepCONC [Zhan et al. 2022] aim\nto train at the same time both phases. In doing so, during training, these techniques\nlearn together the query and document encoders together while performing product\nquantisation.\nMultiple representation systems such as ColBERT [Khattab and Zaharia 2020] are characterised by a very large number of document embeddings. They do not use flat indexes, due to unacceptable efficiency degradation of brute-force search, and exploit\nIVFPQ indexes and ANN search. With these indexes, the document embeddings are\nstored in a quantised form, suitable for fast searching. However, the approximate similarity scores between these compressed embeddings are inaccurate, and hence are not\nused for computing the final top documents. Indeed, in a first stage, ANN search computes, for each query embedding, the set of the k′ most similar document embeddings;\nthe retrieved document embeddings for each query embedding are mapped back to\ntheir documents. These documents are exploited to compute the final list of top k documents in a second stage. To this end, the set of documents computed in the first stage\nis re-ranked using the query embeddings and the documents’ multiple embeddings\nto produce exact scores that determine the final ranking, according to the relevance\nfunction in Eq. (12) (see Figure 9.\nFurther optimisations can reduce the number of query embeddings to be processed\nin the first stage [Tonellotto and Macdonald 2021], or the number of documents to be\nprocessed in the second stage [Macdonald and Tonellotto 2021]."
        },
        "Learned Sparse Retrieval": {
            "content": "Traditional IR systems are based on sparse representations, inverted indexes and lexicalbased relevance scoring functions such as BM25. In industry-scale web search, BMis a widely adopted baseline due to its trade-off between effectiveness and efficiency.\nOn the other side, neural IR systems are based on dense representations of queries and\ndocuments, that have shown impressive benefits in search effectiveness, but at the cost\nof query processing times. In recent years, there have been some proposals to incorporate the effectiveness improvements of neural networks into inverted indexes, with\ntheir efficient query processing algorithms, through learned sparse retrieval approaches.\nIn learned sparse retrieval the transformer architectures are used in different scenarios:\n• document expansion learning: sequence-to-sequence models are used to modify\nthe actual content of documents, boosting the statistics of the important terms\nand generating new terms to be included in a document;\n• impact score learning: the output embeddings of documents provided as input to\nencoder-only models are further transformed with neural networks to generate a\nsingle real value, used to estimate the average relevance contribution of the term\nin the document;\n• sparse representation learning: the output embeddings of documents provided as\ninput to encoder-only models are projected with a learned matrix on the collection vocabulary, in order to estimate the relevant terms in a document. These\nrelevant terms can be part of the documents or not, hence representing another\nform of document enrichment.\nIn Sections 5.1, 5.2, and 5.3, we describe the main existing approaches in these scenarios, respectively.\n5.1 Document expansion learning\nDocument expansion techniques address the vocabulary mismatch problem [Zhao\n2012]: queries can use terms semantically similar but lexically different from those\nused in the relevant documents. Traditionally, this problem has been addressed using\nquery expansion techniques, such as relevance feedback [Rocchio 1971] and pseudo\nrelevance feedback [Lavrenko and Croft 2001]. The advances in neural networks and\nnatural language processing have paved the way to different techniques to address the\nvocabulary mismatch problem by expanding the documents by learning new terms.\nDoc2Query [Nogueira et al. 2019b] and DocT5Query [Nogueira and Lin 2019] showed\nfor the first time that transformer architectures can be used to expand the documents’\ncontent to include new terms or to boost the statistics of existing termw. Both approaches focus on the same task, that is, generating new queries for which a specific\ndocument will be relevant. Given a dataset of query and relevant document pairs,\nDoc2Query fine-tunes a sequence-to-sequence transformer model [Vaswani et al. 2017],\nwhile DocT5Query fine-tunes the T5 model [Raffel et al. 2020] by taking as input the relevant document and generating the corresponding query. Then, the fine-tuned model\nis used to predict new queries using top k random sampling [Fan et al. 2018a] to enrich\nthe document by appending these queries before indexing, as illustrated in Figure 10.\nInstead of leveraging the encoder-decoder models for sentence generation and finetune them on document expansion, a different approach computes the importance of\nall terms in the vocabulary w.r.t. a given document and selects the most important\nnew terms to enrich the document, leveraging an encoder-only architecture to compute the document embeddings. TILDEv2 [Zhuang and Zuccon 2021b] exploits the\nBERT model to compute the [CLS] output embedding of a document, and linearly\nprojects it over the whole BERT vocabulary. In doing so, TILDEv2 computes a probability distribution over the vocabulary, i.e., a document language model, and then\nadds to the document a certain number of new terms, corresponding to those with the\nhighest probabilities. As another way of expanding documents, SparTerm [Bai et al.\n2020] computes a document language model for each BERT output token, including\n[CLS], and sums them up to compute the term importance distribution over the vocabulary for the given document. Finally, a learned gating mechanism only keeps a sparse\nsubset of those, to compute the final expanded document contents.\n5.2 Impact score learning\nClassical inverted indexes store statistical information on term occurrences in documents in posting lists, one per term in the collection. Every posting list stores a posting for each document in which the corresponding term appears in, and the posting\ncontains a document identifier and the in-document term frequency, i.e., a positive integer counting the number of occurrences of the term in the document. When a new\nquery arrives, the posting lists of the terms in the query are processed to compute the\ntop scoring documents, using a classical ranking function, such as BM25, and efficient\nquery processing algorithms [Tonellotto et al. 2018].\nThe goal of impact score learning is to leverage the document embeddings generated\nby an encoder-only model to compute a single integer value to be stored in postings,\nand to be used as a proxy of the relevance of the term in the corresponding posting, i.e.,\nits term importance. The simplest way to compute term importance in a document is to\nproject the document embeddings of each term with a neural network into a singlevalue representation, filtering out negative values with ReLU functions and discarding\nzeros. To save space, the real values can be further quantised into a 8-bit positive integers. A common problem in impact score learning is the vocabulary to use. Since most\nencoder-only models use a sub-word tokeniser, the collection vocabulary can be constructed in two different ways: by using the terms produced by the encoder-specific\nsub-word tokeniser, e.g., by BERT-like tokenisers, or by using the terms produced by\na word tokeniser. These two alternatives have an impact on the final inverted index:\nin the former case, we have fewer terms, but longer and denser posting lists, while in\nthe latter case, we have more terms, with shorter posting lists and with smaller query\nprocessing times [Mallia et al. 2022].\nThe current impact score learning systems are DeepCT [Dai and Callan 2019b, 2020],\nDeepImpact [Mallia et al. 2021], TILDEv2 [Zhuang and Zuccon 2021a,b], and UniCOIL [Lin\nand Ma 2021].\nDeepCT [Dai and Callan 2019b, 2020] represents the first example of term importance\nboosting. DeepCT exploits the contextualised word representations from BERT to learn\nnew in-document term frequencies, to be used with classical ranking functions such\nas BM25. For each term wi ∈ V in a given document, DeepCT estimates its contextspecific importance zi ∈ R, that is then scaled and rounded as frequency-like integer\nvalue t fi that can be stored in an inverted index. Formally, for each document d ∈ D,\nDeepCT projects the `-dimensional representations ψi for each input BERT token wi\nin the document, with i = 1, . . . , |d|, into a scalar term importance with the learned\nmatrix W ∈R1×`:\n[ψ0, ψ1, . . .] = Encoder(d)\nzi = Wψi\n(24)\nDeepCT is trained with a per-token regression task, trying to predict the importance\nof the terms. The actual term importance to predict is derived from the document\ncontaining the term, or from a training set of query, relevant document pairs. A term\nappearing in multiple relevant documents and in different queries has a higher importance than a term matching fewer documents, and/or fewer queries. To handle\nBERT’s sub-word tokens, DeepCT uses the importance of the first sub-word token for\nthe entire word, and when a term occurs multiple times in the document, it takes the\nmaximum importance across the multiple occurrences.\nDeepImpact [Mallia et al. 2021] proposes for the first time to directly compute an impact score for each unique term in a document, without resorting to classical ranking\nfunctions, but simply summing up, at query processing time, the impacts of the query\nterms appearing in a document to compute its relevance score. For each term wi ∈ V\nin a given document d ∈ D, DeepImpact estimates its context-specific impact zi ∈ R.\nDeepImpact feeds the encoder-only model with the document sub-word tokens, producing an embedding for each input token. A non-learned gating layer Mask removes\nthe embeddings of the sub-word tokens that do not correspond to the first sub-token of\nthe whole word. Then DeepImpact transforms the remaining `-dimensional representations with two feed forward networks with ReLU activations. The first network has\na weight matrix W1 ∈R`×`, and the second network has a weight matrix W2 ∈R1×`:\n[ψ0, ψ1, . . .] = Encoder(DocT5Query(d))\n[x0, x1, . . .] = Mask(ψ0, ψ1, . . .)\nyi = ReLU(W1 xi )\nzi = ReLU(W2yi )\n(25)\nThe output real numbers zi , with i = 1, . . . , |d|, one per whole word in the input document, are then linearly quantised into 8-bit integers that can be stored in an inverted\nindex. This produces a single-value score for each unique term in the document, representing its impact. Given a query q, the score of the document d is simply the sum\nof impacts for the intersection of terms in q and d. DeepImpact is trained with query,\nrelevant document, non-relevant document triples, and, for each triple, two scores for\nthe corresponding two documents are computed. The model is optimized via pairwise cross-entropy loss over the document scores. Moreover, DeepImpact has been the\nfirst sparse learned model leveraging at the same time documents expansion learning and impact score learning. In fact, DeepImpact leverages DocT5Query to enrich the\ndocument collection before learning the term impact.\nTILDEv2 [Zhuang and Zuccon 2021b] computes the terms’ impact with an approach\nsimilar to DeepImpact. The main differences are (i) the use of a single layer feed forward\nnetwork with ReLU activations, instead of a two-layer network, to project the document embeddings into a single positive scalar value using a learned matrix W ∈R1×`,\n(ii) the use of its own document expansion technique, as discussed in Section 5.1, (iii)\nthe use of an index with sub-word terms instead of whole word terms, and (iv) the\nselection of the highest-valued impact score for a token if that token appears multiple\ntimes in a document:\n[ψ0, ψ1, . . .] = Encoder(TILDEv2(d))\nzi = ReLU(Wψi ) (26)\nThe zi scores are then summed up, obtaining an accumulated query-document score.\nUniCOIL [Lin and Ma 2021] exploits the COIL approach (see Sec. 3), but instead of projecting the query and document embeddings on 8-32 dimensions, it projects them to\nsingle-dimension query weights and document weights. In UniCOIL the query and\ndocument [CLS] embeddings are not used, and the embeddings corresponding to normal query and document tokens are projected into single scalar values v1, . . . , v|d| using a learned matrix W ∈ R1×`, with ReLU activations on the output term weights of\nthe base COIL model to force the model to generate non-negative weights.\n[φ0, φ1, . . .] = Encoder(q)\n[ψ0, ψ1, . . .] = Encoder(DocT5Query(d))\n[v1, v2, . . .] = [Wφ1, Wφ2, . . .]\n[z1, z2, . . .] = [Wψ1, Wψ2, . . .]\ns(q, d) = ∑ti ∈q\nmaxtj ∈d, tj =ti\nvi zj\n(27)\nThe document weights zi are then linearly quantised into 8-bit integers, and the final\nquery-document score is computed by summing up the highest valued document impact scores times its query weight vi, computed at query processing time, as in Eq. (27).\n5.3 Sparse representation learning\nInstead of independently learning to expand the documents and then learning the\nimpact score of the terms in the expanded documents, sparse representation learning\naims at learning both at the same time. At its core, sparse representation learning\nprojects the output embeddings of an encoder-only model into the input vocabulary,\ncompute, for each input term in the document, a language model, i.e., a probability\ndistribution over the whole vocabulary. These term-based language models capture\nthe semantic correlations between the input term and all other terms in the collection,\nand they can be used to (i) expand the input text with highly correlated terms, and (ii)\ncompress the input text by removing terms with low probabilities w.r.t. the other terms.\nEncoder-only models such as BERT already compute term-based language models,\nas part of their training as masked language models. Formally, given a document\nd, together with the output embeddings ψ[CLS], ψ1, . . . , ψ|d|, an encoder-only model\nalso returns the masked language heads χ1, . . . , χ|d|, one for each token in the document,\nwhere χi ∈R|V| for i = 1, . . . , |d|is an estimation of the importance of each word in the\nvocabulary implied by the i-th token in the document d. EPIC [MacAvaney et al. 2020a]\nand SparTerm [Bai et al. 2020] have been the first systems focusing on vocabulary-based\nexpansion and importance estimation, and inspired the SPLADE [Formal et al. 2021]\nsystem, on which we focus.\nFor a given document d ∈D, SPLADE computes its per-token masked language heads\nχ1, . . . , χ|d| using BERT, filters and sums up these vocabulary-sized vectors into a single vector γ(d) ∈ R|V| representing the whole document, and then uses this vector to\nrepresent the document itself, together with the term importance scores:\n[χ1, . . . , χ|d|] = Encoder(d)\nγ(d) =\n|d|\n∑i=1\nlog (1 + ReLU(χi )) (28)\nThe logarithm and ReLU functions in Eq. (28) are computed element-wise; the logarithm prevents some terms with large values from dominating, and the ReLU function\ndeals with the negative components of γ(d).\nThe document representation γ potentially contains all terms in the vocabulary, even\nif the logarithm and ReLU functions in Eq. (28) can zero out some of its components.\nTo learn to “sparsify” the document representations, Formal et al. [2021] leverage the\nFLOPS regulariser LFLOPS [Paria et al. 2020]. As part of the SPLADE loss function\nused during training, the FLOPS loss is computed as the sum, across the terms in the\nvocabulary, of the squared probability p2w that a term w has a non-zero weight in a document. Minimising the FLOPS loss coincides with minimising the non-zero weights in\na document, i.e., maximising the number of zero weights in a document. The square\noperation helps in reducing high term weights more than low term weights. The probability that a term w ∈V has a non-zero weight in a document d is proportional to the\naverage weight of that term γt (d) estimated through the whole collection. To make\nthe computation feasible, the average is computed on a batch b of documents during\ntraining, considered as a representative sample of the whole collection:\nLFLOPS = ∑t∈V\np2t = ∑t∈V\n( 1\n|b| ∑d∈b\nγt (d)\n)2\n(29)\nSPLADE does not limit expansion to documents only. Indeed, Eq. (28) can be applied\nto a query q as well, to compute the corresponding vector γ(q) ∈ R|V|. However,\nthis query expansion must be carried out at query processing time; to reduce the latency, the expanded query should be far more sparse than a document. To enforce\nthis different behaviour, Formal et al. [2021] adopt two distinct FLOPS regularisers for\ndocuments and queries, both as in Eq. (29)."
        },
        "conclusion": {
            "content": "This overview aimed to provide the foundations of neural IR approaches for ad-hoc\nranking, focusing on the core concepts and the most commonly adopted neural architecture in current research. In particular, in Section 1 we provided a background on\nthe different ways to represent texts, such as queries and documents, to be processed\nin IR systems. Sections 2 and 3 illustrated the main system architectures in neural\nIR, namely interaction-focused and representation-focused systems. In Section 4 we\nreviewed the multi-stage retrieval architectures exploiting neural IR systems, and we\nprovided a quick overview of the embedding indexes and algorithms for dense retrieval. Finally, in Section 5, we discussed the main state-of-the-art solutions for learning sparse representations.\nNeural IR systems are currently a hot research topic, and many excellent surveys complement and deepen the concepts discussed in this overview. The interested readers\ncan find a fully-fledged detailed presentation of pre-trained transformers for ranking,\nwith many experimental evaluations, in the recent book by [Lin et al. 2021], as well\nas applications of dense retrieval to conversational system in the books by Gao et al.\n[2022] and Zamani et al. [2022]."
        }

    }
}
