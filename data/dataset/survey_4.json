{

    "Lecture Notes on Neural Information Retrieval": {
        "introduction": {
            "content": "Text Information Retrieval (IR) systems focus on retrieving text documents able to ful- fill the information needs of their users, typically expressed as textual queries. Over the years, this inherently vague description has been formalised and characterised by the specific nature of documents, information needs, and users. At the core of the for- malisation lies the concept of relevance of a document with respect to a query, and how to estimate their relevance. Over the years, many different ranking models have been proposed to estimate the relevance of documents in response to a query. These mod- els depend on the information provided by the queries and the documents, that are exploited to derive “relevance signals”. Many ranking models have been developed over the years, ranging from Boolean models to probabilistic and statistical language models. These “bag of words” models leverage the presence or the number of occur- rences of query terms in the documents to infer their relevance to a query, exploiting hand-crafted functions to combine these occurrences such as BM25. With the rise of the Web and social platforms, more sources of relevance information about documents have been identified. Machine learning methods have been proved effective to deal with this abundance of relevance signals, and their application to rank the documents in order of relevance estimates w.r.t. a query has given birth to many learning-to-rank (LTR) models. Relevance signals are input features in LTR models, and they are often designed by hand, a time-consuming process. Motivated by their breakthroughs in many computer vision and natural language processing tasks, neural networks repre- sent the current state-of-the-art approach in ranking documents w.r.t. query relevance. Neural Information Retrieval focuses on retrieving text documents able to fulfill the information needs of their users exploiting deep neural networks. In neural IR, neural networks are typically used in two different ways: to learn the ranking functions com- bining the relevance signals to produce an ordering of documents, and to learn the abstract representations of documents and queries to capture their relevance informa- tion. In the following, we provide an introduction to the recent approaches in neural IR. Since the research in the field is rapidly evolving, we do not pretend to cover every single aspect of neural IR, but to provide a principled introduction to the main ideas and existing systems in the field. When available, we provide links to relevant and more detailed surveys. Here is a quick overview over what the sections are about. Section 1 provides a short depiction of the different representations for text adopted in IR, from the classical BOW encodings to learning-to-rank features to word embeddings. Section 2 presents the main neural architectures for computing a joint representation of query and document pairs for relevance ranking. Section 3 focuses on the neural architectures specif- ically tailored for learning abstract complex representations of query and documents texts independently. Section 4 overviews the deployment schemes adopted in neural IR systems, together with an overview of the most common dense retrieval indexes supporting exact and approximate nearest neighbour search. Section 5 discusses the current approaches in learned sparse retrieval, dealing with the learning of low di- mensional representations of documents amenable to be stored in an inverted index or similar data structures. Finally, Section 6 draws concluding remarks."
        },
        "Text Representations for Ranking": {
            "content": "According to the Probability Ranking Principle [Robertson 1977], under certain as-\nsumptions, for a given user’s query, documents in a collection should be ranked in\norder of the (decreasing) probability of relevance w.r.t. the query, to maximise the\noverall effectiveness of a retrieval system for the user. The task of ad-hoc ranking is, for\neach query, to compute an ordering of the documents equivalent or most similar to\nthe optimal ordering based on the probability of relevance. It is common to limit the\ndocuments to be ordered to just the top k documents in the optimal ordering.\nLet D denote a collection of (text) documents, and Q denote a log of (text) queries.\nQueries and documents share the same vocabulary V of terms. A ranking function,\nalso known as scoring function, s : Q×D → R computes a real-valued score for the\ndocuments in the collection D w.r.t. the queries in the log Q. Given a query q and a\ndocument d, we call the value s(q, d) relevance score of the document w.r.t. the query.\nFor a given query, the scores of the documents in the collection can be used to induce\nan ordering of the documents, in reverse value of score. The closer this induced or-\ndering is to the optimal ordering, the more effective an IR system based on the scoring\nfunction is.\nWithout loss of generality, the scoring function s(q, d) can be further decomposed as:\ns(q, d) = f (φ(q), ψ(d), η(q, d)), (1)\nwhere φ : Q → V1, ψ : D → V2, and η : Q×D → V3 are three representation func-\ntions, mapping queries, documents, and query-document pairs into the latent repre-\nsentation spaces V1, V2, and V3, respectively [Guo et al. 2020]. These functions build\nabstract mathematical representations of the text sequences of documents and queries\namenable for computations. The elements of these vectors represent the features used\nto describe the corresponding objects, and the aggregation function f : V1 ×V2 ×V3 →\nR computes the relevance score of the document representation w.r.t. the query repre-\nsentation.\nThe representation functions φ, ψ and η, and the aggregation function f can be de-\nsigned by hand, leveraging some axioms or heuristics, or computed through machine\nlearning algorithms. In the following, we will overview the representation functions\nadopted in classical IR (Section 1.1), in LTR scenarios (Section 1.2) and the recently\nproposed word embeddings (Section 1.3).",
            "subsections": {
                "BOW Encodings": {
                    "content": "In classical IR, both representation and aggregation functions are designed manually,\nincorporating some lexical statistics such as number of occurrences of terms in a doc-\nument or in the whole collection. Classical IR ranking models, e.g., vector space mod-\nels [Salton et al. 1975], probabilistic models [Robertson and Zaragoza 2009] and sta-\ntistical language models [Ponte and Croft 1998], are based on the bag of words (BOW)\nmodel, where queries and documents are represented as a set of terms from the vo-\ncabulary V together with the number of occurrences of the corresponding tokens in\nthe text. More formally, queries and documents are represented as vectors φ(q) and\nψ(d) in N|V|, called BOW encodings, where the i-th component of both representations\nencodes the number of occurrences of the term wi ∈ V in the corresponding text.\nThe query-document representation function η is not present in these ranking func-\ntions. The aggregation function f over these representations is an explicit formula\ntaking into account the components of the query and document representations, i.e.,\nthe in-query and in-document term frequencies, together with other document nor-\nmalisation operations. These representations are referred to as sparse representations,\nsince most of their components are equal to 0 because they correspond to tokens not\nappearing in the query/document. Sparse representations can be trivially computed\nand efficiently stored in specialised data structures called inverted indexes, which rep-\nresent the backbone of commercial Web search engine [Cambazoglu and Baeza-Yates\n2015]; see [B  ̈uttcher et al. 2010, Manning et al. 2008, Tonellotto et al. 2018] for more\ndetails on inverted indexes and classical IR ranking models."
                },
                "LTR Features": {
                    "content": "With the advent of the Web, new sources of relevance information about the docu-\nments have been made available. The importance of a Web page, e.g., PageRank,\nadditional document statistics, e.g., term frequencies in the title or anchors text, and\nsearch engine interactions, e.g., clicks, can be exploited as relevance signals. Moreover,\ncollaborative and social platforms such as Wikipedia, Twitter and Facebook represent\nnew sources of relevance signals. These relevance signals have been exploited to build\nricher query and document representations in LTR. The relevance signals extracted\nfrom queries and/or documents are called features. There are various classes of these\nfeatures [Bendersky et al. 2011, Macdonald et al. 2012], such as:\n• query-only features, i.e., components of φ(q): query features with the same value\nfor each document, such as query type, query length, and query performance\npredictors;\n• query-independent features, i.e., components of ψ(d): document features with the\nsame value for each query, such as importance score, URL length, and spam\nscore;\n• query-dependent features, i.e., components of η(q, d): document features that de-\npend on the query, such as different term weighting models on different fields.\nIn LTR, the representation functions are hand-crafted: exploiting the relevance signals\nfrom heterogeneous information sources, the different components of query and doc-\nument representations are computed with feature-specific algorithms. Hence, the rep-\nresentations φ(q), ψ(d), and η(q, d) are elements of vector spaces over R, but whose di-\nmensions depend on the number of hand-crafted query-only, query-independent, and\nquery-dependent features, respectively. Moreover the different components of these\nvectors are heterogeneous, and do not carry any specific semantic meaning. Using\nthese representations, in LTR the aggregation function f is machine-learned, for ex-\nample using logistic regression [Gey 1994], gradient-boosted regression trees [Burges\n2010] or neural networks [Burges et al. 2005]; see [Liu 2009] for a detailed survey."
                },
                "Word Embeddings":{
                    "content": "Both BOW encodings and LTR features are widely adopted in commercial search en-\ngines, but they suffer from several limitations. On the one hand, semantically-related\nterms end up having completely different BOW encodings. Although the two terms\ncatalogue and directory can be considered synonyms, their BOW encodings are com-\npletely different, with the single 1 appearing in different components. Similarly, two\ndifferent documents on a same topic can end up having two unrelated BOW encod-\nings. On the other hand, LTR features create text representations by hand via feature\nengineering, with heterogeneous components and no explicit concept of similarity.\nIn the 1950s, many linguists formulated the distributional hypothesis: words that occur\nin the same contexts tend to have similar meanings [Harris 1954]. According to this\nhypothesis, the meaning of words can be inferred by their usage together with other\nwords in existing texts. Hence, by leveraging the large text collections available, it is\npossible to learn useful representations of terms, and devise new methods to use these\nrepresentations to build up more complex representations for queries and documents.\nThese learned representations are vectors in Rn, with n |V|, called distributional rep-\nresentations or word embeddings. The number of dimensions n ranges approximatively\nfrom 50 to 1000 components, instead of the vocabulary size |V|. Moreover, the com-\nponents of word embeddings are rarely 0: they are real numbers, and can also have\nnegative values. Hence, word embeddings are also referred to as dense representations.\nAmong the different techniques to compute these representations, there are algorithms\nto compute global representations of the words, i.e., a single fixed embedding for each\nterm in the vocabulary, called static word embeddings, and algorithms to compute local\nrepresentations of the terms, which depend on the other tokens used together with\na given term, i.e., its context, called contextualised word embeddings. Static word em-\nbeddings used in neural IR are learned from real-world text with no explicit training\nlabels: the text itself is used in a self-supervised fashion to compute word represen-\ntations. There are different kinds of static word embeddings, for different languages,\nsuch as word2vec [Mikolov et al. 2013], fasttext [Joulin et al. 2017] and GloVe [Pennington\net al. 2014]. Static word embeddings map terms with multiple senses into an average\nor most common sense representation based on the training data used to compute the\nvectors; each term in the vocabulary is associated with a single vector. Contextualised\nword embeddings map tokens used in a particular context to a specific vector; each\nterm in the vocabulary is associated with a different vector every time it appears in\na document, depending on the surrounding tokens. The most popular contextualised\nword embeddings are learned with deep neural networks such as the Bidirectional En-\ncoder Representations from Transformers (BERT) [Devlin et al. 2019], the Robustly Op-\ntimized BERT Approach (RoBERTa) [Liu et al. 2019], and the Generative Pre-Training\nmodels (GPT) [Radford and Narasimhan 2018].\nIn neural IR, word embeddings are used to compute the representation functions φ,\nψ and η, and the aggregation function f through (deep) neural networks. Depend-\ning on the assumptions over the representation functions, the neural ranking models\ncan be classified in interaction-focused models and representation-focused models. In\ninteraction-focused models, the query-document representation function η(q, d), tak-\ning into account the interaction between the query and document contents, is explicitly\nconstructed and used as input to a deep neural network, or it is implicitly generated\nand directly used by a deep neural network. In representation-focused models, the\nquery-document representation function η(q, d) is not present; query and document\nrepresentations φ(q) and ψ(d) are computed independently by deep neural networks\nIn the following, we discuss the main interaction-focused models for ad-hoc ranking\n(Section 2) and representation-focused models for queries and documents (Section 3)."
                }
            }       
        },
        "Interaction-focused Systems" : {
            "content": "The interaction-focused systems used in neural IR model the word and n-gram re-\nlationships across a query and a document using deep neural networks. These sys-\ntems receive as input both a query q and a document d, and output a query-document\nrepresentation η(q, d). Among others, two neural network architectures have been\ninvestigated to build a representation of these relationships: convolutional neural net-\nworks and transformers. Convolutional neural networks represent one of the first\napproaches in building joint representations of queries and documents, as discussed\nin Section 2.1. Transformers represent the major turning point in neural IR, as their\napplication to textual inputs gave birth to pre-trained language models, presented\nin Section 2.2. In neural IR, pre-trained language models are used to compute query-\ndocument representations, and the two main transformer models used for this task are\nBERT and T5 illustrated in Sections 2.3 and 2.4, respectively. Section 2.5 describes how\npre-trained language models are fine-tuned to compute effective query-document rep-\nresentations, and Section 2.6 briefly discusses how pre-trained language models can\ndeal with long documents.",
            "subsections": {
                "Convolutional Neural Networks": {
                    "content": "A convolutional neural network is a family of neural networks designed to capture\nlocal patterns in structured inputs, such as images and texts [LeCun and Bengio 1998].\nThe core component of a convolutional neural network is the convolution layer, used\nin conjunction with feed forward and pooling layers. A convolutional layer can be\nseen as a small linear filter, sliding over the input and looking for proximity patterns.\nSeveral neural models employ convolutional neural networks over the interactions\nbetween queries and documents to produce relevance scores. Typically, in these mod-\nels, the word embeddings of the query and document tokens are aggregated into an\ninteraction matrix, on top of which convolutional neural networks are used to learn\nhierarchical proximity patterns such as unigrams, bigrams and so on. Then, the final\ntop-level proximity patterns are fed into a feed forward neural network to produce\nthe relevance score s(q, d) between the query q and the document d, as illustrated in\nFigure 2.\nThe query q and the document d are tokenised into m and n tokens, respectively, and\neach token is mapped to a corresponding static word embedding. The interaction\nmatrix η(q, d) ∈ Rm×n is composed of the cosine similarities between a query token\nembedding and a document token embedding.\nOne of the first neural models leveraging the interaction matrix is the Deep Relevance\nMatching Model (DRMM) [Guo et al. 2016]). In DRMM, the cosine similarities of every\nquery token w.r.t. the document tokens are converted into a discrete distribution using\nhard bucketing, i.e., into a query token histogram. Then the histogram of each query to-\nken is provided as input to a feed forward neural network to compute the final query\ntoken-document relevance score. These relevance scores are then aggregated through\nan IDF-based weighted sum across the different query terms. Instead of using hard\nbucketing, the Kernel-based Neural Ranking Model (KNRM) [Xiong et al. 2017] pro-\nposes to use Gaussian kernels to smoothly distribute the contribution of each cosine\nsimilarity across different buckets before providing the histograms with soft bucketing\nto the feed forward neural networks.\nBoth DRMM and KNRM exploit the interaction matrix, but they do not incorporate.\nany convolutional layer. In the Convolutional KNRM model (ConvKNRM) [Dai et al.\n2018], the query and document embeddings are first independently processed through\nk convolutional neural networks, to build unigam, bigram, up to k-gram embeddings.\nThese convolutions allow to build word embeddings taking into account multiple\nclose words at the same time. Then, k2 cosine similarity matrices are built, between\neach combination of query and document n-gram embeddings, and these matrices\nare processed with KNRM. In the Position-Aware Convolutional Recurrent Relevant\nmodel (PACRR) [Hui et al. 2017], the interaction matrix is processed through several\nconvolutional and pooling layers to take into account words proximity. Convolutional\nlayers are used also in other similar neural models [Fan et al. 2018b, Hu et al. 2014, Hui\net al. 2018, Pang et al. 2016, 2017]."
                },
                "Pre-trained Language Models": {
                    "content": "Static word embeddings map words with multiple senses into an average or most\ncommon-sense representation based on the training data used to compute the vectors.\nThe vector of a word does not change with the other words used in a sentence around\nit. The transformer is a neural network designed to explicitly take into account the\ncontext of arbitrary long sequences of text, thanks to a special neural layer called self-\nattention, used in conjunction with feed forward and linear layers. The self-attention\nlayer maps input sequences to output sequences of the same length. When comput-\ning the i-th output element, the layer can access all the n input elements (bidirectional\nself-attention) or only the first i input elements (causal self-attention). A self-attention\nlayer allows the network to take into account the relationships among different ele-\nments in the same input. When the input elements are tokens of a given text, a self-\nattention layer computes token representations that take into account their context,\ni.e., the surrounding words. In doing so, the transformer computes contextualised word\nembeddings, where the representation of each input token is conditioned by the whole\ninput text.\nTransformers have been successfully applied to different natual language processing\ntasks, such as machine translation, summarisation, question answering and so on. All\nthese tasks are special instances of a more general task, i.e., transforming an input\ntext sequence to some output text sequence. The sequence-to-sequence model has been\ndesigned to address this general task. The sequence-to-sequence neural network is\ncomposed of two parts: an encoder model, which receives an input sequence and builds\na contextualised representation of each input element, and a decoder model, which\nuses these contextualised representations to generate a task-specific output sequence\nBoth models are composed of several stacked transformers. The transformers in the\nencoder employ bidirectional self-attention layers on the input sequence or the output\nsequence of the previous transformer. The transformers in the decoder employ causal\nself-attention on the previous decoder transformer’s output and bidirectional cross-\nattention of the output of the final encoder transformer’s output.\nIn neural IR, two specific instances of the sequence-to-sequence models have been\nstudied: encoder-only models and encoder-decoder models. Encoder-only models re-\nceive as input all the tokens of a given input sentence, and they compute an output\ncontextualised word embedding for each token in the input sentence. Representatives\nof this family of models include BERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019],\nand DistilBERT [Sanh et al. 2019]. Encoder-decoder models generate new output sen-\ntences depending on the given input sentence. The encoder model receives as input all\nthe tokens of a given sequence and builds a contextualised representation, and the de-\ncoder model sequentially accesses these embeddings to generate new output tokens,\none token at a time. Representatives of this family of models include BART [Lewis\net al. 2020] and T5 [Raffel et al. 2020].\nSequence-to-sequence models can be trained as language models, by projecting with a\nlinear layer every output embedding to a given vocabulary and computing the tokens\nprobabilities with a softmax operation. The softmax operation is a function σ : Rk →\n[0, 1]k that takes as input k > 1 real values z1, z2, . . . , zk and transforms each input zi as\nfollows:\nσ(zi ) = ezi\n∑kj=1 ezj (2)\nThe softmax operation normalises the input values into a probability distribution. In\nthe context of deep learning, the inputs of a softmax operation are usually called logits,\nand they represent the raw predictions generated by a multi-class classification model,\nturned into a probability distribution over the classes by the softmax operation.\nDepending on the training objective, a sequence-to-sequence model can be trained\nas a masked language model (MLM), as for BERT, or a casual language model (CLM), as\nfor T5. MLM training focuses on learning to predict missing tokens in a sequence\ngiven the surrounding tokens; CLM training focuses on predicting the next token in\nan output sequence given the preceding tokens in the input sequence. In both cases,\nit is commonplace to train these models using massive text data to obtain pre-trained\nlanguage models. In doing so, we allow the model to learn general-purpose knowledge\nabout a language that can be adapted afterwards to a more specific downstream task. In\nthis transfer learning approach, a pre-trained language model is used as initial model\nto fine-tune it on a domain-specific, smaller training dataset for the downstream target\nask. In other words, fine-tuning is the procedure to update the parameters of a pre-\ntrained language model for the domain data and target task.\nAs illustrated in Figure 3, pre-training typically requires a huge general-purpose train-\ning corpus, such as Wikipedia or Common Crawl web pages, expensive computa-\ntion resources and long training times, spanning several days or weeks. On the other\nside, fine-tuning requires a small domain-specific corpus focused on the downstream\ntask, affordable computational resources and few hours or days of additional training.\nSpecial cases of fine-tuning are few-shot learning, where the domain-specific corpus is\ncomposed of a very limited number of training data, and zero-shot learning, where a\npre-trained language model is used on a downstream task that it was not fine-tuned\non.\nIn neural IR, the interaction-focused systems that use pre-trained language models are\ncalled cross-encoder models, as they receive as input a pair (q, d) of query and document\ntexts. Depending on the type of sequence-to-sequence model, different cross-encoders\nare fine-tuned in different ways, but, in general, they aim at computing a relevance\nscore s(q, d) to rank documents w.r.t. a given query. In the following, we illustrate\nthe most common cross-encoders leveraging both encoder-only models (Sec. 2.3) and\nencoder-decoder models (Sec. 2.4)."
                },
                "Ranking with Encoder-only Models": {
                    "content": "The most widely adopted transformer architecture in neural IR is BERT, an encoder-\nonly model. Its input text is tokenised using the WordPiece sub-word tokeniser [Wu\net al. 2016]. The vocabulary V of this tokeniser is composed of 30, 522 terms, where the\nuncommon/rare words, e.g., goldfish, are splitted up in sub-words, e.g., gold## and\n##fish. The first input token of BERT is always the special [CLS] token, that stands\nfor “classification”. BERT accepts as input other special tokens, such as [SEP], that\ndenotes the end of a text provided as input or to separate two different texts provided\nas a single input. BERT accepts as input at most 512 tokens, and produces an output\nembedding in R` for each input token. The most commonly adopted BERT version\n16\nis BERT base, which stacks 12 transformer layers, and whose output representation\nspace has ` = 768 dimensions.\nNogueira and Cho [2019] and MacAvaney et al. [2019] illustrated how to fine-tune\nBERT as a cross-encoder1, in two slightly different ways. Given a query-document\npair, both texts are tokenised into token sequences q1, . . . , qm and d1, . . . , dn. Then,\nthe tokens are concatenated with BERT special tokens to form the following input\nconfiguration:\n[CLS] q1 ···qm [SEP] d1 ···dn [SEP]\nthat will be used as BERT input. In doing so, the self-attention layers in the BERT en-\ncoders are able to take into account the semantic interactions among the query tokens\nand the document tokens. The output embedding η[CLS] ∈ R`, corresponding to the\ninput [CLS] token, serves as a contextual representation of the query-document pair\nas a whole.\nNogueira et al. [2019a] fine-tune BERT on a binary classification task to compute the\nquery-document relevance score, as illustrated in Figure 4. To produce the relevance\nscore s(q, d), the query and the document are processed by BERT to generate the out-\nput embedding η[CLS] ∈R`, that is multiplied by a learned set of classification weights\nW2 ∈R2×` to produce two real scores z0 and z1, and then through a softmax operation\nto transform the scores into a probability distribution p0 and p1 over the non-relevant\nand relevant classes. The probability corresponding to the relevant class, convention-\nally assigned to label 1, i.e., p1, is the final relevance score.\nMacAvaney et al. [2019] fine-tune BERT by projecting the output embedding η[CLS] ∈\nR` through the learned matrix W1 ∈R1×` into a single real value z, that represents the\nfinal relevance score.\nη[CLS] = BERT(q, d)\n[z0, z1] = W2η[CLS] or z = W1η[CLS]\n[p0, p1] = softmax([z0, z1])\ns(q, d) = p1 or s(q, d) = z\n(3)\nIn Section 2.5 we illustrate how a BERT-based cross-encoder is typically fine-tuned for\nad-hoc ranking"
                },
                "Ranking with Encoder-decoder Models": {
                    "content": "Instead of using an encoder-only transformer model to compute the latent representa-\ntion of a query-document pair and to convert it into a relevance score, it is possible also\nto use an encoder-decoder model [Raffel et al. 2020] with prompt learning, by converting\nthe relevance score computation task into a cloze test, i.e., a fill-in-the-blank problem.\nPrompting has been successfully adopted in article summarisation tasks [Radford et al.\n2019] and knowledge base completion tasks [Petroni et al. 2019].\nIn prompt learning, the input texts are reshaped as a natural language template, and\nthe downstream task is reshaped as a cloze-like task. For example, in topic classifi-\ncation, assuming we need to classify the sentence text into two classes c0 and c1, the\ninput template can be:\nInput: text Class: [OUT]\nAmong the vocabulary terms, two label terms w0 and w1 are selected to correspond to\nthe classes c0 and c1, respectively. The probability to assign the input text to a class\ncan be transferred into the probability that the input token [OUT] is assigned to the\ncorresponding label token:\np(c0|text) = p([OUT] = w0|Input: text Class: [OUT])\np(c1|text) = p([OUT] = w1|Input: text Class: [OUT])\n18\nNogueira et al. [2020] proposed a prompt learning approach for relevance ranking\nusing a T5 model2, as illustrated in Figure 5. The query and the document texts q and\nd are concatenated to form the following input template:\nQuery: q Document: d Relevant: [OUT]\nAn encoder-decoder model is fine-tuned with a downstream task taking as input this\ninput configuration, and generating an output sequence whose last token is equal to\nTrue or False, depending on whether the document d is relevant or non-relevant to\nthe query q.\nThe query-document relevance score is computed by normalising only the False and\nTrue output probabilities, computed over the whole vocabulary, with a softmax opera-\ntion.\nTo produce the relevance score s(q, d), the query and the document are processed by\nT5 to generate the output embedding η[OUT] ∈R`, that is projected by a learned set of\nclassification weights WV ∈R|V|×` over the vocabulary V. The outputs zFalse and zTrue\ncorresponding to the False and True terms, respectively, are transformed into a proba-\nbility distribution with a softmax operation, to yield the required predictions pFalse and\npTrue over the “non-relevant” and “relevant” classes. The prediction corresponding to the relevant class, i.e., pTrue, is the final relevance score.\nη[OUT] = T5(q, d)\n[. . . , zFalse, . . . , zTrue, . . .]T = WV η[OUT]\n[pFalse, pTrue] = softmax([zFalse, zTrue])\ns(q, d) = pTrue\n(4)\nIn the next section we discuss how a T5-based cross-encoder is typically fine-tuned for\nad-hoc ranking."

                },
                "Fine-tuning Interaction-focused Systems":{
                    "content": "As discussed in Section 2.2, the pre-trained language models adopted in IR require\na fine-tuning of the model on a specific downstream task. Given an input query-\ndocument pair (q, d) a neural IR model M(θ), parametrised by θ, computes sθ (q, d) ∈\nR, the score of the document d w.r.t. the query q. We are supposed to predict y ∈\n{+, −}from (q, d) ∈Q×D, where −stands for non-relevant and + for relevant.\nThis problem can be framed as a binary classification problem. We perform the clas-\nsification by assuming a joint distribution p over {+, −}×Q×D from which we\ncan sample correct pairs (+, q, d) ≡ (q, d+) and (−, q, d) ≡ (q, d−). Using sampled\ncorrect pairs we learn a score function sθ (q, d) as an instance of a metric learning prob-\nlem [Xing et al. 2002]: the score function must assign a high score to a relevant docu-\nment and a low score to a non-relevant document, as in Eq. (3) and Eq. (4). Then we\nfind θ∗ that minimises the (binary) cross entropy lCE between the conditional proba-\nbility p(y|q, d) and the model probability pθ (y|q, d):\nθ∗ = arg minθ E\n[\nlCE (y, q, d)\n]\n(5)\nwhere the expectation is computed over (y, q, d) ∼ p, and the cross entropy is com-\nputed as:\nlCE\n(y, (q, d)) =\n\n\n−log (sθ (q, d)) if y = +\n−log (1 −sθ (q, d)) if y = − (6)\nTypically, a dataset T available for fine-tuning pre-trained language models for rele-\nvance scoring is composed of a list of triples (q, d+, d−), where q is a query, d+ is a\nrelevant document for the query, and d− is a non-relevant document for the query. In\nthis case, the expected cross entropy is approximated by the sum of the cross entropies\n20\ncomputed for each triple:\nE\n[\nlCE\n(y, (q, d))]\n≈ 1\n2|T| ∑(q,d+,d−)∈T\n(\n−log (sθ (q, d+))−log (1 −sθ (q, d−)))\n(7)\nIn doing so, we do not take into account documents in the collection that are not ex-\nplicitly labeled as relevant or non-relevant. This approach is limited to take into ac-\ncount positive and negative triples in a pairwise independent fashion. In Section 3.3\nwe will discuss a different fine-tuning approach commonly used for representation-\nfocused systems, taking into account multiple non-relevant documents per relevant\ndocument."
                },
                "Dealing with long texts":{
                    "content": "BERT and T5 models have an input size limited to 512 tokens, including the special\nones. When dealing with long documents, that cannot be fed completely into a trans-\nformer model, we need to split them into smaller texts in a procedure referred to as\npassaging. Dai and Callan [2019a] propose to split a long document into overlapping\nshorter passages, to be processed independently together with the same query by a\ncross-encoder. During training, if a long document is relevant, all its passages are\nrelevant, and vice-versa. Then, the relevance scores for each composing passage are\naggregated back to a single score for the long document. In this scenario, the common\naggregation functions are FirstP, i.e., the document score is the score of the first pas-\nsage, MaxP, i.e., the document score is the highest score across all passages, and SumP,\ni.e., the document score is the sum of the scores of its passages. Alternatively, Li et al.\n[2020] generate the [CLS] output embedding for each passage to compute a query-\npassage representation for each passage. Then, the different passage embeddings are\naggregated together to compute a final relevance score for the whole document using\nfeed forward neural networks, convolutional neural networks or simple transformer\narchitectures.\n3 Representation-focused Systems\nThe representation-focused systems build up independent query and document rep-\nresentations, in such a way that document representations can be pre-computed and\nstored in advance. During query processing, only the query representation is com-\nputed, and the top documents are searched through the stored document representa-\ntions. In doing so, representation-based systems are able to identify the relevant docu-\n21\n"
                }
            }  
        },
        "Representation-focused Systems": {
            "content": 0,
            "subsections": {}  
    },
        "Retrieval Architectures and Vector Search": {
            "content": 0,
            "subsections": {}  
           },
        "Learned Sparse Retrieval": {
            "content": 0,
            "subsections": {}  
       },
        "conclusion": {
            "content": "This overview aimed to provide the foundations of neural IR approaches for ad-hoc\nranking, focusing on the core concepts and the most commonly adopted neural architecture in current research. In particular, in Section 1 we provided a background on\nthe different ways to represent texts, such as queries and documents, to be processed\nin IR systems. Sections 2 and 3 illustrated the main system architectures in neural\nIR, namely interaction-focused and representation-focused systems. In Section 4 we\nreviewed the multi-stage retrieval architectures exploiting neural IR systems, and we\nprovided a quick overview of the embedding indexes and algorithms for dense retrieval. Finally, in Section 5, we discussed the main state-of-the-art solutions for learning sparse representations.\nNeural IR systems are currently a hot research topic, and many excellent surveys complement and deepen the concepts discussed in this overview. The interested readers\ncan find a fully-fledged detailed presentation of pre-trained transformers for ranking,\nwith many experimental evaluations, in the recent book by [Lin et al. 2021], as well\nas applications of dense retrieval to conversational system in the books by Gao et al.\n[2022] and Zamani et al. [2022]."
        }

    }
}
