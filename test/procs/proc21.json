{
    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "subsections": {
                "BOW Encodings": {
                    "content": "Bag-of-Words (BoW) encodings represent documents as fixed-sized vectors, where each dimension corresponds to the frequency of a word in the vocabulary, offering an efficient way to represent and compare documents for information retrieval tasks [7]. However, traditional BoW representations do not capture the semantic relationships in the text and usually have limited performance in ranking tasks compared to more advanced methods using dense representations [1,4].\n\nRecent research has sought ways to improve the BoW model performance with additional information and optimizations. In [2], the authors propose SparTerm, a method that combines an importance predictor and a gating controller to generate term-based sparse representations based on semantic relationships with the input text. This approach maintains the interpretability and efficiency of BoW while enhancing ranking performance. Moreover, the Entropy Optimized Feature-Based Bag-of-Words (EO-BoW) proposed in [7] improves retrieval performance by optimizing the dictionary construction using an entropy-based criterion, providing better average precision and lowering encoding time and storage requirements.\n\nOn the other hand, the success of transformer-based models in dense representation learning has cast a shadow on BoW-based approaches [1]. For instance, pretrained transformers like BERT and its variants have demonstrated substantial effectiveness gains over BoW methods in text ranking tasks [4]. Despite the benefits of the dense representation models, they come with higher computational costs [1,4], motivating researchers to seek ways to bridge the performance gap between the two representations while maintaining BoW's efficiency advantages [2,7]. The ongoing research in this area suggests that there is still room for improvements in BoW and term-based representations while keeping their inherent advantages in computational cost and interpretability.",
                    "references": [
                        "@Article{Lin2020PretrainedTF,\n author = {Jimmy J. Lin and Rodrigo Nogueira and Andrew Yates},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},\n title = {Pretrained Transformers for Text Ranking: BERT and Beyond},\n year = {2020}\n}\n",
                        "@Article{Bai2020SparTermLT,\n author = {Yang Bai and Xiaoguang Li and Gang Wang and Chaoliang Zhang and Lifeng Shang and Jun Xu and Zhaowei Wang and Fangshan Wang and Qun Liu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval},\n volume = {abs/2010.00768},\n year = {2020}\n}\n",
                        "@Article{Wang2015IncorporatingLK,\n author = {Yan Wang and Zhiyuan Liu and Maosong Sun},\n booktitle = {PLoS ONE},\n journal = {PLoS ONE},\n title = {Incorporating Linguistic Knowledge for Learning Distributed Word Representations},\n volume = {10},\n year = {2015}\n}\n",
                        "@Article{Zhuang2021FastPR,\n author = {Shengyao Zhuang and G. Zuccon},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Fast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansion},\n volume = {abs/2108.08513},\n year = {2021}\n}\n",
                        "@Article{Lin2020DistillingDR,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Jimmy J. Lin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Distilling Dense Representations for Ranking using Tightly-Coupled Teachers},\n volume = {abs/2010.11386},\n year = {2020}\n}\n",
                        "@Article{Paul2019RankingAS,\n author = {Debjit Paul and A. Frank},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {3671-3681},\n title = {Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs},\n year = {2019}\n}\n",
                        "@Article{Passalis2016EntropyOF,\n author = {N. Passalis and A. Tefas},\n booktitle = {IEEE Transactions on Knowledge and Data Engineering},\n journal = {IEEE Transactions on Knowledge and Data Engineering},\n pages = {1664-1677},\n title = {Entropy Optimized Feature-Based Bag-of-Words Representation for Information Retrieval},\n volume = {28},\n year = {2016}\n}\n",
                        "@Article{Lin2021InBatchNF,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Jimmy J. Lin},\n booktitle = {Workshop on Representation Learning for NLP},\n pages = {163-173},\n title = {In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval},\n year = {2021}\n}\n"
                    ],
                    "title": "BOW Encodings"
                },
                "LTR Features": {
                    "content": "Learning to Rank (LTR) is a crucial technique employed in information retrieval and search engine ranking tasks [7]. The effectiveness of LTR models is highly influenced by the selection of relevant features and the proper usage of different sources of information [3, 5, 7]. Generally, LTR features can be categorized into query-dependent or dynamic features (e.g., BM25 score) [4], query-level features (e.g., number of words in query) [4], and document-dependent features (e.g., term frequency, spam score) [4]. It is important to pay attention to feature selection for LTR, as features in information retrieval are not always independent from each other, and reducing the feature subset can lead to improved performance [3]. \n\nOne approach for feature selection in the context of LTR is through hierarchical feature selection, which includes a two-phase process for determining ranking functions [3]. Another approach leverages Learning-to-Rank with BERT, which combines ranking losses with BERT representations for passage ranking [2]. This method demonstrated the effectiveness of fine-tuning BERT representations for query-document pairs within a ranking framework [2]. Other feature selection algorithms specifically designed for LTR also exist, such as the ones proposed by [5], which have shown to outperform well-known state-of-the-art competitors. In summary, selecting the right set of features and exploiting all available sources of information are essential elements for optimizing LTR models and improving search engine rankings.",
                    "references": [
                        "@Book{Zamani2017NeuralRM,\n author = {Hamed Zamani and Bhaskar Mitra and Xia Song and Nick Craswell and Saurabh Tiwary},\n booktitle = {Web Search and Data Mining},\n journal = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},\n title = {Neural Ranking Models with Multiple Document Fields},\n year = {2017}\n}\n",
                        "@Article{Han2020LearningtoRankWB,\n author = {Shuguang Han and Xuanhui Wang and Michael Bendersky and Marc Najork},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning-to-Rank with BERT in TF-Ranking},\n volume = {abs/2004.08476},\n year = {2020}\n}\n",
                        "@Article{Hua2010HierarchicalFS,\n author = {G. Hua and Min Zhang and Yiqun Liu and Shaoping Ma and Liyun Ru},\n booktitle = {The Web Conference},\n pages = {1113-1114},\n title = {Hierarchical feature selection for ranking},\n year = {2010}\n}\n",
                        "@Article{Mitra2017NeuralMF,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Models for Information Retrieval},\n volume = {abs/1705.01509},\n year = {2017}\n}\n",
                        "@Article{Gigli2016FastFS,\n author = {Andrea Gigli and C. Lucchese and F. M. Nardini and R. Perego},\n booktitle = {International Conference on the Theory of Information Retrieval},\n journal = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},\n title = {Fast Feature Selection for Learning to Rank},\n year = {2016}\n}\n",
                        "@Article{Joachims2016UnbiasedLW,\n author = {T. Joachims and Adith Swaminathan and Tobias Schnabel},\n booktitle = {Web Search and Data Mining},\n journal = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},\n title = {Unbiased Learning-to-Rank with Biased Feedback},\n year = {2016}\n}\n",
                        "@Article{Azarbonyad2020LearningTR,\n author = {H. Azarbonyad and M. Dehghani and M. Marx and J. Kamps},\n booktitle = {Natural Language Engineering},\n journal = {Natural Language Engineering},\n pages = {89 - 111},\n title = {Learning to rank for multi-label text classification: Combining different sources of information},\n volume = {27},\n year = {2020}\n}\n",
                        "@Article{Oosterhuis2018DifferentiableUO,\n author = {Harrie Oosterhuis and M. de Rijke},\n booktitle = {International Conference on Information and Knowledge Management},\n journal = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},\n title = {Differentiable Unbiased Online Learning to Rank},\n year = {2018}\n}\n"
                    ],
                    "title": "LTR Features"
                },
                "Word Embeddings": {
                    "content": "Word embeddings have emerged as a powerful tool in natural language processing (NLP), offering a compact and effective representation of words that capture their semantics and syntactic properties. Several methods have been proposed to generate word embeddings, such as the Neural Network Language Model [1], Sparse Coding Approach [1], and the popular Word2Vec [3]. Incorporating semantic knowledge can significantly improve the quality of the learned embeddings, as demonstrated by the work on learning semantic word embeddings (SWE) based on ordinal knowledge constraints [2]. These improvements can be seen in various NLP tasks, including word similarity measure, sentence completion, name entity recognition, and TOEFL synonym selection [2].\n\nMoreover, word embeddings have been successfully applied in a variety of NLP applications such as document and query representation for information retrieval [4], sentiment-aware word embedding for emotion classification [6], and integrating extra knowledge into word embedding for biomedical NLP tasks [7]. The development of Dual Word Embeddings [8] has enhanced the document ranking process by utilizing both input and output embeddings in Word2Vec, offering a more suitable similarity measure for document ranking. Additionally, simple word embedding-based models with associated pooling mechanisms have demonstrated their potential as strong baselines for text representation learning and highlighted the computational expressiveness tradeoff while selecting compositional functions for distinct NLP problems [5].",
                    "references": [
                        "@Inproceedings{Li2018WordEF,\n author = {Yang Li and Tao Yang},\n pages = {83-104},\n title = {Word Embedding for Understanding Natural Language: A Survey},\n year = {2018}\n}\n",
                        "@Article{LIU2015LearningSW,\n author = {QUAN LIU and Hui Jiang and Si Wei and Zhenhua Ling and Yu Hu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1501-1511},\n title = {Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints},\n year = {2015}\n}\n",
                        "@Article{Ji2015WordRankLW,\n author = {Shihao Ji and Hyokun Yun and Pinar Yanardag and Shin Matsushima and S. Vishwanathan},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {658-668},\n title = {WordRank: Learning Word Embeddings via Robust Ranking},\n year = {2015}\n}\n",
                        "@Article{Roy2016RepresentingDA,\n author = {Dwaipayan Roy and Debasis Ganguly and Mandar Mitra and G. Jones},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Representing Documents and Queries as Sets of Word Embedded Vectors for Information Retrieval},\n volume = {abs/1606.07869},\n year = {2016}\n}\n",
                        "@Article{Shen2018BaselineNM,\n author = {Dinghan Shen and Guoyin Wang and Wenlin Wang and Martin Renqiang Min and Qinliang Su and Yizhe Zhang and Chunyuan Li and Ricardo Henao and L. Carin},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {440-450},\n title = {Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms},\n year = {2018}\n}\n",
                        "@Article{Mao2019SentimentAwareWE,\n author = {Xingliang Mao and Shuai Chang and Jinjing Shi and Fangfang Li and Ronghua Shi},\n booktitle = {Applied Sciences},\n journal = {Applied Sciences},\n title = {Sentiment-Aware Word Embedding for Emotion Classification},\n year = {2019}\n}\n",
                        "@Article{Ling2017IntegratingEK,\n author = {Yuan Ling and Yuan An and Mengwen Liu and Sadid A. Hasan and Ye-tian Fan and Xiaohua Hu},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {2017 International Joint Conference on Neural Networks (IJCNN)},\n pages = {968-975},\n title = {Integrating extra knowledge into word embedding models for biomedical NLP tasks},\n year = {2017}\n}\n",
                        "@Article{Nalisnick2016ImprovingDR,\n author = {Eric T. Nalisnick and Bhaskar Mitra and Nick Craswell and R. Caruana},\n booktitle = {The Web Conference},\n journal = {Proceedings of the 25th International Conference Companion on World Wide Web},\n title = {Improving Document Ranking with Dual Word Embeddings},\n year = {2016}\n}\n"
                    ],
                    "title": "Word Embeddings"
                }
            },
            "content": "The problem of ranking, which involves learning a real-valued ranking function to induce an ordering over an instance space, is an important task in machine learning [2]. Recent advancements in neural networks, especially deep convolutional neural networks (CNNs), have offered the potential for multidimensional signal processing and improved the ability to automatically learn features from large data sets for classification tasks [3]. In order to tackle the challenges posed by the increasing amount of digital data for retrieval systems, a novel retrieval framework has been proposed, which consists of weighted subspace-based filtering and ranking components [4]. This framework addresses the difficulty of comparing semantics between low-level features and high-level concepts by applying multiple correspondence analysis (MCA) to explore relationships between feature categories and concept classes [4]. Consequently, these techniques help in efficiently filtering and ranking results in multimedia retrieval applications. Using advanced algorithms, such as kernel-based ranking algorithms that perform regularization in reproducing kernel Hilbert space, generalization bounds can be derived for ranking tasks based on the notion of algorithmic stability [2]. By considering algorithmic stability and the advancements in neural networks like CNNs, one can improve the ranking effectiveness and learn better text representations for various applications.",
            "references": [
                "@Article{Duchi2012TheAO,\n author = {John C. Duchi and Lester W. Mackey and Michael I. Jordan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Asymptotics of Ranking Algorithms},\n volume = {abs/1204.1688},\n year = {2012}\n}\n",
                "@Article{Agarwal2009GeneralizationBF,\n author = {S. Agarwal and P. Niyogi},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {441-474},\n title = {Generalization Bounds for Ranking Algorithms via Algorithmic Stability},\n volume = {10},\n year = {2009}\n}\n",
                "@Article{Gonzalez2018DeepCN,\n author = {Rafael C. Gonzalez},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {79-87},\n title = {Deep Convolutional Neural Networks [Lecture Notes]},\n volume = {35},\n year = {2018}\n}\n",
                "@Article{Lin2011WeightedSF,\n author = {Lin Lin and Chao Chen and M. Shyu and Shu\u2010Ching Chen},\n booktitle = {IEEE Multimedia},\n journal = {IEEE MultiMedia},\n pages = {32-43},\n title = {Weighted Subspace Filtering and Ranking Algorithms for Video Concept Retrieval},\n volume = {18},\n year = {2011}\n}\n",
                "@Article{Baraniuk2007CompressiveS,\n author = {Richard Baraniuk},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {118-121},\n title = {Compressive Sensing [Lecture Notes]},\n volume = {24},\n year = {2007}\n}\n",
                "@Article{Kuo2017TheCA,\n author = {C.-C. Jay Kuo},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {81-89},\n title = {The CNN as a Guided Multilayer RECOS Transform [Lecture Notes]},\n volume = {34},\n year = {2017}\n}\n",
                "@Article{Chatzivasileiadis2018LectureNO,\n author = {Spyros Chatzivasileiadis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Lecture Notes on Optimal Power Flow (OPF)},\n volume = {abs/1811.00943},\n year = {2018}\n}\n",
                "@Article{Lidar2019LectureNO,\n author = {Daniel A. Lidar},\n journal = {arXiv: Quantum Physics},\n title = {Lecture Notes on the Theory of Open Quantum Systems},\n year = {2019}\n}\n"
            ]
        },
        "Interaction-focused Systems": {
            "subsections": {
                "Convolutional Neural Networks": {
                    "content": "Convolutional Neural Networks (CNNs) are a type of neural network that leverage convolution operations as a core component in their design, allowing them to efficiently process spatially structured data and extract local patterns [2]. These networks have found widespread success across numerous fields, including computer vision [5], natural language processing [3], and even molecular neural networks [7]. Advancements in Graph Convolutional Networks (GCNs) have further extended the power of CNNs by incorporating graph-structured data, enabling the propagation of information across local graph neighborhoods through iterative aggregation and feature transformation [4]. Such versatility makes CNNs ideal candidates for applications involving large datasets and complex feature extraction processes. Moreover, developments in neuromorphic photonic systems have led to Digital Electronic and Analog Photonic (DEAP) CNN hardware architectures, which hold the potential to be faster and more energy-efficient than current state-of-the-art GPUs [8]. Through such innovations, CNNs continue to augment our ability to process and understand complex datasets, setting new standards in various fields of research and application.",
                    "references": [
                        "@Article{Xu2018HowPA,\n author = {Keyulu Xu and Weihua Hu and J. Leskovec and S. Jegelka},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {How Powerful are Graph Neural Networks?},\n volume = {abs/1810.00826},\n year = {2018}\n}\n",
                        "@Article{Ketkar2021ConvolutionalNN,\n author = {Nikhil S. Ketkar},\n booktitle = {Deep Learning with Python},\n journal = {Deep Learning with Python},\n title = {Convolutional Neural Networks},\n year = {2021}\n}\n",
                        "@Article{Li2016MiningOS,\n author = {Qiudan Li and Zhipeng Jin and Can Wang and D. Zeng},\n booktitle = {Knowledge-Based Systems},\n journal = {Knowl. Based Syst.},\n pages = {289-300},\n title = {Mining opinion summarizations using convolutional neural networks in Chinese microblogging systems},\n volume = {107},\n year = {2016}\n}\n",
                        "@Article{Ying2018GraphCN,\n author = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and J. Leskovec},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},\n year = {2018}\n}\n",
                        "@Article{Schirrmeister2017DeepLW,\n author = {R. Schirrmeister and J. T. Springenberg and L. Fiederer and M. Glasstetter and Katharina Eggensperger and M. Tangermann and F. Hutter and W. Burgard and T. Ball},\n booktitle = {Human Brain Mapping},\n journal = {Human Brain Mapping},\n pages = {5391 - 5420},\n title = {Deep learning with convolutional neural networks for EEG decoding and visualization},\n volume = {38},\n year = {2017}\n}\n",
                        "@Article{Choy20194DSC,\n author = {C. Choy and JunYoung Gwak and S. Savarese},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3070-3079},\n title = {4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks},\n year = {2019}\n}\n",
                        "@Article{Finzi2020GeneralizingCN,\n author = {Marc Finzi and S. Stanton and Pavel Izmailov and A. Wilson},\n booktitle = {International Conference on Machine Learning},\n pages = {3165-3176},\n title = {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},\n year = {2020}\n}\n",
                        "@Article{Bangari2019DigitalEA,\n author = {V. Bangari and B. Marquez and Heidi B. Miller and A. Tait and M. Nahmias and T. F. de Lima and Hsuan-Tung Peng and P. Prucnal and B. Shastri},\n booktitle = {IEEE Journal of Selected Topics in Quantum Electronics},\n journal = {IEEE Journal of Selected Topics in Quantum Electronics},\n pages = {1-13},\n title = {Digital Electronics and Analog Photonics for Convolutional Neural Networks (DEAP-CNNs)},\n volume = {26},\n year = {2019}\n}\n"
                    ],
                    "title": "Convolutional Neural Networks"
                },
                "Pre-trained Language Models": {
                    "content": "Pre-trained language models have demonstrated significant potential in various natural language processing tasks, transforming the way we approach problems in the field. Models such as GPT-2, GPT-3, BERT, and RoBERTa have led groundbreaking advancements [1,4,5]. In task-oriented dialogue systems, authors in [4] propose building on top of the TransferTransfo framework and the generative model pre-training to adapt these language models to generate task-specific responses. TOD-BERT [1] is an example of a pre-trained language model specifically aimed at task-oriented dialogues, demonstrating the adaptability of these models.\n\nAdditionally, pre-trained language models have proven effective in other domains, such as mental healthcare with MentalBERT [5] and recommendation systems with M6-Rec [2]. Pre-trained language models have also been employed for multilingual applications, as demonstrated in mLUKE [3], which highlights the power of multilingual models for cross-lingual transfer learning. Moreover, the development of Meta Language Models, such as METALM [8], showcases their potential as general-purpose interfaces, allowing users to interact with models using natural language and supporting multi-turn conversational interactions. Ultimately, pre-trained language models have transformed the landscape of NLP, demonstrating remarkable performance across a breadth of tasks and domains.",
                    "references": [
                        "@Article{Wu2020TODBERTPN,\n author = {Chien-Sheng Wu and S. Hoi and R. Socher and Caiming Xiong},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {917-929},\n title = {TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue},\n year = {2020}\n}\n",
                        "@Article{Cui2022M6RecGP,\n author = {Zeyu Cui and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems},\n volume = {abs/2205.08084},\n year = {2022}\n}\n",
                        "@Article{Ri2021mLUKETP,\n author = {Ryokan Ri and Ikuya Yamada and Yoshimasa Tsuruoka},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {7316-7330},\n title = {mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models},\n year = {2021}\n}\n",
                        "@Article{Budzianowski2019HelloIG,\n author = {Pawe\u0142 Budzianowski and Ivan Vulic},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {15-22},\n title = {Hello, It\u2019s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems},\n year = {2019}\n}\n",
                        "@Article{Ji2021MentalBERTPA,\n author = {Shaoxiong Ji and Tianlin Zhang and Luna Ansari and Jie Fu and P. Tiwari and E. Cambria},\n booktitle = {International Conference on Language Resources and Evaluation},\n pages = {7184-7190},\n title = {MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare},\n year = {2021}\n}\n",
                        "@Article{Qiao2022ReasoningWL,\n author = {Shuofei Qiao and Yixin Ou and Ningyu Zhang and Xiang Chen and Yunzhi Yao and Shumin Deng and Chuanqi Tan and Fei Huang and Huajun Chen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reasoning with Language Model Prompting: A Survey},\n volume = {abs/2212.09597},\n year = {2022}\n}\n",
                        "@Article{Li2021PretrainedLM,\n author = {Junyi Li and Tianyi Tang and Wayne Xin Zhao and Ji-rong Wen},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {4492-4499},\n title = {Pretrained Language Models for Text Generation: A Survey},\n year = {2021}\n}\n",
                        "@Article{Hao2022LanguageMA,\n author = {Y. Hao and Haoyu Song and Li Dong and Shaohan Huang and Zewen Chi and Wenhui Wang and Shuming Ma and Furu Wei},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Language Models are General-Purpose Interfaces},\n volume = {abs/2206.06336},\n year = {2022}\n}\n"
                    ],
                    "title": "Pre-trained Language Models"
                },
                "Dealing with long texts": {
                    "content": "Approaches to handling long texts vary across different disciplines and applications. In the context of speaker verification, researchers have proposed algorithms to improve the robustness of embedding-based deep convolution neural networks (CNNs) for longer duration utterances [1]. Specifically, the discriminability of embeddings in speaker recognition is enhanced by reducing intra-speaker variation using center loss and increasing inter-speaker discrepancy with softmax loss [1]. When examining long texts in the context of ancient Near East archives, archaeologists have found that these archives were accumulated as long as they were considered useful, and they were organized using a classification system, giving insights into their application and utility [4]. Evaporation studies across different fields, such as oceanography and hydrology, involve varied approaches depending on the constraints of the system of interest [5]. For example, oceanographers focus on the aerodynamic approach when dealing with evaporation from large bodies of water, while hydrologists may use different methods such as water budgets or empirical approaches when evaluating long-term losses from heterogeneous basins [5]. These examples demonstrate the diverse methodologies that various disciplines employ to tackle the challenges associated with processing and analyzing long texts.",
                    "references": [
                        "@Article{Li2018DeepDE,\n author = {Na Li and Deyi Tuo and Dan Su and Zhifeng Li and Dong Yu},\n booktitle = {Interspeech},\n pages = {2262-2266},\n title = {Deep Discriminative Embeddings for Duration Robust Speaker Verification},\n year = {2018}\n}\n",
                        "@Conference{Sadhana2017MiningTO,\n author = {S. Sadhana and L. Sairamesh and S. Sabena and S. Ganapathy and A. Kannan},\n booktitle = {2017 Second International Conference on Recent Trends and Challenges in Computational Models (ICRTCCM)},\n journal = {2017 Second International Conference on Recent Trends and Challenges in Computational Models (ICRTCCM)},\n pages = {196-200},\n title = {Mining Target Opinions from Online Reviews Using Semi-supervised Word Alignment Model},\n year = {2017}\n}\n",
                        "@Article{Limsopatham2016BidirectionalLF,\n author = {Nut Limsopatham and Nigel Collier},\n booktitle = {NUT@COLING},\n pages = {145-152},\n title = {Bidirectional LSTM for Named Entity Recognition in Twitter Messages},\n year = {2016}\n}\n",
                        "@Inproceedings{Michel2018ConstitutionCF,\n author = {C\u00e9cile Michel},\n pages = {43-70},\n title = {Constitution, Contents, Filing and Use of Private Archives: The Case of Old Assyrian Archives (nineteenth century BCE)},\n volume = {11},\n year = {2018}\n}\n",
                        "@Article{Jobson1982EvaporationIT,\n author = {H. E. Jobson},\n journal = {Eos, Transactions American Geophysical Union},\n pages = {1223-1224},\n title = {Evaporation Into the Atmosphere: Theory, History, and Applications},\n volume = {63},\n year = {1982}\n}\n"
                    ],
                    "title": "Dealing with long texts"
                }
            },
            "content": "Interaction-focused systems have gained significant popularity and interest in recent years due to the increasing need for personalized and efficient information processing. Recommender systems are one example of such technology, where the aim is to provide personalized suggestions based on users' preferences and behavior [1,3]. These systems rely on algorithms that gather and analyze data to optimize the user experience, making them highly relevant in the age of information overload. An important aspect of interaction-focused systems is managing the continuous and rapidly changing data streams that characterize modern applications [8]. These data streams pose unique challenges in terms of query processing, algorithmic design, and understanding the underlying structure and relationships within the data.\n\nAnother key aspect of interaction-focused systems is the development and integration of real-time systems, which are essential for distributed embedded applications that require deterministic, composable, and fault-tolerant designs [7]. Real-time systems must meet stringent requirements in terms of timing, stability, and accuracy to ensure that the desired outcomes are achieved in a timely and consistent manner.\n\nSwitching in systems and control is another important topic within the realm of interaction-focused systems [6]. Switching mechanisms help stabilize systems under arbitrary conditions and deal with the inherent complexities associated with sensor or actuator constraints and large modeling uncertainties. Additionally, the incorporation of deep learning systems, particularly in the area of automated whitebox testing, has opened new doors for improving the design, accuracy, and efficiency of interaction-focused applications [5].\n\nIn conclusion, the development of interaction-focused systems involves a multifaceted approach that combines insights from diverse fields such as recommender systems, data stream processing, real-time systems, switching mechanisms, and deep learning algorithms. Such systems hold the potential to revolutionize the way users interact with technology and process information in an increasingly interconnected world.",
            "references": [
                "@Inproceedings{Ricci2011IntroductionTR,\n author = {F. Ricci and L. Rokach and Bracha Shapira},\n booktitle = {Recommender Systems Handbook},\n pages = {1-35},\n title = {Introduction to Recommender Systems Handbook},\n year = {2011}\n}\n",
                "@Inproceedings{Cassandras1999IntroductionTD,\n author = {C. Cassandras and S. Lafortune},\n booktitle = {The Kluwer International Series on Discrete Event Dynamic Systems},\n pages = {1-823},\n title = {Introduction to Discrete Event Systems},\n volume = {11},\n year = {1999}\n}\n",
                "@Inproceedings{Ricci2015RecommenderSI,\n author = {F. Ricci and L. Rokach and Bracha Shapira},\n booktitle = {Recommender Systems Handbook},\n pages = {1-34},\n title = {Recommender Systems: Introduction and Challenges},\n year = {2015}\n}\n",
                "@Article{Kriegeskorte2008RepresentationalSA,\n author = {N. Kriegeskorte and Marieke Mur and P. Bandettini},\n booktitle = {Frontiers in Systems Neuroscience},\n journal = {Frontiers in Systems Neuroscience},\n title = {Representational Similarity Analysis \u2013 Connecting the Branches of Systems Neuroscience},\n volume = {2},\n year = {2008}\n}\n",
                "@Article{Pei2017DeepXploreAW,\n author = {Kexin Pei and Yinzhi Cao and Junfeng Yang and S. Jana},\n booktitle = {Symposium on Operating Systems Principles},\n journal = {Proceedings of the 26th Symposium on Operating Systems Principles},\n title = {DeepXplore: Automated Whitebox Testing of Deep Learning Systems},\n year = {2017}\n}\n",
                "@Inproceedings{Liberzon2003SwitchingIS,\n author = {D. Liberzon},\n booktitle = {Systems & Control: Foundations & Applications},\n pages = {1-233},\n title = {Switching in Systems and Control},\n year = {2003}\n}\n",
                "@Book{Kopetz2011RealTimeSD,\n author = {H. Kopetz},\n booktitle = {Real-Time Systems Series},\n journal = {Real-Time Systems},\n title = {Real-Time Systems: Design Principles for Distributed Embedded Applications},\n year = {2011}\n}\n",
                "@Article{Babcock2002ModelsAI,\n author = {Brian Babcock and Shivnath Babu and Mayur Datar and R. Motwani and J. Widom},\n booktitle = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},\n pages = {1-16},\n title = {Models and issues in data stream systems},\n year = {2002}\n}\n"
            ]
        }
    }
}