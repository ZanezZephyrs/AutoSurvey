{
    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "subsections": {
                "BOW Encodings": {
                    "content": "Bag-of-words (BOW) model has been a common baseline representation technique for text data used in various tasks, including text ranking, summarization, and retrieval [4,5,6]. BOW has a simple yet efficient representation by treating text as an unordered set or \"bag\" of words. While it does capture the frequency of individual terms, the BOW model falls short in encoding the semantic relationship between words when comparing sentences [4]. Recently, alternative text representation methods have emerged to enrich the BOW model's efficiency, resulting in improved performance across multiple domains.\n\nIn preprocessing text data for more advanced models, distributed representations like pretrained transformers such as BERT [2] have taken the center stage. Representations for ranking involve powerful transformer-based encoders to model queries and text corpus, addressing the challenge in capturing relevance and model balance [2]. On the other hand, researchers have also explored models that directly learn term-based sparse representation in the full vocabulary space [3]. For instance, SparTerm integrates an importance predictor and a gating controller to ensure sparsity flexibility and improve the ranking performance of term-based representations while maintaining interpretability and efficiency [3]. \n\nThe integration of both sparse and dense representations such as the ones provided by transformers can have significant impact on the overall efficiency and effectiveness of text-related tasks [6] . As deep learning models become increasingly sophisticated with high query latency costs, it is essential to balance the benefits of these new representations with their efficiency implications [6]. Despite BOW's simplicity, integrating new strategies, and techniques that extend its functionality can prove valuable in making text representations even more practical, interpretable, and efficient.",
                    "references": [
                        "@Article{Lin2020PretrainedTF,\n author = {Jimmy J. Lin and Rodrigo Nogueira and Andrew Yates},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},\n title = {Pretrained Transformers for Text Ranking: BERT and Beyond},\n year = {2020}\n}\n",
                        "@Article{Bai2020SparTermLT,\n author = {Yang Bai and Xiaoguang Li and Gang Wang and Chaoliang Zhang and Lifeng Shang and Jun Xu and Zhaowei Wang and Fangshan Wang and Qun Liu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval},\n volume = {abs/2010.00768},\n year = {2020}\n}\n",
                        "@Article{Mani2017MultiDocumentSU,\n author = {Kaustubh Mani and Ishan Verma and Lipika Dey},\n booktitle = {International Conference on Wirtschaftsinformatik},\n journal = {2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI)},\n pages = {672-675},\n title = {Multi-Document Summarization Using Distributed Bag-of-Words Model},\n year = {2017}\n}\n",
                        "@Article{Wang2015IncorporatingLK,\n author = {Yan Wang and Zhiyuan Liu and Maosong Sun},\n booktitle = {PLoS ONE},\n journal = {PLoS ONE},\n title = {Incorporating Linguistic Knowledge for Learning Distributed Word Representations},\n volume = {10},\n year = {2015}\n}\n",
                        "@Article{Zhuang2021FastPR,\n author = {Shengyao Zhuang and G. Zuccon},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Fast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansion},\n volume = {abs/2108.08513},\n year = {2021}\n}\n",
                        "@Article{Lin2020DistillingDR,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Jimmy J. Lin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Distilling Dense Representations for Ranking using Tightly-Coupled Teachers},\n volume = {abs/2010.11386},\n year = {2020}\n}\n",
                        "@Article{Paul2019RankingAS,\n author = {Debjit Paul and A. Frank},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {3671-3681},\n title = {Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs},\n year = {2019}\n}\n",
                        "@Article{Lin2021InBatchNF,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Jimmy J. Lin},\n booktitle = {Workshop on Representation Learning for NLP},\n pages = {163-173},\n title = {In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval},\n year = {2021}\n}\n"
                    ],
                    "title": "BOW Encodings"
                }
            },
            "content": "Text representations play a crucial role in tasks such as information retrieval, text classification, and document similarity measurement [6]. Different approaches have been proposed to construct text representations, such as the graph-of-words (GoW) method, which captures co-occurrence relationships between words [6]. Additionally, attentional mechanisms have been introduced to enhance the expressiveness of text representations in various neural models [5]. Sparse and dense methods, as well as hybrid sparse-dense techniques, can also be used to improve the precision of text retrieval [2]. Furthermore, n-grams and different weighting schemes, like term frequency, tf-idf, and binary schemes, have been examined to optimize text representations for tasks like ranking [9]. Overall, leveraging diverse representation techniques and models can help enhance the ability to rank and retrieve relevant documents and improve overall performance in various natural language processing tasks.",
            "references": [
                "@Article{Luan2020SparseDA,\n author = {Y. Luan and Jacob Eisenstein and Kristina Toutanova and M. Collins},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {329-345},\n title = {Sparse, Dense, and Attentional Representations for Text Retrieval},\n volume = {9},\n year = {2020}\n}\n",
                "@Book{Gao2019LearningGP,\n author = {Hongyang Gao and Yongjun Chen and Shuiwang Ji},\n booktitle = {The Web Conference},\n journal = {The World Wide Web Conference},\n title = {Learning Graph Pooling and Hybrid Convolutional Operations for Text Representations},\n year = {2019}\n}\n",
                "@Article{Lyu2020TowardsDP,\n author = {Lingjuan Lyu and Yitong Li and Xuanli He and Tong Xiao},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {Towards Differentially Private Text Representations},\n year = {2020}\n}\n",
                "@Article{Yu2022CoCaCC,\n author = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {CoCa: Contrastive Captioners are Image-Text Foundation Models},\n volume = {2022},\n year = {2022}\n}\n",
                "@Article{Chen2019UNITERLU,\n author = {Yen-Chun Chen and Linjie Li and Licheng Yu and Ahmed El Kholy and Faisal Ahmed and Zhe Gan and Yu Cheng and Jingjing Liu},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {UNITER: Learning UNiversal Image-TExt Representations},\n volume = {abs/1909.11740},\n year = {2019}\n}\n",
                "@Article{Zayats2021RepresentationsFQ,\n author = {V. Zayats and Kristina Toutanova and Mari Ostendorf},\n booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},\n pages = {2895-2906},\n title = {Representations for Question Answering from Documents with Tables and Text},\n year = {2021}\n}\n",
                "@Article{Liu2017LearningST,\n author = {Yang Liu and Mirella Lapata},\n booktitle = {International Conference on Topology, Algebra and Categories in Logic},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {63-75},\n title = {Learning Structured Text Representations},\n volume = {6},\n year = {2017}\n}\n",
                "@Article{Yogatama2015BayesianOO,\n author = {Dani Yogatama and Lingpeng Kong and Noah A. Smith},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2100-2105},\n title = {Bayesian Optimization of Text Representations},\n year = {2015}\n}\n"
            ]
        }
    }
}