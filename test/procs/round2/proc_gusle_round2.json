{
    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "subsections": {
                "LTR Features": {
                    "content": "Text representations play a crucial role in neural information  retrieval systems, particularly in the context of ranking. The goal of ranking is to match processed queries with indexed documents  effectively and efficiently [1]. Learning to rank (LTR)  techniques have emerged as a promising approach to improve ranking  models by leveraging machine learning technologies [1]. In this  section, we discuss the use of LTR features for text  representations in ranking. One important aspect of LTR features is the learning time, which  can increase with the sample size [0]. To address this,  researchers have investigated the number of relevant documents  identified for query sets when the number of retrieved documents is unconstrained [0]. It has been observed that precision tends to  fall below a certain threshold after a certain rank, indicating  that retrieving additional documents may yield minimal gains in  terms of relevance [0]. Therefore, an original sample of a  sufficient number of documents, such as 5000, has been deemed  suitable for experiments [0]. Different approaches have been proposed for learning to rank,  including pointwise, pairwise, and listwise approaches [1].  These approaches provide frameworks and algorithms for ranking  models, each with its own theoretical properties [1]. For  instance, PRank is a ranker based on the perceptron algorithm,  which maps a feature vector to the reals using learned weights  [4]. PRank also learns the values of increasing thresholds to  determine the rank of a document [7]. On the other hand,  LambdaRank and LambdaMART update their parameters differently, with LambdaRank updating all weights after each query and LambdaMART  updating only a few parameters at a time [3]. Ordinal regression has also been employed in learning to rank,  where the problem is cast as learning the mapping of an input  vector to an ordered set of numerical ranks [4]. The positions  of rank boundaries play a critical role in the final ranking  function [4]. Additionally, the use of sampling has been  motivated by the need for efficient application of a learned model, reducing the number of documents for which features are calculated  [5]. This is particularly advantageous when certain features are computationally expensive [5]. In learning to rank, the independence and identically distributed  (i.i.d) assumption does not hold for every example document in the  training data [6]. Instead, documents associated with each query form a group, where the groups are i.i.d but the documents within a group are not i.i.d [6]. Furthermore, the learning time of many  LTR techniques increases with the number of documents in the sample [6]. Therefore, careful consideration of the sample size is  necessary to balance efficiency and effectiveness [6]. In summary, text representations for ranking in neural information  retrieval systems involve the use of LTR features. These features  are designed to optimize the ranking process by considering factors such as learning time, different learning approaches, ordinal  regression, and the use of sampling. By leveraging these features,  researchers aim to improve the efficiency and effectiveness of  ranking models in information retrieval applications.",
                    "references": [
                        "@Book{Zamani2017NeuralRM,\n author = {Hamed Zamani and Bhaskar Mitra and Xia Song and Nick Craswell and Saurabh Tiwary},\n booktitle = {Web Search and Data Mining},\n journal = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},\n title = {Neural Ranking Models with Multiple Document Fields},\n year = {2017}\n}\n",
                        "@Article{Guo2019ADL,\n author = {J. Guo and Yixing Fan and Liang Pang and Liu Yang and Qingyao Ai and Hamed Zamani and Chen Wu and W. Bruce Croft and Xueqi Cheng},\n booktitle = {Information Processing & Management},\n journal = {ArXiv},\n title = {A Deep Look into Neural Ranking Models for Information Retrieval},\n volume = {abs/1903.06902},\n year = {2019}\n}\n",
                        "@Article{Forman2008BNSFS,\n author = {George Forman},\n booktitle = {International Conference on Information and Knowledge Management},\n pages = {263-270},\n title = {BNS feature scaling: an improved representation over tf-idf for svm text classification},\n year = {2008}\n}\n",
                        "@Article{Han2020LearningtoRankWB,\n author = {Shuguang Han and Xuanhui Wang and Michael Bendersky and Marc Najork},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning-to-Rank with BERT in TF-Ranking},\n volume = {abs/2004.08476},\n year = {2020}\n}\n",
                        "@Article{Lin2020PretrainedTF,\n author = {Jimmy J. Lin and Rodrigo Nogueira and Andrew Yates},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},\n title = {Pretrained Transformers for Text Ranking: BERT and Beyond},\n year = {2020}\n}\n"
                    ],
                    "title": "LTR Features"
                }
            },
            "content": "",
            "references": [
                "@Article{Mitra2017NeuralMF,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Models for Information Retrieval},\n volume = {abs/1705.01509},\n year = {2017}\n}\n",
                "@Book{Mitra2016LearningTM,\n author = {Bhaskar Mitra and Fernando Diaz and Nick Craswell},\n booktitle = {The Web Conference},\n journal = {Proceedings of the 26th International Conference on World Wide Web},\n title = {Learning to Match using Local and Distributed Representations of Text for Web Search},\n year = {2016}\n}\n",
                "@Article{Craswell2016NeuIRTS,\n author = {Nick Craswell and W. Bruce Croft and J. Guo and Bhaskar Mitra and M. de Rijke},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval},\n title = {Neu-IR: The SIGIR 2016 Workshop on Neural Information Retrieval},\n year = {2016}\n}\n",
                "@Article{Wu2016LearningOM,\n author = {Fei Wu and Xinyan Lu and Jun Song and Shuicheng Yan and Zhongfei Zhang and Y. Rui and Yueting Zhuang},\n booktitle = {IEEE Transactions on Image Processing},\n journal = {IEEE Transactions on Image Processing},\n pages = {630-642},\n title = {Learning of Multimodal Representations With Random Walks on the Click Graph},\n volume = {25},\n year = {2016}\n}\n",
                "@Inproceedings{Bartell1994OptimizingRF,\n author = {B. Bartell},\n title = {Optimizing ranking functions: a connectionist approach to adaptive information retrieval},\n year = {1994}\n}\n"
            ]
        },
        "Interaction-focused Systems": {
            "subsections": {
                "Convolutional Neural Networks": {
                    "content": "Convolutional Neural Networks (CNNs) have gained significant  attention in the field of neural information retrieval due to their ability to capture local patterns and interactions between queries  and documents. In this section, we will explore the use of CNNs in  interaction-focused systems for information retrieval. One important consideration in the design of CNN-based models is  the selection of hyperparameters. Limited computational resources  necessitate the careful determination of hyperparameter ranges  based on pilot experiments and domain insights [0]. For  instance, the choice of the number of filters, kernel size, and  pooling sizes can significantly impact the performance of the model [1]. Additionally, the selection of hyperparameters such as the  dimensionality of the embeddings and the number of layers can also  influence the model's effectiveness [2]. Positional information plays a crucial role in capturing the  interactions between queries and documents. However, incorporating  positional information in deep neural IR models is non-trivial.  Models like MatchPyramid and local DUET have attempted to account  for positional information by incorporating convolutional layers  based on similarity matrices between queries and documents [1].  Despite these efforts, these models have struggled to outperform  the DRMM model, indicating the challenges in effectively utilizing  positional information in multi-dimensional interactions [1]. One of the key advantages of CNNs in neural IR is their ability to  model n-grams. Traditional IR approaches treat n-grams as discrete  terms, which can lead to data sparsity and an explosion in the  parameter space. CNNs address this issue by learning a  convolutional layer that forms n-grams from individual word  embeddings, allowing for the matching of n-grams of different  lengths [2]. This approach enables the model to capture the  semantic relationships between query terms and document content,  enhancing the retrieval performance [2]. Several interaction-focused models have been proposed in the  literature. For instance, Deep Match Tree defines interactions in  the product space of dependency trees, leveraging a deep neural  network to make matching decisions based on these local  interactions [3]. Match-SRNN, on the other hand, models the  recursive matching structure in local interactions to capture long- distance dependencies [3]. These models have been evaluated on  various tasks, including short text matching, community-based  question answering, and paper citation matching [3]. However, it is important to note that most of these deep matching models are  primarily designed for semantic matching problems, which differ  significantly from the relevance matching problem in ad-hoc  retrieval [3]. The application of deep learning in information retrieval has the  potential to revolutionize the field. Deep neural networks have the ability to discover hidden structures and features at different  levels of abstraction, which can be beneficial for IR tasks [4]. The success of deep learning in other domains, such as computer  vision, speech recognition, and natural language processing,  suggests that it can have a significant impact on IR as well  [4]. In summary, CNNs have emerged as a powerful tool in interaction- focused systems for neural information retrieval. By effectively  capturing local patterns and interactions between queries and  documents, CNN-based models have shown promise in improving  retrieval performance. However, challenges remain in incorporating  positional information and designing models that can handle multi- dimensional interactions. Further research is needed to explore and enhance the capabilities of CNNs in neural information retrieval.",
                    "references": [
                        "@Article{Hu2018GatherExciteEF,\n author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and A. Vedaldi},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks},\n volume = {abs/1810.12348},\n year = {2018}\n}\n",
                        "@Article{Kravchik2018DetectingCA,\n author = {Moshe Kravchik and A. Shabtai},\n booktitle = {CPS-SPC@CCS},\n journal = {Proceedings of the 2018 Workshop on Cyber-Physical Systems Security and PrivaCy},\n title = {Detecting Cyber Attacks in Industrial Control Systems Using Convolutional Neural Networks},\n year = {2018}\n}\n",
                        "@Article{Ying2018GraphCN,\n author = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and J. Leskovec},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},\n year = {2018}\n}\n",
                        "@Article{Defferrard2016ConvolutionalNN,\n author = {M. Defferrard and X. Bresson and P. Vandergheynst},\n booktitle = {NIPS},\n pages = {3837-3845},\n title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},\n year = {2016}\n}\n",
                        "@Article{Ryczko2017ConvolutionalNN,\n author = {Kevin Ryczko and Kyle Mills and Iryna Luchak and Christa M. Homenick and Isaac Tamblyn},\n booktitle = {Computational materials science},\n journal = {Computational Materials Science},\n title = {Convolutional neural networks for atomistic systems},\n year = {2017}\n}\n"
                    ],
                    "title": "Convolutional Neural Networks"
                },
                "Pre-trained Language Models": {
                    "content": "Pre-trained language models have revolutionized the field of neural information retrieval by providing a powerful foundation for  various natural language processing tasks. In recent years, there  has been a surge of interest in interaction-focused systems that  leverage pre-trained language models to enhance the retrieval  process. These systems aim to improve the effectiveness and  efficiency of information retrieval by incorporating user  interactions and feedback into the retrieval process. One prominent example of interaction-focused systems is the use of  pre-trained language models such as BERT (Bidirectional Encoder  Representations from Transformers) [1]. BERT has demonstrated  remarkable performance across a wide range of tasks, including  natural language inference, question answering, and sentiment  analysis. Its success can be attributed to its ability to capture  contextual information and generate meaningful representations of  text. The application of pre-trained language models in interaction- focused systems involves fine-tuning the models on specific  retrieval tasks. This process typically involves two stages: pre- training and fine-tuning. During pre-training, the language model  is trained on a large corpus of text to learn general language  understanding [2]. Fine-tuning, on the other hand, involves  training the model on task-specific data to adapt it to the  retrieval task at hand. One key advantage of pre-trained language models is their ability  to capture semantic relationships between words and phrases. This  enables them to understand the context of a query and retrieve  relevant information accordingly. For example, BERT has been shown  to outperform previous state-of-the-art models on various benchmark datasets, achieving significant improvements in accuracy [1]. Another important aspect of interaction-focused systems is the  incorporation of user interactions and feedback. These systems aim  to leverage user feedback, such as relevance judgments or explicit  feedback, to improve the retrieval process. By incorporating user  interactions, the models can adapt and refine their retrieval  strategies based on user preferences and information needs. The effectiveness of interaction-focused systems using pre-trained  language models has been demonstrated in various studies. For  instance, researchers have explored the use of BERT in question  answering systems, where user interactions play a crucial role in  refining the answers provided [4]. By incorporating user  feedback, these systems can iteratively improve the quality of the  answers and provide more accurate and relevant information. Furthermore, the use of pre-trained language models in interaction- focused systems has also shown promise in text generation tasks.  Models such as BART (Bidirectional and Auto-Regressive  Transformers) have been developed to generate coherent and  contextually relevant text [2]. By fine-tuning these models on  specific text generation tasks, they can generate high-quality  responses to user queries, enhancing the overall user experience. In conclusion, pre-trained language models have emerged as a  powerful tool in the development of interaction-focused systems for neural information retrieval. These models, such as BERT and BART,  provide a solid foundation for capturing semantic relationships and understanding user interactions. By incorporating user feedback and fine-tuning the models on specific retrieval tasks, interaction- focused systems can significantly enhance the effectiveness and  efficiency of information retrieval. Future research in this area  should focus on exploring novel techniques to further improve the  performance of these systems and address the challenges associated  with user interactions in the retrieval process.",
                    "references": [
                        "@Article{Budzianowski2019HelloIG,\n author = {Pawe\u0142 Budzianowski and Ivan Vulic},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Hello, It\u2019s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems},\n volume = {abs/1907.05774},\n year = {2019}\n}\n",
                        "@Article{Li2019PretrainedLM,\n author = {Liangyou Li and Xin Jiang and Qun Liu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Pretrained Language Models for Document-Level Neural Machine Translation},\n volume = {abs/1911.03110},\n year = {2019}\n}\n",
                        "@Article{Zhao2022DenseTR,\n author = {Wayne Xin Zhao and Jing Liu and Ruiyang Ren and Ji-rong Wen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dense Text Retrieval based on Pretrained Language Models: A Survey},\n volume = {abs/2211.14876},\n year = {2022}\n}\n",
                        "@Article{Lin2020ConversationalQR,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Rodrigo Nogueira and Ming-Feng Tsai and Chuan-Ju Wang and Jimmy J. Lin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models},\n volume = {abs/2004.01909},\n year = {2020}\n}\n",
                        "@Article{Cui2022M6RecGP,\n author = {Zeyu Cui and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems},\n volume = {abs/2205.08084},\n year = {2022}\n}\n"
                    ],
                    "title": "Pre-trained Language Models"
                },
                "Dealing with long texts": {
                    "content": "Dealing with long texts poses a significant challenge in neural  information retrieval systems. Long documents often contain  multiple passages that may be relevant to a user's query, requiring effective methods to handle and retrieve the most relevant  information. In this section, we discuss various approaches and  techniques employed by interaction-focused systems to address this  challenge. One approach to handling long texts is the use of passage-based  retrieval models. These models aim to identify and rank relevant  passages within a document, rather than treating the entire  document as a single unit [4]. For instance, PARADE (Passage  Ranking with Document-level Context) is an end-to-end document  reranking model that incorporates diverse relevance signals from  the full text into ad-hoc ranking [0]. By aggregating relevance  signals across passages, PARADE improves the effectiveness of  retrieval, particularly when the number of relevant passages per  document is low [0]. Similarly, Birch-Passage, an improved  variant of the Birch model, utilizes passages instead of sentences  as input and is trained end-to-end on the target corpus [2].  These passage-based models demonstrate the importance of  considering passage-level relevance in long document retrieval. Another approach is the use of passage aggregation techniques to  determine the overall relevance score of a document. ELECTRA-MaxP,  for example, adopts the maximum score of passages within a document as the overall relevance score [2]. This approach allows for a  more comprehensive representation of the document's relevance,  taking into account the varying degrees of relevance across  different passages. ELECTRA-KNRM, on the other hand, is a kernel- pooling neural ranking model that leverages the query-document  similarity matrix to capture the relevance between passages and the query [3]. These techniques highlight the importance of  effectively aggregating evidence from multiple passages to improve  retrieval performance. In addition to passage-based models and aggregation techniques, the choice of dataset also plays a crucial role in addressing the  challenge of long texts. Different datasets exhibit variations in  document length and the number of relevant passages per document.  For instance, the TREC DL and MS MARCO datasets share similarities  in query overlap, suggesting that queries in both collections can  be sufficiently answered by a single highly relevant passage  [6]. On the other hand, the Genomics dataset contains \"natural\"  passages that can be longer, requiring consideration when drawing  conclusions [1]. Understanding the characteristics of the  dataset is essential for developing effective retrieval models  tailored to specific contexts. Furthermore, the evaluation of interaction-focused systems on long  texts is facilitated by the availability of benchmark collections.  The TREC-COVID challenge, for example, was developed to address the urgent demand for reliable retrieval of COVID-19 academic  literature [7]. This challenge utilizes the CORD-19 dataset,  which is a dynamic collection enlarged over time [7]. The  availability of such benchmark collections enables researchers to  evaluate and compare the performance of their systems in retrieving information from long texts. In summary, interaction-focused systems employ various techniques  to handle long texts in neural information retrieval. Passage-based models, passage aggregation techniques, dataset characteristics,  and benchmark collections all contribute to addressing the  challenges associated with long documents. These approaches aim to  improve the effectiveness of retrieval by considering passage-level relevance and effectively aggregating evidence from multiple  passages.",
                    "references": [
                        "@Article{Li2018DeepDE,\n author = {Na Li and Deyi Tuo and Dan Su and Zhifeng Li and Dong Yu},\n booktitle = {Interspeech},\n pages = {2262-2266},\n title = {Deep Discriminative Embeddings for Duration Robust Speaker Verification},\n year = {2018}\n}\n",
                        "@Article{Bordel2016ProbabilisticKF,\n author = {Germ\u00e1n Bordel and M. Pe\u00f1agarikano and Luis Javier Rodriguez-Fuentes and Aitor \u00c1lvarez and A. Varona},\n booktitle = {IEEE Signal Processing Letters},\n journal = {IEEE Signal Processing Letters},\n pages = {126-129},\n title = {Probabilistic Kernels for Improved Text-to-Speech Alignment in Long Audio Tracks},\n volume = {23},\n year = {2016}\n}\n",
                        "@Article{Montfort2007OrderingEI,\n author = {Nick Montfort},\n booktitle = {AAAI Fall Symposium: Intelligent Narrative Technologies},\n pages = {87-94},\n title = {Ordering Events in Interactive Fiction Narratives},\n year = {2007}\n}\n",
                        "@Conference{Sadhana2017MiningTO,\n author = {S. Sadhana and L. Sairamesh and S. Sabena and S. Ganapathy and A. Kannan},\n booktitle = {2017 Second International Conference on Recent Trends and Challenges in Computational Models (ICRTCCM)},\n journal = {2017 Second International Conference on Recent Trends and Challenges in Computational Models (ICRTCCM)},\n pages = {196-200},\n title = {Mining Target Opinions from Online Reviews Using Semi-supervised Word Alignment Model},\n year = {2017}\n}\n",
                        "@Article{Limsopatham2016BidirectionalLF,\n author = {Nut Limsopatham and Nigel Collier},\n booktitle = {NUT@COLING},\n pages = {145-152},\n title = {Bidirectional LSTM for Named Entity Recognition in Twitter Messages},\n year = {2016}\n}\n"
                    ],
                    "title": "Dealing with long texts"
                }
            },
            "content": "",
            "references": [
                "@Article{Mitra2018AnIT,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {Foundations and Trends in Information Retrieval},\n journal = {Found. Trends Inf. Retr.},\n pages = {1-126},\n title = {An Introduction to Neural Information Retrieval},\n volume = {13},\n year = {2018}\n}\n",
                "@Book{Kenter2017NeuralNF,\n author = {Tom Kenter and Alexey Borisov and Christophe Van Gysel and Mostafa Dehghani and M. de Rijke and Bhaskar Mitra},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {Neural Networks for Information Retrieval},\n year = {2017}\n}\n",
                "@Book{Gao2022NeuralAT,\n author = {Jianfeng Gao and Chenyan Xiong and Paul Bennett and Nick Craswell},\n booktitle = {The Information Retrieval Series},\n journal = {Neural Approaches to Conversational Information Retrieval},\n title = {Neural Approaches to Conversational Information Retrieval},\n year = {2022}\n}\n",
                "@Article{Yang2018ResponseRW,\n author = {Liu Yang and Minghui Qiu and Chen Qu and J. Guo and Yongfeng Zhang and W. Bruce Croft and Jun Huang and Haiqing Chen},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval},\n title = {Response Ranking with Deep Matching Networks and External Knowledge in Information-seeking Conversation Systems},\n year = {2018}\n}\n",
                "@Article{Liu2018EntityDuetNR,\n author = {Zhenghao Liu and Chenyan Xiong and Maosong Sun and Zhiyuan Liu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2395-2405},\n title = {Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval},\n year = {2018}\n}\n"
            ]
        }
    }
}