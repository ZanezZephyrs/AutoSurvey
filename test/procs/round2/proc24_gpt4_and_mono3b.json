{
    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "subsections": {
                "LTR Features": {
                    "content": "Learning-to-Rank (LTR) algorithm has proven quite effective in the realm of information retrieval, especially when integrated into neural ranking models[2]. These LTR models, such as RankSVM and LambdaMart, traditionally utilize human-designed features to improve ranking performance[2]. In a similar vein, exploratory research has been conducted to leverage other evaluative means to enhance text classification. One such technique is Bi-Normal Separation (BNS), which replaces the conventional IDF used in TF-IDF representation, demonstrating remarkably improved performance in terms of SVM accuracy and F-measure, even sometimes eliminating the need for additional feature selection[3]. \n\nNRM-F has also shown effectiveness in eliminating the need for handcrafted feature engineering for ad-hoc retrieval and leads to superior retrieval performance by learning an accurate document representation[1]. The addition of BERT into these models, as seen in the TFR-BERT model, provides a way of allowing LTR models to fine-tune BERT representations of query-document pairs within the TF-Ranking environment. This multidimensional representation learning approach evidenced robust performance for passage ranking tasks on the MS MACRO benchmark[4]. Ultimately, the effectiveness of these LTR features in ranking tasks emphasizes the potential for evolving applications in various text-ranking contexts, hinting at a promising future for the field[5].",
                    "references": [
                        "@Book{Zamani2017NeuralRM,\n author = {Hamed Zamani and Bhaskar Mitra and Xia Song and Nick Craswell and Saurabh Tiwary},\n booktitle = {Web Search and Data Mining},\n journal = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},\n title = {Neural Ranking Models with Multiple Document Fields},\n year = {2017}\n}\n",
                        "@Article{Guo2019ADL,\n author = {J. Guo and Yixing Fan and Liang Pang and Liu Yang and Qingyao Ai and Hamed Zamani and Chen Wu and W. Bruce Croft and Xueqi Cheng},\n booktitle = {Information Processing & Management},\n journal = {ArXiv},\n title = {A Deep Look into Neural Ranking Models for Information Retrieval},\n volume = {abs/1903.06902},\n year = {2019}\n}\n",
                        "@Article{Forman2008BNSFS,\n author = {George Forman},\n booktitle = {International Conference on Information and Knowledge Management},\n pages = {263-270},\n title = {BNS feature scaling: an improved representation over tf-idf for svm text classification},\n year = {2008}\n}\n",
                        "@Article{Han2020LearningtoRankWB,\n author = {Shuguang Han and Xuanhui Wang and Michael Bendersky and Marc Najork},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning-to-Rank with BERT in TF-Ranking},\n volume = {abs/2004.08476},\n year = {2020}\n}\n",
                        "@Article{Lin2020PretrainedTF,\n author = {Jimmy J. Lin and Rodrigo Nogueira and Andrew Yates},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},\n title = {Pretrained Transformers for Text Ranking: BERT and Beyond},\n year = {2020}\n}\n"
                    ],
                    "title": "LTR Features"
                }
            },
            "content": "Text representations that utilize dense vectors, or embeddings, have become a focal point in the development of neural ranking models for information retrieval. Traditional learning to rank models often employ machine learning techniques over hand-crafted IR characteristics [1]. The neural models aim to extract raw text and learn language representations that can bridge the gap present between the query and document vocabulary [1]. The approach proposed by Huang et al. (as cited in [2]) uses a distributed representation of the query and document title for the task of document ranking, employing input representation from character trigraphs and ranking document titles that have been previously clicked over randomly chosen titles [2]. The successful employment of distributed representations in document ranking is also evident in other works [2]. \n\nIn the context of multimedia information retrieval, vector-based representations can be used to represent different modalities in a shared feature space. Such practices have led to the development of more cutting-edge models, like the multimodal random walk neural network (MRW-NN), which leverages user click data as a large graph and aims to encode explicit and implicit relevance relationships between vertices in the click graph [4]. The performance achieved by MRW-NN on cross-modal retrieval tasks indicates that this type of approach is both efficient and effective [4]. In addition to these innovative applications of neural networks in ranking tasks, adaptive methods have been proposed to numerically optimise parameters used by retrieval systems, thereby improving their ability to rank documents more effectively [5]. This becomes increasingly significant when considering the inherent ambiguities present within natural language, large data collection sizes, and the diverse needs of users [5].",
            "references": [
                "@Article{Mitra2017NeuralMF,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Models for Information Retrieval},\n volume = {abs/1705.01509},\n year = {2017}\n}\n",
                "@Book{Mitra2016LearningTM,\n author = {Bhaskar Mitra and Fernando Diaz and Nick Craswell},\n booktitle = {The Web Conference},\n journal = {Proceedings of the 26th International Conference on World Wide Web},\n title = {Learning to Match using Local and Distributed Representations of Text for Web Search},\n year = {2016}\n}\n",
                "@Article{Craswell2016NeuIRTS,\n author = {Nick Craswell and W. Bruce Croft and J. Guo and Bhaskar Mitra and M. de Rijke},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval},\n title = {Neu-IR: The SIGIR 2016 Workshop on Neural Information Retrieval},\n year = {2016}\n}\n",
                "@Article{Wu2016LearningOM,\n author = {Fei Wu and Xinyan Lu and Jun Song and Shuicheng Yan and Zhongfei Zhang and Y. Rui and Yueting Zhuang},\n booktitle = {IEEE Transactions on Image Processing},\n journal = {IEEE Transactions on Image Processing},\n pages = {630-642},\n title = {Learning of Multimodal Representations With Random Walks on the Click Graph},\n volume = {25},\n year = {2016}\n}\n",
                "@Inproceedings{Bartell1994OptimizingRF,\n author = {B. Bartell},\n title = {Optimizing ranking functions: a connectionist approach to adaptive information retrieval},\n year = {1994}\n}\n"
            ]
        },
        "Interaction-focused Systems": {
            "subsections": {
                "Convolutional Neural Networks": {
                    "content": "Convolutional neural networks (CNNs) have proven successful in a plethora of applications using both traditional grid input data and non-traditional graph-based data. In the context of feature context exploitation, Hu et. al. propose an approach in \"Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks\" which introduces a pair of operators to aggregate and redistribute pooled information to local features, noting the efficiency of this approach in terms of parameters and computation [1]. Leveraging the power of CNNs in anomaly detection, \"Detecting Cyber Attacks in Industrial Control Systems Using Convolutional Neural Networks\" compares the efficiency of different CNN architectures for detecting anomalies in industrial control systems [2].\n\nIn a shift away from traditional grid inputs, convolutional neural network techniques have been applied in web-scale recommender systems and atomistic systems. \"Graph Convolutional Neural Networks for Web-Scale Recommender Systems\" explains the application of GCNs in recommender systems where graph-sturcture is fundamential and the GCN's success in this field is attributed to the ability to leverage both content information as well as graph structure [3]. In extending the applicability of CNNs, the work \"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\" proposes a formulation of CNNs in the context of spectral graph theory, which, as suggested, is proficient in tackling high-dimensional irregular domains, such as social networks or words\u2019 embedding, represented by graphs [4]. A novel approach to atomic structure prediction using CNNs, named CNNAS (convolutional neural networks for atomistic systems), is introduced in \"Convolutional neural networks for atomistic systems\" [5]. This approach predicts energies obtained using density functional theory (DFT) for 2D hexagonal lattices of various types while maintaining the accuracy of ab initio calculations [5]. These broad applications of convolutional neural networks show the adaptability and power of this deep learning approach.",
                    "references": [
                        "@Article{Hu2018GatherExciteEF,\n author = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and A. Vedaldi},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks},\n volume = {abs/1810.12348},\n year = {2018}\n}\n",
                        "@Article{Kravchik2018DetectingCA,\n author = {Moshe Kravchik and A. Shabtai},\n booktitle = {CPS-SPC@CCS},\n journal = {Proceedings of the 2018 Workshop on Cyber-Physical Systems Security and PrivaCy},\n title = {Detecting Cyber Attacks in Industrial Control Systems Using Convolutional Neural Networks},\n year = {2018}\n}\n",
                        "@Article{Ying2018GraphCN,\n author = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and J. Leskovec},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},\n year = {2018}\n}\n",
                        "@Article{Defferrard2016ConvolutionalNN,\n author = {M. Defferrard and X. Bresson and P. Vandergheynst},\n booktitle = {NIPS},\n pages = {3837-3845},\n title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},\n year = {2016}\n}\n",
                        "@Article{Ryczko2017ConvolutionalNN,\n author = {Kevin Ryczko and Kyle Mills and Iryna Luchak and Christa M. Homenick and Isaac Tamblyn},\n booktitle = {Computational materials science},\n journal = {Computational Materials Science},\n title = {Convolutional neural networks for atomistic systems},\n year = {2017}\n}\n"
                    ],
                    "title": "Convolutional Neural Networks"
                },
                "Pre-trained Language Models": {
                    "content": "Pretrained Language Models (PLMs) have taken center stage in natural language processing due to their capacity to handle complex datasets in various language-based applications [1,3,4,5]. Their success has been marked not only by their performance in retrieving text in a densely populated dataset [3] but also in dialogue systems [1] and neural machine translations [2]. By operating solely on text input, PLMs are able to bypass explicit policy and language generation modules and provide concise and accurate information across multiple domains on the scale of an absurdly small amount of task-specific data [1]. Given their proficiency in both dialogue systems and machine translations, PLMs can also be utilized in task-oriented dialogue models and document-level NMT (Neural Machine Translation) [1,2]. \n\nPLMs have been suggested as a robust approach to overcome challenges such as data scarcity in the field of task-oriented dialogue systems absorbing grammatical, syntax, dialogue reasoning, and language generation tasks [1]. This is further substantiated in their application in document-level NMT, where the use of large contexts and manipulation methods has been proposed to manage the influence of the extensive contexts [2]. PLMs such as BERT and its variants have shown versatile performances in retrieving dense text data by employing the pretraining and fine-tuning paradigm [3]. Furthermore, the success of applying these PLMs is evidently highlighted in the context of Conversational Question Reformulation (CQR) and industrial recommender systems, which affirm their adaptability in processing complex tasks and providing reliable recommendations, respectively [4,5].",
                    "references": [
                        "@Article{Budzianowski2019HelloIG,\n author = {Pawe\u0142 Budzianowski and Ivan Vulic},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Hello, It\u2019s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems},\n volume = {abs/1907.05774},\n year = {2019}\n}\n",
                        "@Article{Li2019PretrainedLM,\n author = {Liangyou Li and Xin Jiang and Qun Liu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Pretrained Language Models for Document-Level Neural Machine Translation},\n volume = {abs/1911.03110},\n year = {2019}\n}\n",
                        "@Article{Zhao2022DenseTR,\n author = {Wayne Xin Zhao and Jing Liu and Ruiyang Ren and Ji-rong Wen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dense Text Retrieval based on Pretrained Language Models: A Survey},\n volume = {abs/2211.14876},\n year = {2022}\n}\n",
                        "@Article{Lin2020ConversationalQR,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Rodrigo Nogueira and Ming-Feng Tsai and Chuan-Ju Wang and Jimmy J. Lin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models},\n volume = {abs/2004.01909},\n year = {2020}\n}\n",
                        "@Article{Cui2022M6RecGP,\n author = {Zeyu Cui and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems},\n volume = {abs/2205.08084},\n year = {2022}\n}\n"
                    ],
                    "title": "Pre-trained Language Models"
                },
                "Dealing with long texts": {
                    "content": "Dealing with long texts, particularly those from various text sources like social media posts, long audio tracks, or prose, requires robust and innovative machine learning models. Long texts present numerous challenges, which include data heterogeneity, semantic complexity, and the varying temporal nature of the information they carry [1,2,3,5]. For instance, to handle varying text durations for speaker verification, deep convolutional neural networks (CNNs) with discriminative utterance-level embeddings based on an Inception-ResNet speaker classifier have shown significant results. The proposed methods in [1] optimize the intra-speaker variation with center loss and inter-speaker discrepancy with softmax loss to enhance the discriminability of the embeddings and improve robustness, particularly for long utterances.\n\nIn dealing with long audio-to-text alignments in synthesis applications, probabilistic kernels, leveraging confusion matrices from large corpora, were shown to improve alignments [2]. These kernels serve as sophisticated phonetic decoders that offer noteworthy accuracy in situations where the audio-to-text synchronization presents significant challenges. Similarly, when it comes to managing text in the complex narrative structures in interactive fiction, an innovative ordered tree representation has been proposed [3]. This model utilizes Richenbach\u2019s concepts of speech time, reference time, and event time to determine the grammatical tense involved in non-chronological narrations. Furthermore, the challenges of mining specific target opinions from long texts in online reviews have been tackled by a novel semi-supervised word alignment model (SWAM) that identifies relations among the words in a sentence. The model extracts long-span relationships among words, thereby reducing the parsing errors in informal texts [4]. For named entity recognition in long Twitter messages, a solution that applies bidirectional long short-term memory (LSTM) has proven effective. Such an approach can automatically learn orthographic features regardless of the large and noisy nature of the text data [5]. These innovations underscore the importance of advanced techniques in dealing with long text challenges.",
                    "references": [
                        "@Article{Li2018DeepDE,\n author = {Na Li and Deyi Tuo and Dan Su and Zhifeng Li and Dong Yu},\n booktitle = {Interspeech},\n pages = {2262-2266},\n title = {Deep Discriminative Embeddings for Duration Robust Speaker Verification},\n year = {2018}\n}\n",
                        "@Article{Bordel2016ProbabilisticKF,\n author = {Germ\u00e1n Bordel and M. Pe\u00f1agarikano and Luis Javier Rodriguez-Fuentes and Aitor \u00c1lvarez and A. Varona},\n booktitle = {IEEE Signal Processing Letters},\n journal = {IEEE Signal Processing Letters},\n pages = {126-129},\n title = {Probabilistic Kernels for Improved Text-to-Speech Alignment in Long Audio Tracks},\n volume = {23},\n year = {2016}\n}\n",
                        "@Article{Montfort2007OrderingEI,\n author = {Nick Montfort},\n booktitle = {AAAI Fall Symposium: Intelligent Narrative Technologies},\n pages = {87-94},\n title = {Ordering Events in Interactive Fiction Narratives},\n year = {2007}\n}\n",
                        "@Conference{Sadhana2017MiningTO,\n author = {S. Sadhana and L. Sairamesh and S. Sabena and S. Ganapathy and A. Kannan},\n booktitle = {2017 Second International Conference on Recent Trends and Challenges in Computational Models (ICRTCCM)},\n journal = {2017 Second International Conference on Recent Trends and Challenges in Computational Models (ICRTCCM)},\n pages = {196-200},\n title = {Mining Target Opinions from Online Reviews Using Semi-supervised Word Alignment Model},\n year = {2017}\n}\n",
                        "@Article{Limsopatham2016BidirectionalLF,\n author = {Nut Limsopatham and Nigel Collier},\n booktitle = {NUT@COLING},\n pages = {145-152},\n title = {Bidirectional LSTM for Named Entity Recognition in Twitter Messages},\n year = {2016}\n}\n"
                    ],
                    "title": "Dealing with long texts"
                }
            },
            "content": "Interaction-focused systems are a crucial aspect of conversational information retrieval (CIR), which enables users to interact with the system to seek information through multi-turn conversations in natural language [3]. These models build a query-document term pairwise interaction matrix that captures the exact matching and semantic matching information between the query-document pairs [4]. After this, the interaction matrix is fed into deep neural networks. The DSSM model is a typical representative case of interaction-focused models that predicts click probability given a query string and a document title [4]. Furthermore, a new development in this area is the Entity-Duet Neural Ranking Model (EDRM) that incorporates entities in interaction-based neural ranking models. EDRM learns the distributed representations of entities using their semantics from knowledge graphs, and matches documents to queries with both bag-of-words and bag-of-entities while using interaction-based neural models. This integration of knowledge graph semantics into neural-IR enhances entity-oriented search with neural networks in interaction-focused systems [5]. \n\nHowever, with the fast pace of modern research, various approaches have been applied to different IR problems, and discerning key insights can be overwhelming. Furthermore, the integration of deep learning into all aspects of modern IR systems provides a multitude of different approaches to many different information retrieval problems [2]. Therefore, constant research and development are necessary for improving and understanding interaction-focused systems better, given their pivotal role in modern IR systems [2].",
            "references": [
                "@Article{Mitra2018AnIT,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {Foundations and Trends in Information Retrieval},\n journal = {Found. Trends Inf. Retr.},\n pages = {1-126},\n title = {An Introduction to Neural Information Retrieval},\n volume = {13},\n year = {2018}\n}\n",
                "@Book{Kenter2017NeuralNF,\n author = {Tom Kenter and Alexey Borisov and Christophe Van Gysel and Mostafa Dehghani and M. de Rijke and Bhaskar Mitra},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {Neural Networks for Information Retrieval},\n year = {2017}\n}\n",
                "@Book{Gao2022NeuralAT,\n author = {Jianfeng Gao and Chenyan Xiong and Paul Bennett and Nick Craswell},\n booktitle = {The Information Retrieval Series},\n journal = {Neural Approaches to Conversational Information Retrieval},\n title = {Neural Approaches to Conversational Information Retrieval},\n year = {2022}\n}\n",
                "@Article{Yang2018ResponseRW,\n author = {Liu Yang and Minghui Qiu and Chen Qu and J. Guo and Yongfeng Zhang and W. Bruce Croft and Jun Huang and Haiqing Chen},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval},\n title = {Response Ranking with Deep Matching Networks and External Knowledge in Information-seeking Conversation Systems},\n year = {2018}\n}\n",
                "@Article{Liu2018EntityDuetNR,\n author = {Zhenghao Liu and Chenyan Xiong and Maosong Sun and Zhiyuan Liu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2395-2405},\n title = {Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval},\n year = {2018}\n}\n"
            ]
        }
    }
}