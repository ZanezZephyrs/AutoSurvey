{
    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "subsections": {
                "LTR Features": {
                    "content": "Learning to rank (LTR) models play a crucial role in information retrieval systems, aiming to accurately rank relevant documents based on user queries. Paper 1 introduces a neural ranking model that utilizes multiple document fields to improve retrieval performance. The study evaluates the model, named NRM-F (Neural Ranking Model with Field Concatenation), which eliminates the need for handcrafted feature engineering typically used in ad-hoc retrieval [1]. The results demonstrate that NRM-F outperforms all baseline models, indicating that learning a multiple-field document representation is superior to scoring based on individual field representations [1].\n\nPaper 2 discusses the application of neural models for information retrieval tasks, focusing on document ranking. The study highlights the significance of neural models in generating good representations for queries and documents, and estimating relevance. Various machine learning models, including neural networks, have been employed for the learning to rank task, with RankNet being a popular choice [2]. The paper emphasizes the importance of good representation generation and relevance estimation steps in the document ranking process [2].\n\nFurthermore, Paper 3 explores the use of different sources of information in multi-label text classification (MLTC) systems. In MLTC, using various sources, such as labeled instances, textual labels of classes, and taxonomy relations of classes, can greatly impact the system's performance. The study adopts a learning to rank approach, where features reflecting the similarity of classes and documents are extracted from different sources, and a ranking problem is formulated to select labels for documents. The incorporation of score propagation, utilizing co-occurrence patterns of classes in labeled documents, further improves classification accuracy [3]. The research highlights the effectiveness of an LTR approach integrating all features and the relative importance of different sources of evidence in MLTC [3].\n\nMoreover, Paper 4 proposes a latent semantic model, called Convolutional Latent Semantic Model (CLSM), for information retrieval. This model incorporates a convolutional-pooling structure to learn low-dimensional semantic vector representations for search queries and Web documents. The CLSM captures contextual features at the word n-gram level and aggregates them to form a sentence-level feature vector. By applying a non-linear transformation, high-level semantic information is extracted to generate continuous vector representations for text strings. The CLSM demonstrates its effectiveness in capturing salient semantic information and outperforms previous state-of-the-art semantic models in Web document ranking tasks [4].\n\nLastly, Paper 5 addresses the representation of features in text classification, specifically focusing on improving the widely used TF-IDF representation. The paper introduces Bi-Normal Separation (BNS) feature scaling as a replacement for IDF in real-valued feature vectors. Empirical evaluation demonstrates that SVM text classification models using BNS scaling achieve substantially better accuracy and F-measure compared to IDF scaling. The study shows that BNS scaling performs better than various other feature representations, including binary features without scaling, and eliminates the need for feature selection [5].\n\nIn summary, these papers highlight the significance of feature representation in LTR models and text classification tasks. Utilizing multiple document fields, employing neural models, incorporating various sources of information, and introducing novel scaling techniques can lead to improved retrieval and classification performance. These advancements contribute to the development of more accurate and effective information retrieval systems.",
                    "references": [
                        "@Book{Zamani2017NeuralRM,\n author = {Hamed Zamani and Bhaskar Mitra and Xia Song and Nick Craswell and Saurabh Tiwary},\n booktitle = {Web Search and Data Mining},\n journal = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},\n title = {Neural Ranking Models with Multiple Document Fields},\n year = {2017}\n}\n",
                        "@Article{Mitra2017NeuralMF,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Models for Information Retrieval},\n volume = {abs/1705.01509},\n year = {2017}\n}\n",
                        "@Article{Azarbonyad2020LearningTR,\n author = {H. Azarbonyad and Mostafa Dehghani and M. Marx and J. Kamps},\n booktitle = {Natural Language Engineering},\n journal = {Natural Language Engineering},\n pages = {89 - 111},\n title = {Learning to rank for multi-label text classification: Combining different sources of information},\n volume = {27},\n year = {2020}\n}\n",
                        "@Book{Shen2014ALS,\n author = {Yelong Shen and Xiaodong He and Jianfeng Gao and L. Deng and Gr\u00e9goire Mesnil},\n booktitle = {International Conference on Information and Knowledge Management},\n journal = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},\n title = {A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval},\n year = {2014}\n}\n",
                        "@Article{Forman2008BNSFS,\n author = {George Forman},\n booktitle = {International Conference on Information and Knowledge Management},\n pages = {263-270},\n title = {BNS feature scaling: an improved representation over tf-idf for svm text classification},\n year = {2008}\n}\n"
                    ],
                    "title": "LTR Features"
                }
            },
            "content": "Text representations for ranking play a crucial role in information retrieval (IR) systems. Traditional approaches to IR employ hand-crafted features for ranking search results based on a query [1]. However, with the advent of neural models, there has been a shift towards learning representations of language from raw text to bridge the gap between query and document vocabulary [1]. These neural ranking models, also known as neural IR models, utilize shallow or deep neural networks to rank search results in response to a query [1]. Unlike traditional IR models, neural IR models require large-scale training data and learn representations of text that capture the semantic relationships between words, improving the overall performance of the ranking system [1]. \n\nIn addition to neural IR models, other deep learning approaches have also been explored for text representation and ranking tasks in the context of IR. For example, a study proposed a document ranking model composed of two separate deep neural networks. One network uses a local representation of text, while the other learns a distributed representation before matching the query and document [2]. This model outperformed traditional IR baselines and other recently proposed models based on shallow and deep neural networks, indicating the benefits of using deep learning for ranking tasks [2].\n\nMoreover, adaptive methods have been developed to optimize ranking functions in IR systems. These methods aim to automatically improve the performance of ranked text retrieval systems by optimizing the free parameters in the retrieval system based on a criterion function [3]. By minimizing the criterion function, the retrieval system learns parameter settings that enable better ranking of relevant documents before irrelevant ones, resulting in more effective document ordering for users [3]. These adaptive methods have been applied to various text retrieval problems, including dimensionality reduction, similarity estimation, and neural network combination, and have shown positive results in terms of performance improvement and adaptability to different retrieval environments [3].\n\nOverall, the use of neural IR models, deep learning approaches for text representation and ranking, and adaptive methods for optimizing ranking functions have demonstrated their potential and effectiveness in improving the performance of information retrieval systems. These advancements in text representations for ranking contribute to the overall enhancement of search results and user experience in various IR applications.\n\nReferences:\n[1] Mitra, B., & Craswell, N. (2017). Neural Models for Information Retrieval. arXiv preprint arXiv:1705.01509.\n[2] Unknown author. (n.d.). Learning to Match using Local and Distributed Representations of Text for Web Search. Retrieved from https://www.microsoft.com/en-us/research/publication/learning-to-match-using-local-and-distributed-representations-of-text-for-web-search/.\n[3] Dean, J. (1990). Optimizing ranking functions: a connectionist approach to adaptive information retrieval. Carnegie Mellon University.",
            "references": [
                "@Article{Mitra2017NeuralMF,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Models for Information Retrieval},\n volume = {abs/1705.01509},\n year = {2017}\n}\n",
                "@Book{Mitra2016LearningTM,\n author = {Bhaskar Mitra and Fernando Diaz and Nick Craswell},\n booktitle = {The Web Conference},\n journal = {Proceedings of the 26th International Conference on World Wide Web},\n title = {Learning to Match using Local and Distributed Representations of Text for Web Search},\n year = {2016}\n}\n",
                "@Inproceedings{Bartell1994OptimizingRF,\n author = {B. Bartell},\n title = {Optimizing ranking functions: a connectionist approach to adaptive information retrieval},\n year = {1994}\n}\n",
                "@Book{Shen2018KnowledgeawareAN,\n author = {Ying Shen and Yang Deng and Min Yang and Yaliang Li and Nan Du and Wei Fan and Kai Lei},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval},\n title = {Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs},\n year = {2018}\n}\n",
                "@Article{Wu2016LearningOM,\n author = {Fei Wu and Xinyan Lu and Jun Song and Shuicheng Yan and Zhongfei Zhang and Y. Rui and Yueting Zhuang},\n booktitle = {IEEE Transactions on Image Processing},\n journal = {IEEE Transactions on Image Processing},\n pages = {630-642},\n title = {Learning of Multimodal Representations With Random Walks on the Click Graph},\n volume = {25},\n year = {2016}\n}\n"
            ]
        },
        "Interaction-focused Systems": {
            "subsections": {
                "Convolutional Neural Networks": {
                    "content": "Convolutional Neural Networks (CNNs) have been widely utilized in various domains, including image classification, natural language processing, and cybersecurity. In the paper titled \"Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\" [1], the authors focus on generalizing CNNs from regular grids to high-dimensional irregular domains, such as social networks and brain connectomes represented by graphs. They propose a formulation of CNNs in the context of spectral graph theory, which enables the design of fast localized convolutional filters on graphs. The experiments conducted on datasets like MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.\n\nAnother application of CNNs can be found in the field of cybersecurity, as discussed in the paper \"Detecting Cyber Attacks in Industrial Control Systems Using Convolutional Neural Networks\" [2]. The authors introduce a model based on 1D CNNs that successfully detects cyber attacks in industrial control systems (ICS). By applying 1D convolutions to each feature across the time dimension, the proposed model can capture mutual dependencies between features. To address the limitations of 1D CNNs, the authors augment the feature space by adding features' first derivatives, enhancing the model's ability to learn beyond the sequence used as a sample. The proposed approach exhibits promising performance and efficiency in detecting attacks in ICS.\n\nFurthermore, CNNs have also been applied in opinion summarizations in microblogging systems. In the paper titled \"Mining opinion summarizations using convolutional neural networks in Chinese microblogging systems\" [3], the authors propose a CNN-based method for opinion summarization in Chinese microblogging systems. The method leverages CNNs to automatically mine useful features and perform sentiment analysis. By making good use of the obtained sentiment features, the semantic relationships among features are computed according to a hybrid ranking function. This approach enables the generation of concise and informative summaries of opinions in microblogging systems.\n\nIn the domain of recommender systems, CNNs have played a crucial role in mining information from graph-structured data. The paper titled \"Graph Convolutional Neural Networks for Web-Scale Recommender Systems\" [4] discusses the application of Graph Convolutional Networks (GCNs) in recommendation tasks. GCNs learn how to iteratively aggregate feature information from local graph neighborhoods using neural networks. By leveraging both content information and graph structure, GCNs have achieved remarkable performance improvements on numerous recommender system benchmarks.\n\nLastly, CNNs have also been generalized for equivariance to Lie groups in arbitrary continuous data, as investigated in the paper titled \"Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data\" [5]. The authors explore how CNNs can perceive images by considering objects and their relations. This work highlights the importance of understanding the perception and processing of images by CNNs.\n\nTo sum up, Convolutional Neural Networks (CNNs) have found applications in various fields, such as graph analysis, cybersecurity, opinion summarization, recommender systems, and image processing. These applications showcase the capability and versatility of CNNs in solving complex problems and extracting meaningful information from different types of data.",
                    "references": [
                        "@Article{Defferrard2016ConvolutionalNN,\n author = {M. Defferrard and X. Bresson and P. Vandergheynst},\n booktitle = {NIPS},\n pages = {3837-3845},\n title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},\n year = {2016}\n}\n",
                        "@Article{Kravchik2018DetectingCA,\n author = {Moshe Kravchik and A. Shabtai},\n booktitle = {CPS-SPC@CCS},\n journal = {Proceedings of the 2018 Workshop on Cyber-Physical Systems Security and PrivaCy},\n title = {Detecting Cyber Attacks in Industrial Control Systems Using Convolutional Neural Networks},\n year = {2018}\n}\n",
                        "@Article{Li2016MiningOS,\n author = {Qiudan Li and Zhipeng Jin and Can Wang and D. Zeng},\n booktitle = {Knowledge-Based Systems},\n journal = {Knowl. Based Syst.},\n pages = {289-300},\n title = {Mining opinion summarizations using convolutional neural networks in Chinese microblogging systems},\n volume = {107},\n year = {2016}\n}\n",
                        "@Article{Ying2018GraphCN,\n author = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and J. Leskovec},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},\n year = {2018}\n}\n",
                        "@Article{Finzi2020GeneralizingCN,\n author = {Marc Finzi and S. Stanton and Pavel Izmailov and A. Wilson},\n booktitle = {International Conference on Machine Learning},\n pages = {3165-3176},\n title = {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},\n year = {2020}\n}\n"
                    ],
                    "title": "Convolutional Neural Networks"
                },
                "Pre-trained Language Models": {
                    "content": "Pre-trained language models (PLMs) have been widely explored and shown promise in various applications. Paper 1 discusses the potential of PLMs as open-ended recommender systems [1]. PLMs are found to be capable of generalizing to open-ended domains and performing zero-shot learning [1]. The authors propose extending the framework of PLMs to multimodal settings [1]. \n\nIn Paper 2, the focus is on using pre-trained language models for task-oriented dialogue systems [2]. The authors demonstrate that recent progress in language modeling pre-training and transfer learning can help overcome the challenge of data scarcity in developing task-oriented dialogue systems [2]. They propose a model that operates solely on text input, bypassing explicit policy and language generation modules [2]. This approach shows promise in addressing the need for large amounts of task-specific data in dialogue systems [2].\n\nPaper 3 provides a survey on dense text retrieval based on pre-trained language models [3]. Pre-trained language models, such as BERT, have significantly improved the performance in a variety of natural language processing tasks [3]. These models are pre-trained over large-scale general text corpora with self-supervised loss functions and can be fine-tuned for downstream tasks [3]. The success of BERT has inspired further research in improved pre-training approaches, refinements in bidirectional representations, and model compression [3].\n\nIn Paper 4, the focus is on conversational question reformulation [4]. The authors propose the use of pre-trained language models to leverage language structures extracted from large corpora [4]. By fine-tuning the pre-trained weights of these models, they aim to improve the capabilities of question reformulation [4]. The experiments are conducted on the CANARD dataset and the TREC 2019 Conversational Assistant Track dataset [4].\n\nFinally, Paper 5 explores the use of pre-trained language models for document-level neural machine translation [5]. The authors initialize the parameters of encoders with pre-trained language models, which are trained on monolingual documents [5]. They also propose methods to manipulate the integration of context information to control the influence of large contexts [5]. The experiments show significant improvements in translation performance on three language pairs [5].\n\nIn summary, pre-trained language models have shown promising results in various domains, including recommender systems, dialogue systems, document-level translation, and dense text retrieval. These models leverage large-scale pre-training on general text corpora and can be fine-tuned for specific tasks. Further research is being conducted to explore their capabilities, refine their training approaches, and extend their applications to multimodal settings.",
                    "references": [
                        "@Article{Cui2022M6RecGP,\n author = {Zeyu Cui and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems},\n volume = {abs/2205.08084},\n year = {2022}\n}\n",
                        "@Article{Budzianowski2019HelloIG,\n author = {Pawe\u0142 Budzianowski and Ivan Vulic},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Hello, It\u2019s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems},\n volume = {abs/1907.05774},\n year = {2019}\n}\n",
                        "@Article{Zhao2022DenseTR,\n author = {Wayne Xin Zhao and Jing Liu and Ruiyang Ren and Ji-rong Wen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dense Text Retrieval based on Pretrained Language Models: A Survey},\n volume = {abs/2211.14876},\n year = {2022}\n}\n",
                        "@Article{Lin2020ConversationalQR,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Rodrigo Nogueira and Ming-Feng Tsai and Chuan-Ju Wang and Jimmy J. Lin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models},\n volume = {abs/2004.01909},\n year = {2020}\n}\n",
                        "@Article{Li2019PretrainedLM,\n author = {Liangyou Li and Xin Jiang and Qun Liu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Pretrained Language Models for Document-Level Neural Machine Translation},\n volume = {abs/1911.03110},\n year = {2019}\n}\n"
                    ],
                    "title": "Pre-trained Language Models"
                },
                "Dealing with long texts": {
                    "content": "Long texts pose various challenges in different domains of natural language processing. In the field of text generation, one approach to address the issue of generating coherent and informative long text is through adversarial training with leaked information. The LeakGAN model, as described in [1], addresses the non-informativeness and sparsity problems of reward signals in previous generative adversarial network (GAN) solutions. The model utilizes step-by-step guiding signals to guide the generator in better generating long texts. The experiments conducted in the study demonstrate that LeakGAN achieves significant performance improvements over previous solutions, as measured by BLEU scores and human ratings. The model shows particularly large performance gains in generating longer sentences. Future work includes applying LeakGAN to dialogue systems and image captioning, as well as enhancing the capacity of the discriminator to check the global consistency of whole sentences [1].\n\nIn other applications such as text-to-speech alignment, dealing with long audio tracks and acoustically inaccurate text transcripts requires more complex methods. [2] proposes the use of probabilistic kernels, which are similarity functions based on a confusion matrix computed from a large corpus of speech. The kernels aim to capture key information about the behavior of the phonetic decoder, improving the alignment between audio and text. The study shows that the proposed probabilistic kernels outperform baseline kernels and alternative approaches in experiments conducted on the Hub4-97 dataset [2]. These findings highlight the importance of exploring advanced techniques to tackle the challenges posed by long texts in different NLP tasks.\n\nOverall, addressing the challenges associated with long texts requires innovative approaches tailored to specific applications. The studies on long text generation and text-to-speech alignment presented in [1] and [2] provide insights into developing models and techniques that can effectively handle long texts. These findings contribute to the advancement of NLP research and highlight the importance of considering the specific characteristics and demands of long texts in various domains.",
                    "references": [
                        "@Article{Guo2017LongTG,\n author = {Jiaxian Guo and Sidi Lu and Han Cai and Weinan Zhang and Yong Yu and Jun Wang},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Long Text Generation via Adversarial Training with Leaked Information},\n volume = {abs/1709.08624},\n year = {2017}\n}\n",
                        "@Article{Bordel2016ProbabilisticKF,\n author = {Germ\u00e1n Bordel and M. Pe\u00f1agarikano and Luis Javier Rodriguez-Fuentes and Aitor \u00c1lvarez and A. Varona},\n booktitle = {IEEE Signal Processing Letters},\n journal = {IEEE Signal Processing Letters},\n pages = {126-129},\n title = {Probabilistic Kernels for Improved Text-to-Speech Alignment in Long Audio Tracks},\n volume = {23},\n year = {2016}\n}\n",
                        "@Article{Li2018DeepDE,\n author = {Na Li and Deyi Tuo and Dan Su and Zhifeng Li and Dong Yu},\n booktitle = {Interspeech},\n pages = {2262-2266},\n title = {Deep Discriminative Embeddings for Duration Robust Speaker Verification},\n year = {2018}\n}\n",
                        "@Article{Wu2021MasteringTE,\n author = {Shengqiong Wu and Hao Fei and Fei Li and Donghong Ji and Meishan Zhang and Yijiang Liu and Chong Teng},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {11513-11521},\n title = {Mastering the Explicit Opinion-Role Interaction: Syntax-Aided Neural Transition System for Unified Opinion Role Labeling},\n year = {2021}\n}\n",
                        "@Article{Ning2017MultiTaskDL,\n author = {Yishuang Ning and Jia Jia and Zhiyong Wu and Runnan Li and Yongsheng An and Yanfeng Wang and H. Meng},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {161-167},\n title = {Multi-Task Deep Learning for User Intention Understanding in Speech Interaction Systems},\n year = {2017}\n}\n"
                    ],
                    "title": "Dealing with long texts"
                }
            },
            "content": "Interaction-focused systems have gained significant attention in the field of information retrieval (IR). Neural models have played a crucial role in such systems, as demonstrated in several studies [1,2,3,4,5]. An Introduction to Neural Information Retrieval provides a comprehensive overview of neural methods for document ranking in response to a query, covering topics such as deep neural network architectures trained for ranking tasks [1]. Another review article, Neural Models for Information Retrieval, explores the application of shallow or deep neural networks in retrieval tasks, considering both text and multimodal search scenarios [2]. Furthermore, the paper Neural Approaches to Conversational Information Retrieval focuses on conversational IR systems with natural language interfaces, highlighting the recent advancements in deep learning and conversational AI that have enhanced human-centric interactions [3]. Additionally, Incorporating Query Term Independence Assumption for Efficient Retrieval and Ranking using Deep Neural Networks presents approaches that learn low-dimensional vector representations of queries and documents, enabling fast retrieval and ranking with interactions between query terms and document representations [4]. Finally, Entity-Duet Neural Ranking investigates the role of entities and semantics in neural IR, introducing a model that combines entity-oriented search with interaction-based neural models, leveraging the knowledge graph semantics to enhance entity-oriented search [5]. Together, these papers shed light on the significant contributions of neural approaches in the development of interaction-focused systems in the field of information retrieval.",
            "references": [
                "@Article{Mitra2018AnIT,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {Foundations and Trends in Information Retrieval},\n journal = {Found. Trends Inf. Retr.},\n pages = {1-126},\n title = {An Introduction to Neural Information Retrieval},\n volume = {13},\n year = {2018}\n}\n",
                "@Article{Mitra2017NeuralMF,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Models for Information Retrieval},\n volume = {abs/1705.01509},\n year = {2017}\n}\n",
                "@Book{Gao2022NeuralAT,\n author = {Jianfeng Gao and Chenyan Xiong and Paul Bennett and Nick Craswell},\n booktitle = {The Information Retrieval Series},\n journal = {Neural Approaches to Conversational Information Retrieval},\n title = {Neural Approaches to Conversational Information Retrieval},\n year = {2022}\n}\n",
                "@Article{Mitra2019IncorporatingQT,\n author = {Bhaskar Mitra and Corby Rosset and D. Hawking and Nick Craswell and Fernando Diaz and Emine Yilmaz},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Incorporating Query Term Independence Assumption for Efficient Retrieval and Ranking using Deep Neural Networks},\n volume = {abs/1907.03693},\n year = {2019}\n}\n",
                "@Article{Liu2018EntityDuetNR,\n author = {Zhenghao Liu and Chenyan Xiong and Maosong Sun and Zhiyuan Liu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2395-2405},\n title = {Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval},\n year = {2018}\n}\n"
            ]
        }
    }
}