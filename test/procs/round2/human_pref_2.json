{

    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "content": "According to the Probability Ranking Principle [Robertson 1977], under certain as- sumptions, for a given user’s query, documents in a collection should be ranked in order of the (decreasing) probability of relevance w.r.t. the query, to maximise the overall effectiveness of a retrieval system for the user. The task of ad-hoc ranking is, for each query, to compute an ordering of the documents equivalent or most similar to the optimal ordering based on the probability of relevance. It is common to limit the documents to be ordered to just the top k documents in the optimal ordering. Let D denote a collection of (text) documents, and Q denote a log of (text) queries. Queries and documents share the same vocabulary V of terms. A ranking function, also known as scoring function, s : Q×D → R computes a real-valued score for the documents in the collection D w.r.t. the queries in the log Q. Given a query q and a document d, we call the value s(q, d) relevance score of the document w.r.t. the query. For a given query, the scores of the documents in the collection can be used to induce an ordering of the documents, in reverse value of score. The closer this induced or- dering is to the optimal ordering, the more effective an IR system based on the scoring function is. Without loss of generality, the scoring function s(q, d) can be further decomposed as: s(q, d) = f (φ(q), ψ(d), η(q, d)), (1) where φ : Q → V1, ψ : D → V2, and η : Q×D → V3 are three representation func- tions, mapping queries, documents, and query-document pairs into the latent repre- sentation spaces V1, V2, and V3, respectively [Guo et al. 2020]. These functions build abstract mathematical representations of the text sequences of documents and queries amenable for computations. The elements of these vectors represent the features used to describe the corresponding objects, and the aggregation function f : V1 ×V2 ×V3 → R computes the relevance score of the document representation w.r.t. the query repre- sentation. The representation functions φ, ψ and η, and the aggregation function f can be de- signed by hand, leveraging some axioms or heuristics, or computed through machine learning algorithms. In the following, we will overview the representation functions adopted in classical IR (Section 1.1), in LTR scenarios (Section 1.2) and the recently proposed word embeddings (Section 1.3).",
            "subsections": {
                "LTR Features": {
                    "content": "With the advent of the Web, new sources of relevance information about the docu- ments have been made available. The importance of a Web page, e.g., PageRank, additional document statistics, e.g., term frequencies in the title or anchors text, and search engine interactions, e.g., clicks, can be exploited as relevance signals. Moreover, collaborative and social platforms such as Wikipedia, Twitter and Facebook represent new sources of relevance signals. These relevance signals have been exploited to build richer query and document representations in LTR. The relevance signals extracted from queries and/or documents are called features. There are various classes of these features [Bendersky et al. 2011, Macdonald et al. 2012], such as: • query-only features, i.e., components of φ(q): query features with the same value for each document, such as query type, query length, and query performance predictors; • query-independent features, i.e., components of ψ(d): document features with the same value for each query, such as importance score, URL length, and spam score; • query-dependent features, i.e., components of η(q, d): document features that de- pend on the query, such as different term weighting models on different fields. In LTR, the representation functions are hand-crafted: exploiting the relevance signals from heterogeneous information sources, the different components of query and doc- ument representations are computed with feature-specific algorithms. Hence, the rep- resentations φ(q), ψ(d), and η(q, d) are elements of vector spaces over R, but whose di- mensions depend on the number of hand-crafted query-only, query-independent, and query-dependent features, respectively. Moreover the different components of these vectors are heterogeneous, and do not carry any specific semantic meaning. Using these representations, in LTR the aggregation function f is machine-learned, for ex- ample using logistic regression [Gey 1994], gradient-boosted regression trees [Burges 2010] or neural networks [Burges et al. 2005]; see [Liu 2009] for a detailed survey."
                }
            }       
        },
        "Interaction-focused Systems" : {
            "content": "The interaction-focused systems used in neural IR model the word and n-gram re- lationships across a query and a document using deep neural networks. These sys- tems receive as input both a query q and a document d, and output a query-document representation η(q, d). Among others, two neural network architectures have been investigated to build a representation of these relationships: convolutional neural net- works and transformers. Convolutional neural networks represent one of the first approaches in building joint representations of queries and documents, as discussed in Section 2.1. Transformers represent the major turning point in neural IR, as their application to textual inputs gave birth to pre-trained language models, presented in Section 2.2. In neural IR, pre-trained language models are used to compute query- document representations, and the two main transformer models used for this task are BERT and T5 illustrated in Sections 2.3 and 2.4, respectively. Section 2.5 describes how pre-trained language models are fine-tuned to compute effective query-document rep- resentations, and Section 2.6 briefly discusses how pre-trained language models can deal with long documents.",
            "subsections": {
                "Convolutional Neural Networks": {
                    "content": "A convolutional neural network is a family of neural networks designed to capture local patterns in structured inputs, such as images and texts [LeCun and Bengio 1998]. The core component of a convolutional neural network is the convolution layer, used in conjunction with feed forward and pooling layers. A convolutional layer can be seen as a small linear filter, sliding over the input and looking for proximity patterns. Several neural models employ convolutional neural networks over the interactions between queries and documents to produce relevance scores. Typically, in these mod- els, the word embeddings of the query and document tokens are aggregated into an interaction matrix, on top of which convolutional neural networks are used to learn hierarchical proximity patterns such as unigrams, bigrams and so on. Then, the final top-level proximity patterns are fed into a feed forward neural network to produce the relevance score s(q, d) between the query q and the document d, as illustrated in Figure 2. The query q and the document d are tokenised into m and n tokens, respectively, and each token is mapped to a corresponding static word embedding. The interaction matrix η(q, d) ∈ Rm×n is composed of the cosine similarities between a query token embedding and a document token embedding. One of the first neural models leveraging the interaction matrix is the Deep Relevance Matching Model (DRMM) [Guo et al. 2016]). In DRMM, the cosine similarities of every query token w.r.t. the document tokens are converted into a discrete distribution using hard bucketing, i.e., into a query token histogram. Then the histogram of each query to- ken is provided as input to a feed forward neural network to compute the final query token-document relevance score. These relevance scores are then aggregated through an IDF-based weighted sum across the different query terms. Instead of using hard bucketing, the Kernel-based Neural Ranking Model (KNRM) [Xiong et al. 2017] pro- poses to use Gaussian kernels to smoothly distribute the contribution of each cosine similarity across different buckets before providing the histograms with soft bucketing to the feed forward neural networks. Both DRMM and KNRM exploit the interaction matrix, but they do not incorporate. any convolutional layer. In the Convolutional KNRM model (ConvKNRM) [Dai et al. 2018], the query and document embeddings are first independently processed through k convolutional neural networks, to build unigam, bigram, up to k-gram embeddings. These convolutions allow to build word embeddings taking into account multiple close words at the same time. Then, k2 cosine similarity matrices are built, between each combination of query and document n-gram embeddings, and these matrices are processed with KNRM. In the Position-Aware Convolutional Recurrent Relevant model (PACRR) [Hui et al. 2017], the interaction matrix is processed through several convolutional and pooling layers to take into account words proximity. Convolutional layers are used also in other similar neural models [Fan et al. 2018b, Hu et al. 2014, Hui et al. 2018, Pang et al. 2016, 2017]."
                },
                "Pre-trained Language Models": {
                    "content": "Static word embeddings map words with multiple senses into an average or most common-sense representation based on the training data used to compute the vectors. The vector of a word does not change with the other words used in a sentence around it. The transformer is a neural network designed to explicitly take into account the context of arbitrary long sequences of text, thanks to a special neural layer called self- attention, used in conjunction with feed forward and linear layers. The self-attention layer maps input sequences to output sequences of the same length. When comput- ing the i-th output element, the layer can access all the n input elements (bidirectional self-attention) or only the first i input elements (causal self-attention). A self-attention layer allows the network to take into account the relationships among different ele- ments in the same input. When the input elements are tokens of a given text, a self- attention layer computes token representations that take into account their context, i.e., the surrounding words. In doing so, the transformer computes contextualised word embeddings, where the representation of each input token is conditioned by the whole input text. Transformers have been successfully applied to different natual language processing tasks, such as machine translation, summarisation, question answering and so on. All these tasks are special instances of a more general task, i.e., transforming an input text sequence to some output text sequence. The sequence-to-sequence model has been designed to address this general task. The sequence-to-sequence neural network is composed of two parts: an encoder model, which receives an input sequence and builds a contextualised representation of each input element, and a decoder model, which uses these contextualised representations to generate a task-specific output sequence Both models are composed of several stacked transformers. The transformers in the encoder employ bidirectional self-attention layers on the input sequence or the output sequence of the previous transformer. The transformers in the decoder employ causal self-attention on the previous decoder transformer’s output and bidirectional cross- attention of the output of the final encoder transformer’s output. In neural IR, two specific instances of the sequence-to-sequence models have been studied: encoder-only models and encoder-decoder models. Encoder-only models re- ceive as input all the tokens of a given input sentence, and they compute an output contextualised word embedding for each token in the input sentence. Representatives of this family of models include BERT [Devlin et al. 2019], RoBERTa [Liu et al. 2019], and DistilBERT [Sanh et al. 2019]. Encoder-decoder models generate new output sen- tences depending on the given input sentence. The encoder model receives as input all the tokens of a given sequence and builds a contextualised representation, and the de- coder model sequentially accesses these embeddings to generate new output tokens, one token at a time. Representatives of this family of models include BART [Lewis et al. 2020] and T5 [Raffel et al. 2020]. Sequence-to-sequence models can be trained as language models, by projecting with a linear layer every output embedding to a given vocabulary and computing the tokens probabilities with a softmax operation. The softmax operation is a function σ : Rk → [0, 1]k that takes as input k > 1 real values z1, z2, . . . , zk and transforms each input zi as follows: σ(zi ) = ezi ∑kj=1 ezj (2) The softmax operation normalises the input values into a probability distribution. In the context of deep learning, the inputs of a softmax operation are usually called logits, and they represent the raw predictions generated by a multi-class classification model, turned into a probability distribution over the classes by the softmax operation. Depending on the training objective, a sequence-to-sequence model can be trained as a masked language model (MLM), as for BERT, or a casual language model (CLM), as for T5. MLM training focuses on learning to predict missing tokens in a sequence given the surrounding tokens; CLM training focuses on predicting the next token in an output sequence given the preceding tokens in the input sequence. In both cases, it is commonplace to train these models using massive text data to obtain pre-trained language models. In doing so, we allow the model to learn general-purpose knowledge about a language that can be adapted afterwards to a more specific downstream task. In this transfer learning approach, a pre-trained language model is used as initial model to fine-tune it on a domain-specific, smaller training dataset for the downstream target ask. In other words, fine-tuning is the procedure to update the parameters of a pre- trained language model for the domain data and target task. As illustrated in Figure 3, pre-training typically requires a huge general-purpose train- ing corpus, such as Wikipedia or Common Crawl web pages, expensive computa- tion resources and long training times, spanning several days or weeks. On the other side, fine-tuning requires a small domain-specific corpus focused on the downstream task, affordable computational resources and few hours or days of additional training. Special cases of fine-tuning are few-shot learning, where the domain-specific corpus is composed of a very limited number of training data, and zero-shot learning, where a pre-trained language model is used on a downstream task that it was not fine-tuned on. In neural IR, the interaction-focused systems that use pre-trained language models are called cross-encoder models, as they receive as input a pair (q, d) of query and document texts. Depending on the type of sequence-to-sequence model, different cross-encoders are fine-tuned in different ways, but, in general, they aim at computing a relevance score s(q, d) to rank documents w.r.t. a given query. In the following, we illustrate the most common cross-encoders leveraging both encoder-only models (Sec. 2.3) and encoder-decoder models (Sec. 2.4)."
                },
                "Dealing with long texts":{
                    "content": "BERT and T5 models have an input size limited to 512 tokens, including the special ones. When dealing with long documents, that cannot be fed completely into a transformer model, we need to split into smaller texts in a procedure referred to as passaging. Dai and Callan [2019a] propose to split a long document into overlapping shorter passages, to be processed independently together with the same query by a cross-encoder. During training, if a long document is relevant, all its passages are relevant, and vice-versa. Then, the relevance scores for each composing passage are aggregated back to a single score for the long document. In this scenario, the common aggregation functions are FirstP, i.e., the document score is the score of the first pas- sage, MaxP, i.e., the document score is the highest score across all passages, and SumP, i.e., the document score is the sum of the scores of its passages. Alternatively, Li et al. [2020] generate the [CLS] output embedding for each passage to compute a query- passage representation for each passage. Then, the different passage embeddings are aggregated together to compute a final relevance score for the whole document using feed forward neural networks, convolutional neural networks or simple transformer architectures"
                }
            }  
        }

    }
}
