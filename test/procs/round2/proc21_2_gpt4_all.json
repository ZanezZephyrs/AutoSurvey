{
    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "subsections": {
                "LTR Features": {
                    "content": "Learning to Rank (LTR) models are pivotal tools in text classification and information retrieval tasks[1,2,3,4,5,7,8]. Some studies[1,3] have focused on using concatenation of multiple fields as single input text, and have found that this form of Neural Ranking Model (NRM-F) results in better retrieval performance. Paper [2] highlights the role of neural LTR models with different features like query-dependent (e.g., BM25) or query-level features (e.g., number of words in query). RankNet, a popular algorithm for training neural L2R models, has been used by industry experts and is known for generating excellent query and document representations[2]. Other research [3] offered insight into the benefits of multi-label text classification (MLTC), which integrates different features from various information sources including labeled training data, textual labels of classes, and taxonomy relations of classes. Implications of these studies reveal that a lean-and mean system using only four features can perform at 96% of a larger LTR model proposed in the research[3].\n\nIn the realm of machine learning models for text, feature extraction methods have been widely varied to improve classification tasks [4,5]. Convolution models like Convolutional Latent Semantic Model (CLSM)[4] utilise a convolutional-pooling structure to capture rich contextual structures in a query or a document. Meanwhile, a study by Forman (2008)[5] introduced Bi-Normal Separation (BNS), a feature scaling method proven superior to TF-IDF. BNS showed better accuracy and F-measure for Support Vector Machine (SVM) without the need for feature selection. Recent research studies in multimodal LTR models [6,7] are adopting more innovative feature extraction methods including instance loss [6], joint embeddings [7] and kernel entity salience modeling [8]. These novel representations reveal the potential of multimodal cues for efficient retrieval and enhanced text understanding [6,7,8].",
                    "references": [
                        "@Book{Zamani2017NeuralRM,\n author = {Hamed Zamani and Bhaskar Mitra and Xia Song and Nick Craswell and Saurabh Tiwary},\n booktitle = {Web Search and Data Mining},\n journal = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},\n title = {Neural Ranking Models with Multiple Document Fields},\n year = {2017}\n}\n",
                        "@Article{Mitra2017NeuralMF,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neural Models for Information Retrieval},\n volume = {abs/1705.01509},\n year = {2017}\n}\n",
                        "@Article{Azarbonyad2020LearningTR,\n author = {H. Azarbonyad and Mostafa Dehghani and M. Marx and J. Kamps},\n booktitle = {Natural Language Engineering},\n journal = {Natural Language Engineering},\n pages = {89 - 111},\n title = {Learning to rank for multi-label text classification: Combining different sources of information},\n volume = {27},\n year = {2020}\n}\n",
                        "@Book{Shen2014ALS,\n author = {Yelong Shen and Xiaodong He and Jianfeng Gao and L. Deng and Gr\u00e9goire Mesnil},\n booktitle = {International Conference on Information and Knowledge Management},\n journal = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},\n title = {A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval},\n year = {2014}\n}\n",
                        "@Article{Forman2008BNSFS,\n author = {George Forman},\n booktitle = {International Conference on Information and Knowledge Management},\n pages = {263-270},\n title = {BNS feature scaling: an improved representation over tf-idf for svm text classification},\n year = {2008}\n}\n",
                        "@Article{Zheng2017DualpathCI,\n author = {Zhedong Zheng and Liang Zheng and Michael Garrett and Yi Yang and Mingliang Xu and Yi-Dong Shen},\n booktitle = {ACM Trans. Multim. Comput. Commun. Appl.},\n journal = {ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},\n pages = {1 - 23},\n title = {Dual-path Convolutional Image-Text Embeddings with Instance Loss},\n volume = {16},\n year = {2017}\n}\n",
                        "@Book{Mithun2018LearningJE,\n author = {Niluthpol Chowdhury Mithun and Juncheng Billy Li and Florian Metze and A. Roy-Chowdhury},\n booktitle = {International Conference on Multimedia Retrieval},\n journal = {Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval},\n title = {Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval},\n year = {2018}\n}\n",
                        "@Book{Xiong2018TowardsBT,\n author = {Chenyan Xiong and Zhengzhong Liu and Jamie Callan and Tie-Yan Liu},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval},\n title = {Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling},\n year = {2018}\n}\n"
                    ],
                    "title": "LTR Features"
                }
            },
            "content": "With the advent of advanced neural ranking models being utilised for information retrieval (IR), various approaches and models have emerged for superior results [2][8]. Substantial progress has been made in creating models that can interact and encode contextual information efficiently for ranking purposes [1][3]. One of the earliest successful models is the Deep Structured Semantic Model (DSSM), which directly addresses the ad-hoc retrieval task [2]. Subsequently, the Contextualized Embeddings for Document Ranking (CEDR) model that incorporates contextualized language models into existing neural ranking architectures has also seen considerable improvements in performance on multiple datasets [3]. Beyond traditional models, strategies like Kernel Pooling in the end-to-end Neural Ad-hoc Ranking model (K-NRM) have also shown strong performance, with the capacity to encode a similarity metric suited for matching query words to document words [4]. \n\nTransformer-based models have signaled a shift from exact matches to soft matches in text representation, creating contextualized term and document representations pre-computed for efficient retrieval during query-time [1][5]. However, the computational costs and need for high-quality relevance information presents inherent limitations in these models' performance [3][5]. Meanwhile, weakly supervised neural ranking models trained using results from unsupervised ranking models, like BM25, offer alternative learning strategies in the absence of human-annotated labels [6]. When combined with suitable objective functions and input representation learning, they exhibit notable effectiveness over various collections, including large-scale web data [6]. Furthermore, newer models like BERT have exhibited state-of-art performance when combined with ranking losses for document ranking tasks [8]. It's worth noting that even though convolutional neural networks (CNNs) have also been utilized successfully for ranking, especially in classifying relations, the lack of a mechanism to consider the entire sequence of a text in a single model remains a challenge [7]. In sum, advancements in text representations for ranking show potential to tackle information retrieval tasks efficiently but also underscore the need for context-sensitive, reliable, and computable models.",
            "references": [
                "@Article{Lin2020PretrainedTF,\n author = {Jimmy J. Lin and Rodrigo Nogueira and Andrew Yates},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},\n title = {Pretrained Transformers for Text Ranking: BERT and Beyond},\n year = {2020}\n}\n",
                "@Article{Guo2019ADL,\n author = {J. Guo and Yixing Fan and Liang Pang and Liu Yang and Qingyao Ai and Hamed Zamani and Chen Wu and W. Bruce Croft and Xueqi Cheng},\n booktitle = {Information Processing & Management},\n journal = {ArXiv},\n title = {A Deep Look into Neural Ranking Models for Information Retrieval},\n volume = {abs/1903.06902},\n year = {2019}\n}\n",
                "@Article{MacAvaney2019CEDRCE,\n author = {Sean MacAvaney and Andrew Yates and Arman Cohan and Nazli Goharian},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {CEDR: Contextualized Embeddings for Document Ranking},\n year = {2019}\n}\n",
                "@Article{Xiong2017EndtoEndNA,\n author = {Chenyan Xiong and Zhuyun Dai and Jamie Callan and Zhiyuan Liu and Russell Power},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {End-to-End Neural Ad-hoc Ranking with Kernel Pooling},\n year = {2017}\n}\n",
                "@Article{MacAvaney2020EfficientDR,\n author = {Sean MacAvaney and F. M. Nardini and R. Perego and N. Tonellotto and Nazli Goharian and O. Frieder},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {Efficient Document Re-Ranking for Transformers by Precomputing Term Representations},\n year = {2020}\n}\n",
                "@Book{Dehghani2017NeuralRM,\n author = {Mostafa Dehghani and Hamed Zamani and Aliaksei Severyn and J. Kamps and W. Bruce Croft},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {Neural Ranking Models with Weak Supervision},\n year = {2017}\n}\n",
                "@Article{Santos2015ClassifyingRB,\n author = {C. D. Santos and Bing Xiang and Bowen Zhou},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {626-634},\n title = {Classifying Relations by Ranking with Convolutional Neural Networks},\n year = {2015}\n}\n",
                "@Article{Han2020LearningtoRankWB,\n author = {Shuguang Han and Xuanhui Wang and Michael Bendersky and Marc Najork},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning-to-Rank with BERT in TF-Ranking},\n volume = {abs/2004.08476},\n year = {2020}\n}\n"
            ]
        },
        "Interaction-focused Systems": {
            "subsections": {
                "Convolutional Neural Networks": {
                    "content": "Convolutional Neural Networks (CNNs), a deep learning model used in visual and image classification tasks, experienced an era of dormancy prior to a resurgence around the mid-2000s [6]. Increased computational power, larger datasets, and improved algorithms contributed to the resurgence and saw CNNs brought to the forefront of a neural network renaissance [6]. Various applications of CNNs have since been documented ranging from cyberattack detection to sentiment analysis in social media, recommendation systems, image classification, and computer-aided detection [1,2,3,4,7]. For instance, in the context of cyberattack detection, CNN model showed promise in detecting ICS cyberattacks [1]. Similarly, the method proposed for sentiment analysis in Chinese microblogging context suggests that CNN-based models can effectively mine useful features and perform sentiment analysis [2].\n\nCNNs have shown promise beyond mere image classification. Mining of opinion summarizations in Chinese microblogging systems has been facilitated with CNNs by conducting sentiment analysis and establishing semantic relationships among features [2]. In a different context, a CNN-based emotion recognition method was proposed, resulting in improved accuracy rates in the recognition of emotions based on electroencephalogram (EEG) [5]. Another study pointed to the potential of Graph Convolutional Networks (GCNs) to aggregate feature information from local graph neighborhoods to enhance the performance of recommender systems [3]. Methodologies utilizing CNNs have been highlighted for their superiority in runtime and performance, such as the effort to detect cyber attacks in industrial control systems [1]. In the mobile computing context, CNNs have been recognized for the low latency they offer, contributing to an optimal user experience, notably in the area of object detection and video scene recognition [8]. Despite the remarkable advancements, an earlier review provided a sobering insight, noting the limitations and challenges that remain in the adoption of CNNs [6]. These involve the computational resources they require, their reliance on large datasets, and the need for further algorithmic developments.",
                    "references": [
                        "@Article{Kravchik2018DetectingCA,\n author = {Moshe Kravchik and A. Shabtai},\n booktitle = {CPS-SPC@CCS},\n journal = {Proceedings of the 2018 Workshop on Cyber-Physical Systems Security and PrivaCy},\n title = {Detecting Cyber Attacks in Industrial Control Systems Using Convolutional Neural Networks},\n year = {2018}\n}\n",
                        "@Article{Li2016MiningOS,\n author = {Qiudan Li and Zhipeng Jin and Can Wang and D. Zeng},\n booktitle = {Knowledge-Based Systems},\n journal = {Knowl. Based Syst.},\n pages = {289-300},\n title = {Mining opinion summarizations using convolutional neural networks in Chinese microblogging systems},\n volume = {107},\n year = {2016}\n}\n",
                        "@Article{Ying2018GraphCN,\n author = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and J. Leskovec},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},\n year = {2018}\n}\n",
                        "@Article{Finzi2020GeneralizingCN,\n author = {Marc Finzi and S. Stanton and Pavel Izmailov and A. Wilson},\n booktitle = {International Conference on Machine Learning},\n pages = {3165-3176},\n title = {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},\n year = {2020}\n}\n",
                        "@Article{Zheng2021ThreedimensionalFM,\n author = {Xiangwei Zheng and Xiaomei Yu and Yongqiang Yin and Tiantian Li and Xiaoyan Yan},\n booktitle = {International Journal of Intelligent Systems},\n journal = {International Journal of Intelligent Systems},\n pages = {6312 - 6336},\n title = {Three\u2010dimensional feature maps and convolutional neural network\u2010based\u00a0emotion recognition},\n volume = {36},\n year = {2021}\n}\n",
                        "@Article{Rawat2017DeepCN,\n author = {Waseem Rawat and Zenghui Wang},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {2352-2449},\n title = {Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review},\n volume = {29},\n year = {2017}\n}\n",
                        "@Article{Roth2015ImprovingCD,\n author = {H. Roth and Le Lu and Jiamin Liu and Jianhua Yao and Ari Seff and Kevin M. Cherry and Lauren Kim and R. Summers},\n booktitle = {IEEE Transactions on Medical Imaging},\n journal = {IEEE Transactions on Medical Imaging},\n pages = {1170-1181},\n title = {Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation},\n volume = {35},\n year = {2015}\n}\n",
                        "@Book{Hanhirova2018LatencyAT,\n author = {Jussi Hanhirova and Teemu K\u00e4m\u00e4r\u00e4inen and S. Sepp\u00e4l\u00e4 and M. Siekkinen and V. Hirvisalo and Antti Yl\u00e4-J\u00e4\u00e4ski},\n booktitle = {ACM SIGMM Conference on Multimedia Systems},\n journal = {Proceedings of the 9th ACM Multimedia Systems Conference},\n title = {Latency and throughput characterization of convolutional neural networks for mobile computer vision},\n year = {2018}\n}\n"
                    ],
                    "title": "Convolutional Neural Networks"
                },
                "Pre-trained Language Models": {
                    "content": "Pre-trained Language Models (PLMs) have shown a massive impact in the world of natural language processing. By learning general text representations, PLMs such as GPT and BERT, can efficiently and accurately adapt to various downstream tasks [6,3]. These models facilitate tasks such as contextual image retrieval, multimodal dialogue [1], recommendation systems [2,3], and dialogue state tracking [4]. A crucial aspect of PLMs is their ability to perform zero-shot learning, by generalizing to open-ended domains [2] and working on tasks where no or limited training data are available [3]. Despite their fair share of challenges, the potential opportunities presented by PLMs present an exciting frontier in this field.\n\nGroundbreaking studies further present PLMs as efficient tools for tackling task-oriented dialogues by providing task-aware history encoding. This functionality is inherently showcased in practical applications such as food ordering, technical support, restaurant/hotel/travel booking [4,5]. Furthermore, PLMs like LaMDA are being adapted to dialogue applications and demonstrated significant improvements in terms of safety and factual grounding [8]. Interestingly, the role of PLMs extends into areas such as web-scale retrieval [7] and higher-dimensional spaces as they are capable of deep semantic matching [7]. The latter allows for online retrieval after document embeddings are pre-computed and cached offline [7]. These advancements have certainly transformed the capabilities of PLMs, making them an integral part of the current language intelligence era [6].",
                    "references": [
                        "@Article{Koh2023GroundingLM,\n author = {Jing Yu Koh and R. Salakhutdinov and Daniel Fried},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Grounding Language Models to Images for Multimodal Generation},\n volume = {abs/2301.13823},\n year = {2023}\n}\n",
                        "@Article{Cui2022M6RecGP,\n author = {Zeyu Cui and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems},\n volume = {abs/2205.08084},\n year = {2022}\n}\n",
                        "@Inproceedings{Zhang2021LanguageMA,\n author = {Yuhui Zhang and Hao Ding and Zeren Shui and Yifei Ma and James Y. Zou and Anoop Deoras and Hao Wang},\n title = {Language Models as Recommender Systems: Evaluations and Limitations},\n year = {2021}\n}\n",
                        "@Article{Lee2021DialogueST,\n author = {Chia-Hsuan Lee and Hao Cheng and Mari Ostendorf},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Dialogue State Tracking with a Language Model using Schema-Driven Prompting},\n volume = {abs/2109.07506},\n year = {2021}\n}\n",
                        "@Article{Budzianowski2019HelloIG,\n author = {Pawe\u0142 Budzianowski and Ivan Vulic},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Hello, It\u2019s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems},\n volume = {abs/1907.05774},\n year = {2019}\n}\n",
                        "@Article{Zhao2022DenseTR,\n author = {Wayne Xin Zhao and Jing Liu and Ruiyang Ren and Ji-rong Wen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dense Text Retrieval based on Pretrained Language Models: A Survey},\n volume = {abs/2211.14876},\n year = {2022}\n}\n",
                        "@Book{Liu2021PretrainedLM,\n author = {Yiding Liu and Guan Huang and Jiaxiang Liu and Weixue Lu and Suqi Cheng and Yukun Li and Daiting Shi and Shuaiqiang Wang and Zhicong Cheng and Dawei Yin},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining},\n title = {Pre-trained Language Model for Web-scale Retrieval in Baidu Search},\n year = {2021}\n}\n",
                        "@Article{Thoppilan2022LaMDALM,\n author = {Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam M. Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and Yaguang Li and Hongrae Lee and H. Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and M. Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Yanqi Zhou and Chung-Ching Chang and I. Krivokon and W. Rusch and Marc Pickett and K. Meier-Hellstern and M. Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and J. S\u00f8raker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark D\u00edaz and B. Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravindran Rajakumar and Alena Butryna and Matthew Lamm and V. Kuzmina and Joseph Fenton and Aaron Cohen and R. Bernstein and R. Kurzweil and Blaise Aguera-Arcas and Claire Cui and M. Croak and E. Chi and Quoc Le},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LaMDA: Language Models for Dialog Applications},\n volume = {abs/2201.08239},\n year = {2022}\n}\n"
                    ],
                    "title": "Pre-trained Language Models"
                },
                "Dealing with long texts": {
                    "content": "The challenge of managing long texts has been approached using various methodologies in the realm of machine learning and natural language processing [1,2,3,4,5,8]. An interesting venture into adversarial training, LeakGAN, has demonstrated its prowess in long-text generation [1]. This model tackles the non-informativeness and sparsity issues observed in previous generative adversarial network solutions by using step-by-step guiding signals. Notably, LeakGAN offers significant performance improvement over its predecessors when longer sentences are generated [1].\n\nOn the other hand, the utility of deep learning in handling long text has also been explored. For instance, Long Short-Term Memory Networks (LSTM) have shown effectiveness in machine reading where it simulates human comprehension by incrementally extracting the meaning of utterances on a word-by-word basis [4]. Additionally, LSTM has been utilized for sentiment analysis on large text data, offering substantial benefits thanks to its ability to handle vast quantities of information [5]. The LSTM technique in conjunction with other techniques, including a bidirectional measure, has also been applied to perform Named Entity Recognition (NER) in lengthy Twitter messages, exhibiting promising performance [8]. Meanwhile, deep discriminative embeddings have been presented as a solution for improving the robustness of deep convolution neural networks (CNNs) when longer utterances are in play [3]. These approaches shed light on the potential methods that can be used to process long texts in the artificial intelligence domain.",
                    "references": [
                        "@Article{Guo2017LongTG,\n author = {Jiaxian Guo and Sidi Lu and Han Cai and Weinan Zhang and Yong Yu and Jun Wang},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Long Text Generation via Adversarial Training with Leaked Information},\n volume = {abs/1709.08624},\n year = {2017}\n}\n",
                        "@Article{Bordel2016ProbabilisticKF,\n author = {Germ\u00e1n Bordel and M. Pe\u00f1agarikano and Luis Javier Rodriguez-Fuentes and Aitor \u00c1lvarez and A. Varona},\n booktitle = {IEEE Signal Processing Letters},\n journal = {IEEE Signal Processing Letters},\n pages = {126-129},\n title = {Probabilistic Kernels for Improved Text-to-Speech Alignment in Long Audio Tracks},\n volume = {23},\n year = {2016}\n}\n",
                        "@Article{Li2018DeepDE,\n author = {Na Li and Deyi Tuo and Dan Su and Zhifeng Li and Dong Yu},\n booktitle = {Interspeech},\n pages = {2262-2266},\n title = {Deep Discriminative Embeddings for Duration Robust Speaker Verification},\n year = {2018}\n}\n",
                        "@Article{Cheng2016LongSM,\n author = {Jianpeng Cheng and Li Dong and Mirella Lapata},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Long Short-Term Memory-Networks for Machine Reading},\n volume = {abs/1601.06733},\n year = {2016}\n}\n",
                        "@Article{Murthy2020TextBS,\n author = {G. N. Murthy and Shanmukha Rao Allu and Bhargavi Andhavarapu and Mounika Belusonti Mounika Bagadi},\n journal = {International Journal of Engineering Research and},\n title = {Text based Sentiment Analysis using LSTM},\n volume = {9},\n year = {2020}\n}\n",
                        "@Article{Fan2015CourseMIRROREL,\n author = {Xiangmin Fan and Wencan Luo and Muhsin Menekse and D. Litman and Jingtao Wang},\n booktitle = {CHI Extended Abstracts},\n journal = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},\n title = {CourseMIRROR: Enhancing Large Classroom Instructor-Student Interactions via Mobile Interfaces and Natural Language Processing},\n year = {2015}\n}\n",
                        "@Article{Geng2022RecommendationAL,\n author = {Shijie Geng and Shuchang Liu and Zuohui Fu and Yingqiang Ge and Yongfeng Zhang},\n booktitle = {ACM Conference on Recommender Systems},\n journal = {Proceedings of the 16th ACM Conference on Recommender Systems},\n title = {Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)},\n year = {2022}\n}\n",
                        "@Article{Limsopatham2016BidirectionalLF,\n author = {Nut Limsopatham and Nigel Collier},\n booktitle = {NUT@COLING},\n pages = {145-152},\n title = {Bidirectional LSTM for Named Entity Recognition in Twitter Messages},\n year = {2016}\n}\n"
                    ],
                    "title": "Dealing with long texts"
                }
            },
            "content": "In the realm of information retrieval, one dynamic area to focus on is the development of systems that prioritize interaction. Neural models have been employed across many variations of information retrieval, including ad-hoc retrieval, recommender systems, multi-media search, and particularly in conversational systems that generate responses to natural language queries [1,2,4]. A conversational information retrieval system presents users with an information retrieval system that incorporates a conversational interface. The progress in deep learning has led to significant improvements in natural language processing and conversational AI. This, combined with a growing demand for more human-centric interactions in information retrieval, has sparked a burgeoning interest in the creation of modern conversational information retrieval systems [2].\n\nModels such as the Neural Relational Inference (NRI) have been proposed to address these interaction-oriented needs. The NRI model is an unsupervised learning system that infers resolutions while simultaneously learning the dynamics purely from observational data. The latent code in the NRI model represents the underlying interaction graph, with the reconstruction based on graph neural networks [6]. Another alternative approach, inspired by cognitive science, utilizes the classic approach of production systems. These systems consist of a set of rule templates that are applied by binding placeholder variables in the rules to specific entities. This disentangling of knowledge achieves robust future-state prediction in rich visual environments and effectively factors entity-specific and rule-based information [8]. These system models aptly demonstrate a move towards developing more interaction-focused systems.",
            "references": [
                "@Article{Mitra2018AnIT,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {Foundations and Trends in Information Retrieval},\n journal = {Found. Trends Inf. Retr.},\n pages = {1-126},\n title = {An Introduction to Neural Information Retrieval},\n volume = {13},\n year = {2018}\n}\n",
                "@Book{Gao2022NeuralAT,\n author = {Jianfeng Gao and Chenyan Xiong and Paul Bennett and Nick Craswell},\n booktitle = {The Information Retrieval Series},\n journal = {Neural Approaches to Conversational Information Retrieval},\n title = {Neural Approaches to Conversational Information Retrieval},\n year = {2022}\n}\n",
                "@Article{Thakur2021BEIRAH,\n author = {Nandan Thakur and Nils Reimers and Andreas Ruckl'e and Abhishek Srivastava and Iryna Gurevych},\n booktitle = {NeurIPS Datasets and Benchmarks},\n journal = {ArXiv},\n title = {BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models},\n volume = {abs/2104.08663},\n year = {2021}\n}\n",
                "@Inproceedings{Manning2008IntroductionTI,\n author = {Christopher D. Manning and P. Raghavan and Hinrich Sch\u00fctze},\n pages = {I-XXI, 1-482},\n title = {Introduction to information retrieval},\n year = {2008}\n}\n",
                "@Article{Cooper1964FactRA,\n author = {W. S. Cooper},\n booktitle = {JACM},\n journal = {J. ACM},\n pages = {117-137},\n title = {Fact Retrieval and Deductive Question-Answering Information Retrieval Systems},\n volume = {11},\n year = {1964}\n}\n",
                "@Article{Fetaya2018NeuralRI,\n author = {T. Fetaya and E. Wang and K.-C. Welling and M. Zemel and Thomas Kipf and Ethan Fetaya and Kuan-Chieh Jackson Wang and M. Welling and R. Zemel},\n booktitle = {International Conference on Machine Learning},\n pages = {2693-2702},\n title = {Neural Relational Inference for Interacting Systems},\n year = {2018}\n}\n",
                "@Article{Mandl2008RecentDI,\n author = {Thomas Mandl},\n booktitle = {Informatica},\n journal = {Informatica (Slovenia)},\n pages = {27-38},\n title = {Recent Developments in the Evaluation of Information Retrieval Systems: Moving Towards Diversity and Practical Relevance},\n volume = {32},\n year = {2008}\n}\n",
                "@Article{Goyal2021NeuralPS,\n author = {Anirudh Goyal and Aniket Didolkar and Nan Rosemary Ke and C. Blundell and Philippe Beaudoin and N. Heess and M. Mozer and Y. Bengio},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Neural Production Systems},\n volume = {abs/2103.01937},\n year = {2021}\n}\n"
            ]
        }
    }
}