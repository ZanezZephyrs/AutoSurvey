{
    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "subsections": {
                "BOW Encodings": {
                    "content": "The traditional approach to text representation called \"Bag-of-Words\" (BoW) has been widely used in text retrieval systems [2]. However, there are some limitations of BoW representations, such as their inability to capture the semantic similarity between words. To overcome these limitations, [2] proposed a new approach called SparTerm, which learns term-based sparse representation in the full vocabulary space. This method uses an importance predictor and a gating controller to generate a term-based sparse representation based on the semantic relationship of the input text with each term in the vocabulary. The proposed method still retains the interpretability and efficiency of BoW while also improving the ranking performance of term-based representations. \n\nSimilarly, in Pretrained Transformers for Text Ranking: BERT and Beyond [1], the authors discuss the challenges of designing effective representations for ranking and the use of transformer-based encoders, such as BERT, to model queries and texts from the corpus. The authors highlight the need for an efficient balance between what can be efficiently computed at scale and what is necessary to capture relevance in terms of the dense representations. Overall, these two papers highlight the limitations of BoW representations, and the need for more powerful and effective methods for representing text for ranking and retrieval systems.",
                    "references": [
                        "@Article{Lin2020PretrainedTF,\n author = {Jimmy J. Lin and Rodrigo Nogueira and Andrew Yates},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},\n title = {Pretrained Transformers for Text Ranking: BERT and Beyond},\n year = {2020}\n}\n",
                        "@Article{Bai2020SparTermLT,\n author = {Yang Bai and Xiaoguang Li and Gang Wang and Chaoliang Zhang and Lifeng Shang and Jun Xu and Zhaowei Wang and Fangshan Wang and Qun Liu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval},\n volume = {abs/2010.00768},\n year = {2020}\n}\n",
                        "@Article{Wang2015IncorporatingLK,\n author = {Yan Wang and Zhiyuan Liu and Maosong Sun},\n booktitle = {PLoS ONE},\n journal = {PLoS ONE},\n title = {Incorporating Linguistic Knowledge for Learning Distributed Word Representations},\n volume = {10},\n year = {2015}\n}\n",
                        "@Article{Paul2019RankingAS,\n author = {Debjit Paul and A. Frank},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {3671-3681},\n title = {Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs},\n year = {2019}\n}\n",
                        "@Article{Lin2021InBatchNF,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Jimmy J. Lin},\n booktitle = {Workshop on Representation Learning for NLP},\n pages = {163-173},\n title = {In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval},\n year = {2021}\n}\n",
                        "@Article{Zhou2020RecommendingTF,\n author = {Yichao Zhou and Shaunak Mishra and Manisha Verma and Narayan L. Bhamidipati and Wei Wang},\n booktitle = {The Web Conference},\n journal = {Proceedings of The Web Conference 2020},\n title = {Recommending Themes for Ad Creative Design via Visual-Linguistic Representations},\n year = {2020}\n}\n",
                        "@Article{Xin2021ZeroShotDR,\n author = {Ji Xin and Chenyan Xiong and A. Srinivasan and Ankita Sharma and Damien Jose and Paul Bennett},\n booktitle = {Findings},\n pages = {4008-4020},\n title = {Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations},\n year = {2021}\n}\n",
                        "@Article{Ji2019BERTbasedRF,\n author = {Zongcheng Ji and Qiang Wei and H. Xu},\n booktitle = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},\n journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},\n pages = {\n          269-277\n        },\n title = {BERT-based Ranking for Biomedical Entity Normalization},\n volume = {2020},\n year = {2019}\n}\n"
                    ],
                    "title": "BOW Encodings"
                },
                "Word Embeddings": {
                    "content": "Word embeddings represent words or phrases as vectors in a high-dimensional space, allowing for numerical manipulations and machine learning models to operate on them. In the first paper, the authors propose a novel method of representing sets of word vectors to develop a similarity metric for information retrieval. This method outperformed existing methods for document indexing and scoring. The second and fourth papers focus on using BERT-based models to develop deep learning-based methods to solve biomedical problems, such as entity normalization and linking named entities to concepts in an ontology. The success of these models can be attributed to their ability to pre-train contextualized word representations that account for word usage in context. Finally, the fifth paper compares classic and contextualized word embeddings for text classification tasks. The authors find that CNN outperforms BiLSTM in most situations and that BERT outperforms ELMo, especially in long document datasets. Overall, the paper highlights the importance of selecting appropriate word embedding methods for specific machine learning tasks.",
                    "references": [
                        "@Article{Roy2016RepresentingDA,\n author = {Dwaipayan Roy and Debasis Ganguly and Mandar Mitra and G. Jones},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Representing Documents and Queries as Sets of Word Embedded Vectors for Information Retrieval},\n volume = {abs/1606.07869},\n year = {2016}\n}\n",
                        "@Article{Ji2019BERTbasedRF,\n author = {Zongcheng Ji and Qiang Wei and H. Xu},\n booktitle = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},\n journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},\n pages = {\n          269-277\n        },\n title = {BERT-based Ranking for Biomedical Entity Normalization},\n volume = {2020},\n year = {2019}\n}\n",
                        "@Article{Carvallo2020AutomaticDS,\n author = {Andr\u00e9s Carvallo and Denis Parra and Hans Lobel and \u00c1. Soto},\n booktitle = {Scientometrics},\n journal = {Scientometrics},\n pages = {3047-3084},\n title = {Automatic document screening of medical literature using word and text embeddings in an active learning setting},\n volume = {125},\n year = {2020}\n}\n",
                        "@Article{Karadeniz2019LinkingET,\n author = {Ilknur Karadeniz and Arzucan \u00d6zg\u00fcr},\n booktitle = {BMC Bioinformatics},\n journal = {BMC Bioinformatics},\n title = {Linking entities through an ontology using word embeddings and syntactic re-ranking},\n volume = {20},\n year = {2019}\n}\n",
                        "@Article{Wang2020ACS,\n author = {Congcong Wang and P. Nulty and David Lillis},\n booktitle = {International Conference on Natural Language Processing and Information Retrieval},\n journal = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},\n title = {A Comparative Study on Word Embeddings in Deep Learning for Text Classification},\n year = {2020}\n}\n"
                    ],
                    "title": "Word Embeddings"
                }
            },
            "content": "Several papers have explored neural networks to create ranking models for information retrieval [1,2]. The Deep Structured Semantic Model (DSSM) is one of the first successful ranking models introduced in 2013 [1]. This survey article also highlights the progress made with deep learning in developing better text retrieval systems, where neural networks can learn query and document representations automatically [2]. Dense text retrieval is an emerging area in information retrieval, where the objective is to retrieve relevant information based on semantic matching, rather than keyword matching. Dense text retrieval aims to map some dense vector representations chosen by some pre-trained neural network to the dense vector representation of some query, where the target is to maximize the inner product between both vectors. Pre-trained language models such as BERT, XLNET, or RoBERTa offer pre-trained neural networks that can be adapted to build dense text retrieval strategies [4]. \n\nMoreover, some papers have explored ways to improve the quality of text representations to obtain better ranking results. For instance, Bhaskar Mitra and Nick Craswell proposed a novel text matching method that uses local and distributed representations of text for web search [5]. They introduced the Duet model, which combines both lexical features and semantic representations to create text representations before ranking, and they showed that this technique improves the quality of retrieval systems. One issue faced by many of these models is the lack of robustness, to adversarial examples in particular. Several recent studies have shown that pre-trained language models are often prone to adversarial attacks [8], which are malicious inputs designed to manipulate the ranking models output. Therefore, future research will have to address this issue to make robust ranking models that can be safely deployed in practice.",
            "references": [
                "@Article{Guo2019ADL,\n author = {J. Guo and Yixing Fan and Liang Pang and Liu Yang and Qingyao Ai and Hamed Zamani and Chen Wu and W. Bruce Croft and Xueqi Cheng},\n booktitle = {Information Processing & Management},\n journal = {ArXiv},\n title = {A Deep Look into Neural Ranking Models for Information Retrieval},\n volume = {abs/1903.06902},\n year = {2019}\n}\n",
                "@Article{Zhao2022DenseTR,\n author = {Wayne Xin Zhao and Jing Liu and Ruiyang Ren and Ji-rong Wen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dense Text Retrieval based on Pretrained Language Models: A Survey},\n volume = {abs/2211.14876},\n year = {2022}\n}\n",
                "@Article{Lin2021APC,\n author = {Jimmy J. Lin},\n booktitle = {SIGIR Forum},\n journal = {ACM SIGIR Forum},\n pages = {1 - 29},\n title = {A proposed conceptual framework for a representational approach to information retrieval},\n volume = {55},\n year = {2021}\n}\n",
                "@Book{Ma2022PretrainAD,\n author = {Xinyu Ma and J. Guo and Ruqing Zhang and Yixing Fan and Xueqi Cheng},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction},\n year = {2022}\n}\n",
                "@Article{Mitra2019IncorporatingQT,\n author = {Bhaskar Mitra and Corby Rosset and D. Hawking and Nick Craswell and Fernando Diaz and Emine Yilmaz},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Incorporating Query Term Independence Assumption for Efficient Retrieval and Ranking using Deep Neural Networks},\n volume = {abs/1907.03693},\n year = {2019}\n}\n",
                "@Book{V\u00f6lske2021TowardsAE,\n author = {Michael V\u00f6lske and Alexander Bondarenko and Maik Fr\u00f6be and Matthias Hagen and Benno Stein and Jaspreet Singh and Avishek Anand},\n booktitle = {International Conference on the Theory of Information Retrieval},\n journal = {Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval},\n title = {Towards Axiomatic Explanations for Neural Ranking Models},\n year = {2021}\n}\n",
                "@Article{Liu2018EntityDuetNR,\n author = {Zhenghao Liu and Chenyan Xiong and Maosong Sun and Zhiyuan Liu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2395-2405},\n title = {Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval},\n year = {2018}\n}\n",
                "@Article{Wu2021AreNR,\n author = {Chen Wu and Ruqing Zhang and Jiafeng Guo and Yixing Fan and Xueqi Cheng},\n booktitle = {ACM Trans. Inf. Syst.},\n journal = {ACM Transactions on Information Systems},\n pages = {1 - 36},\n title = {Are Neural Ranking Models Robust?},\n volume = {41},\n year = {2021}\n}\n"
            ]
        }
    }
}