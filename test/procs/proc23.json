{
    "Lecture Notes on Neural Information Retrieval": {
        "Text Representations for Ranking": {
            "subsections": {
                "BOW Encodings": {
                    "content": "The Bag-of-Words (BoW) method of encoding has been the mainstay for the first-stage text retrieval in various applications [1,7]. Unlike dense retrieval (DR) methods which encode texts into a dense embedding space [4], BoW emphasizes the exact matching of keywords [1]. Hence, it maintains an edge when it comes to efficiency, interpretability, and exact term matching [1]. However, to augment the BoW method in terms of its semantic-level matching, there have been attempts to transfer the deep learning from pre-trained language models (PLM) to Term-Based Sparse representations [1]. SparTerm is a novel framework that aims to enhance the BoW using PLM [1]. This model predicts the importance of each term in the vocabulary and uses a gating controller to control the term activation. SparTerm, therefore, bridges the gap between term-weighting and expansion in the same framework [1].\n\nDespite the efficiency of BoW, incorporating linguistic knowledge in learning distributed word representations has been proposed [8]. Rather than purely relying on unsupervised learning from large-scale data, this method encodes both prior knowledge from knowledge bases and statistical knowledge from large-scale text corpora. Clearly, an integrated approach to the BoW inclusion of semantic understanding, pre-trained models, and linguistic knowledge holds the potential of revolutionizing the first stage text retrieval process [1,8]. Recent developments in information retrieval also pointed to integrating dense and sparse retrieval methods [7]. This conceptual framework, defined in terms of encoders and comparison functions, alludes to the promising future of information retrieval processes. However, striking a balance among efficiency, exact term matching, and semantic-level understanding is the challenge that awaits these integrated models [1,7,8].",
                    "references": [
                        "@Article{Bai2020SparTermLT,\n author = {Yang Bai and Xiaoguang Li and Gang Wang and Chaoliang Zhang and Lifeng Shang and Jun Xu and Zhaowei Wang and Fangshan Wang and Qun Liu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval},\n volume = {abs/2010.00768},\n year = {2020}\n}\n",
                        "@Article{Lin2020PretrainedTF,\n author = {Jimmy J. Lin and Rodrigo Nogueira and Andrew Yates},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},\n title = {Pretrained Transformers for Text Ranking: BERT and Beyond},\n year = {2020}\n}\n",
                        "@Article{Lin2021InBatchNF,\n author = {Sheng-Chieh Lin and Jheng-Hong Yang and Jimmy J. Lin},\n booktitle = {Workshop on Representation Learning for NLP},\n pages = {163-173},\n title = {In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval},\n year = {2021}\n}\n",
                        "@Article{Xin2021ZeroShotDR,\n author = {Ji Xin and Chenyan Xiong and A. Srinivasan and Ankita Sharma and Damien Jose and Paul Bennett},\n booktitle = {Findings},\n pages = {4008-4020},\n title = {Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations},\n year = {2021}\n}\n",
                        "@Article{Paul2019RankingAS,\n author = {Debjit Paul and A. Frank},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {3671-3681},\n title = {Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs},\n year = {2019}\n}\n",
                        "@Article{Ji2019BERTbasedRF,\n author = {Zongcheng Ji and Qiang Wei and H. Xu},\n booktitle = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},\n journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},\n pages = {\n          269-277\n        },\n title = {BERT-based Ranking for Biomedical Entity Normalization},\n volume = {2020},\n year = {2019}\n}\n",
                        "@Article{Lin2021APC,\n author = {Jimmy J. Lin},\n booktitle = {SIGIR Forum},\n journal = {ACM SIGIR Forum},\n pages = {1 - 29},\n title = {A proposed conceptual framework for a representational approach to information retrieval},\n volume = {55},\n year = {2021}\n}\n",
                        "@Article{Wang2015IncorporatingLK,\n author = {Yan Wang and Zhiyuan Liu and Maosong Sun},\n booktitle = {PLoS ONE},\n journal = {PLoS ONE},\n title = {Incorporating Linguistic Knowledge for Learning Distributed Word Representations},\n volume = {10},\n year = {2015}\n}\n"
                    ],
                    "title": "BOW Encodings"
                },
                "Word Embeddings": {
                    "content": "Word embeddings are an essential component of deep learning models providing a vector representation of words [3]. They significantly contribute to a variety of natural language processing tasks including biomedical entity normalization, information retrieval, and document screening [1,2,4]. For instance, the use of BERT-based word embeddings have shown considerable progress in biomedical tasks [1]. The application of BERT, BioBERT, and ClinicalBERT successfully addressed term variation problems, highlighting the effectiveness of these word embeddings for entity normalization [1].\n\nVarious methods of integrating word embeddings with existing systems show noteworthy advancements. For instance, a study represented a document and a query as sets of word vectors and used a standard similarity measure between these sets for document ranking [2]. This approach improved the mean average precision by up to 5.77% compared to standard text-based language model similarity. In another instance, an unsupervised approach was applied to link named entities to concepts in an ontology using word embeddings. A syntactic parser was used to increase the weight of the most informative word in the named entity mentions, indicating the flexibility of word embeddings in addressing a range of tasks [5]. Ultimately, the use of word and text embeddings consistently outperforms traditional TF-IDF representations and improves the efficiency of document screening tasks [4]. Thus, word embeddings provide a rich and powerful resource not only for deep learning models but also for a variety of natural-language-processing-driven tasks in general.",
                    "references": [
                        "@Article{Ji2019BERTbasedRF,\n author = {Zongcheng Ji and Qiang Wei and H. Xu},\n booktitle = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},\n journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},\n pages = {\n          269-277\n        },\n title = {BERT-based Ranking for Biomedical Entity Normalization},\n volume = {2020},\n year = {2019}\n}\n",
                        "@Article{Roy2016RepresentingDA,\n author = {Dwaipayan Roy and Debasis Ganguly and Mandar Mitra and G. Jones},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Representing Documents and Queries as Sets of Word Embedded Vectors for Information Retrieval},\n volume = {abs/1606.07869},\n year = {2016}\n}\n",
                        "@Article{Wang2020ACS,\n author = {Congcong Wang and P. Nulty and David Lillis},\n booktitle = {International Conference on Natural Language Processing and Information Retrieval},\n journal = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},\n title = {A Comparative Study on Word Embeddings in Deep Learning for Text Classification},\n year = {2020}\n}\n",
                        "@Article{Carvallo2020AutomaticDS,\n author = {Andr\u00e9s Carvallo and Denis Parra and Hans Lobel and \u00c1. Soto},\n booktitle = {Scientometrics},\n journal = {Scientometrics},\n pages = {3047-3084},\n title = {Automatic document screening of medical literature using word and text embeddings in an active learning setting},\n volume = {125},\n year = {2020}\n}\n",
                        "@Article{Karadeniz2019LinkingET,\n author = {Ilknur Karadeniz and Arzucan \u00d6zg\u00fcr},\n booktitle = {BMC Bioinformatics},\n journal = {BMC Bioinformatics},\n title = {Linking entities through an ontology using word embeddings and syntactic re-ranking},\n volume = {20},\n year = {2019}\n}\n"
                    ],
                    "title": "Word Embeddings"
                }
            },
            "content": "The use of deep learning models, particularly Neural Networks, has significantly enhanced the field of Information Retrievenal (IR) and has been employed in different scenarios such as recommender systems, multi-media search, and even conversational systems to generate answers for natural language queries [1]. Recent advancements in the field provide a number of neural retrieval techniques, such as supervised learning to rank models that use deep neural network structures and are trained end-to-end for ranking tasks [1]. A classic application of neural networks within IR systems is deep convolutional neural networks (CNNs), effective multidimensional signal processing models known for their ability to automatically train their functional parameters and generalize responses based on representative data [5].\n\nSpecifically, in the task of document ranking, a significant shift in approach has been shown, transitioning from sparse to dense representations of data [7]. This move towards dense retrieval, characterized by models such as DPR and ANCE [7], stands in stark contrast to traditional models that relied on sparse representations of textual data. Dense representations, combined with learned (supervised) representations, are offering a more sophisticated framework to understand information retrieval [7]. These models aim to capture the semantics of the text more accurately by using high dimensional vector spaces, which helps significantly in improving the reliability of the ranking achieved by IR systems [7].\n",
            "references": [
                "@Article{Mitra2018AnIT,\n author = {Bhaskar Mitra and Nick Craswell},\n booktitle = {Foundations and Trends in Information Retrieval},\n journal = {Found. Trends Inf. Retr.},\n pages = {1-126},\n title = {An Introduction to Neural Information Retrieval},\n volume = {13},\n year = {2018}\n}\n",
                "@Article{Kuo2017TheCA,\n author = {C.-C. Jay Kuo},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {81-89},\n title = {The CNN as a Guided Multilayer RECOS Transform [Lecture Notes]},\n volume = {34},\n year = {2017}\n}\n",
                "@Article{Chatzivasileiadis2018LectureNO,\n author = {Spyros Chatzivasileiadis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Lecture Notes on Optimal Power Flow (OPF)},\n volume = {abs/1811.00943},\n year = {2018}\n}\n",
                "@Article{Lidar2019LectureNO,\n author = {Daniel A. Lidar},\n journal = {arXiv: Quantum Physics},\n title = {Lecture Notes on the Theory of Open Quantum Systems},\n year = {2019}\n}\n",
                "@Article{Gonzalez2018DeepCN,\n author = {Rafael C. Gonzalez},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {79-87},\n title = {Deep Convolutional Neural Networks [Lecture Notes]},\n volume = {35},\n year = {2018}\n}\n",
                "@Article{Borot2017LectureNO,\n author = {G. Borot},\n journal = {arXiv: Mathematical Physics},\n title = {Lecture notes on topological recursion and geometry},\n year = {2017}\n}\n",
                "@Article{Lin2021AFB,\n author = {Jimmy J. Lin and Xueguang Ma},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Few Brief Notes on DeepImpact, COIL, and a Conceptual Framework for Information Retrieval Techniques},\n volume = {abs/2106.14807},\n year = {2021}\n}\n",
                "@Article{Baraniuk2007CompressiveS,\n author = {Richard Baraniuk},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {118-121},\n title = {Compressive Sensing [Lecture Notes]},\n volume = {24},\n year = {2007}\n}\n"
            ]
        }
    }
}