from AutoSurvey.llm_inference.openai_inference import OpenAIInference
from AutoSurvey.searchers.semantic_scholar import SemanticScholarSearcher
from AutoSurvey.filters.field_in_list_filter import FieldInListFilter
from AutoSurvey.filters.field_numeric_filter import FieldNumericFilter
from AutoSurvey.pdf_generation.pdf_generator import PDFGenerator
from AutoSurvey.llm_inference.query_augmentor import QueryAugmentor
from AutoSurvey.ranker import MonoT5
from AutoSurvey.pdf_extraction.pdf_extractor import get_pdf_windows
from tqdm.notebook import tqdm
import argparse
import json
import logging
import os
import tempfile

paper_template="""- Paper {I}
Title:{TITLE}
Content: {CONTENT}
"""


template_messages=[
    {"role": "system", "content": "You Are an expert in scientific literature review, your job is, given a series of papers and theirs summaries, write a paragraph with a given title citing relevant information in the traditional format (e.g [1,2,3] [1]) from the provided papers, you must write as much as you can, and must write at least 2 paragraphs"},
    {"role": "user", "content": "Paper 1\nTitle:Evaluating GPT-3.5 and GPT-4 Models on Brazilian - University Admission Exams\nSummary: he present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino Médio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on experiments are available at https://github.com/piresramon/gpt-4-enem \n\n- Paper 2\nTitle:BLUEX: a benchmark based on Brazilian Leading Universities Entrance eXams\nSummary: o enable future comparisons, we evaluated our dataset using several language models, ranging from 6B to 66B parameters, as well as the OpenAI’s GPT-4 and GPT-3.5-Turbo models. Our experiments focused on a simple scenario where the model had to answer a question in a 1-shot setting, based on a random example from an exam of the same university but from a different year. We excluded all questions containing images since the language models we used can only process text. This resulted in a total of 638 questions being used, which corresponds to approximately 60% of the dataset. Table 3 summarizes our experimental findings, including the mean score achieved by exam-taking students, as well as the mean cutoff score of the most competitive major, which is medicine in both universities.1 The BLUEX column shows the accuracy of the whole subset used in the evaluation, while the UNICAMP and USP columns account for only the questions from the respective universities. The MR and BK columns account only for questions that include those categories. Among the language models tested in the 7B-parameter range, Sabi ́a [17], a model further pre- trained in Portuguese, consistently outperformed all other models, coming close to matching the average human score. Among the open-source models in the 60B-parameter range, LLaMA 65B [25] significantly outperformed OPT 66B [30] and achieved similar performance to GPT-3.5-Turbo. Sabi ́a 65B achieved better performance than GPT-3.5-Turbo but still lagged behind GPT-4 by ten points. GPT-4 was by far the best model in our evaluations but did not achieve an average score high enough to pass in medicine, the most competitive major. It is worth noting that the average and cutoff scores provided in Table 3 are computed taking into account the whole exam, including questions with images, while the scores obtained by the language models utilize only the subset of questions with no images. \n\n- Paper 3\nTitle:Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential - Benefits of ChatGPT in Promoting Teaching and Learning\nSummary: Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators as this advancement in AI seems to revolutionize existing educational praxis. This review article synthesizes recent extant literature to offer some potential benefits of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students’ learning.\n\nParagraph Subject: The capacity and impact of ChatGPT in brazillian education"},
    {"role": "system", "content": "Since it's introduction, ChatGPT proved that it was capable of handling numerous tasks, [1,2] showed that GPT4 in particular is quite skilled at solving some of the most challeging high school level standarized tests in Brazil, namely ENEM [1] ,Unicamp and USP [2]. Since GPT4 is able to answer such high level tests, it is reasonable to ask the impact it could have in education. Numerous discussions exist regarding if the use of IA such as gpt3.5 and gpt4 is beneficial or harmful to students and teeachers, in particular [3] lists some benefits such as personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning, but also lists some limitations such as generating wrong information, biases in data training which may augment existing biases, privacy issues etc."}
]

survey_data="/content/AutoSurvey/data/dataset/survey_4_partial.json"
out_path="/content/proc21.json"
reranker=True
# reranker_model="castorini/monot5-3b-msmarco-10k"
reranker_model="castorini/monot5-large-msmarco"
number_of_docs_to_gpt=8

#create debug logger that writes to file
debug_logger = logging.getLogger('debug')
debug_logger.setLevel(logging.DEBUG)
fh = logging.FileHandler('debug.log')
fh.setLevel(logging.DEBUG)
debug_logger.addHandler(fh)


searcher=SemanticScholarSearcher()

agent=OpenAIInference(api_key="", engine="gpt-4")
agent_augment=OpenAIInference(api_key="", engine="gpt-4")


query_augmentor=QueryAugmentor(agent=agent_augment)

def search_one_query(query, filters=None, rerank=False, model=None):
  params={
      "query": query,
      "limit": 50, # max 100
      "fields": "title,authors,citationCount,year,tldr,url,abstract,influentialCitationCount,citationStyles"
  }

  # compose as many filters as you like, they are applied in sequence

  if filters is None:
    filters=[
        FieldNumericFilter("citationCount", lower_bound=15), # at least 10 citations
        #FieldNumericFilter("authors", lower_bound=2), # at least 2  authors
        #FieldNumericFilter("year", lower_bound=2020, upper_bound=2023), # between 2020 and 2023
    ]

  docs = searcher.search(params, filters=filters)
  if len(docs) == 0:
    debug_logger.debug(f"query: {query} returned 0 results with filters {filters}")
    return []
  augmented_docs = []
  tot=0
  for doc in docs:
    if doc["citationStyles"] and doc["citationStyles"].get("bibtex"):
      ref=doc["citationStyles"]["bibtex"]
    else:
       ref=""
    if rerank:
      pdf = searcher.download_pdf(doc["paperId"])
      if pdf:
        tot+=1
        temp_file = tempfile.NamedTemporaryFile()
        temp_file_path = temp_file.name
        temp_file.write(pdf)
        windows = get_pdf_windows(temp_file_path)
        temp_file.close()

        augmented_docs.extend([{"title": doc["title"], "content": window, "reference": ref} for window in windows])
        # debug_logger.debug(f"extracted {len(windows)} windows from pdf {doc['title']}\n")
        continue
    if doc["abstract"]:
      augmented_docs.append({"title": doc["title"], "content": doc["abstract"], "reference": ref})
    elif doc["tldr"] and doc["tldr"].get("text"):
      augmented_docs.append({"title": doc["title"], "content": doc["tldr"]["text"], "reference": ref})
    else:
      continue
  print("number of documents with pdf extracted", tot)

  debug_logger.debug(f"query: {query} returned {len(docs)} results with filters {filters}")

  return augmented_docs

def query_to_documents(query, survey_title=None, filters=None, augment_query=True, rerank=False, model=None):

  query_for_search=query
  if survey_title:
    query_for_search=survey_title + " " + query
  if augment_query:
    queries= query_augmentor.augment_queries(query_for_search)
    queries.append(query_for_search)
    queries=queries[-3:]
    print("augmentation resulted in the following queries", queries)
    debug_logger.debug(f"augmentation resulted in the following queries {queries}")

    results=[]
    for query in queries:
      results.extend(search_one_query(query, filters=filters, rerank=rerank, model=model))



  else:
    results=search_one_query(query, filters=filters, rerank=rerank, model=model)

  if rerank:
    query_for_rerank=query
    if survey_title:
      query_for_rerank=survey_title + ", " + query
    debug_logger.debug(f"reranking documents with query {query_for_rerank}")
    scores = model.rescore(query_for_rerank, [doc["content"] for doc in results])
    assert len(scores) == len(results), "The number of scores should be the same as the number of documents"
    final_results = [x for _, x in sorted(zip(scores, results), key=lambda x: x[0], reverse=True)]

  return final_results


def documents_to_section(results, query):

  user_prompt=""
  current_paper_number=0
  used_refs=[]
  used_papers=set()
  for i,result in enumerate(results):

    if current_paper_number>=number_of_docs_to_gpt:
      break

    if result["title"] in used_papers:
       continue
    current_paper_number+=1

    paper_text=paper_template.format(I=current_paper_number, CONTENT=result["content"], TITLE=result["title"])
    user_prompt+=paper_text + "\n"
    used_papers.add(result["title"])

    if result["reference"]:
      used_refs.append(result["reference"])

  user_prompt+= "Paragraph Subject: " + query

  current_msgs=template_messages.copy() 

  current_msgs.append({
      "role": "user",
      "content": user_prompt
  })

  debug_logger.debug(f"prompt for model {user_prompt}")
  response=agent.complete(current_msgs)
  debug_logger.debug(f"response from model {response}", )

  return response, used_refs


# python -m test.full_test --survey_data C:\Users\Thiago\Documents\projeto_final_ia368\AutoSurvey\data\dataset\survey_1.json --out_path proc5/proc5.json

# /content/AutoSurvey/data/dataset/survey_3.json


def write_sections(survey_title, sections):

    print(sections)

    sections_data=[]

    debug_logger.debug(f"survey title -> {survey_title}, sections -> {sections}")

    model = None
    if reranker:
      model = MonoT5(reranker_model, fp16=True)

    for section in tqdm(sections, desc="Processing sections"):
      documents=query_to_documents(section, survey_title=survey_title,filters=None, rerank=reranker, model=model)
      if len(documents)==0:
          print("skipping section, no relevant documents found for query", section)
          continue

      if not reranker:
          documents=sorted(documents, key=lambda x: x["influentialCitationCount"], reverse=True)


      section_text, used_refs=documents_to_section(documents, section)
      sections_data.append({
          "content": section_text,
          "references": used_refs,
          "title": section,
      })
      print(sections_data)


    # with open(out_path, "w") as f:
    #   json.dump(sections_data, f, indent=4)

    return sections_data


if not os.path.exists(out_path):
  from pathlib import Path
  os.makedirs(Path(out_path).parent, exist_ok=True)


with open(survey_data, "r") as f:
    data = json.load(f)


survey_title=list(data.keys())[0]

first_level_sections=[key for key in data[survey_title].keys() if data[survey_title][key]["content"] != "" and key.lower() not in ["introduction", "conclusion"]]

second_level_sections={}
for section in first_level_sections:
    if data[survey_title][section].get("subsections"):
        second_level_sections[section]=[key for key in data[survey_title][section]["subsections"].keys() if data[survey_title][section]["subsections"][key]["content"] != ""]

    

# write 2 level sections

all_survey_data={
    survey_title: {}
}
for section in list(second_level_sections.keys()):
    
    all_survey_data[survey_title][section]={}
    all_survey_data[survey_title][section]["subsections"]={}

    subsections=write_sections(section, second_level_sections[section])
    for subsection in subsections:
       all_survey_data[survey_title][section]["subsections"][subsection["title"]]=subsection
       

# write 1 level sections

for section in first_level_sections:
    
    section_content=write_sections(survey_title, [section])[0]

    if section not in all_survey_data[survey_title]:
        all_survey_data[survey_title][section]={}
    all_survey_data[survey_title][section]["content"]= section_content["content"]
    all_survey_data[survey_title][section]["references"]= section_content["references"]
  
with open(out_path, "w") as f:
  json.dump(all_survey_data, f, indent=4)

# pdf_path=args.out_path.replace(".json", ".pdf")
# PDFGenerator.generate_pdf(survey_title, sections_data, output_file=pdf_path)