{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji8wCG1_fQbw",
        "outputId": "d8e279cd-f544-4952-aeac-659387ee0267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/ZanezZephyrs/AutoSurvey.git\n",
            "  Cloning https://****@github.com/ZanezZephyrs/AutoSurvey.git to /tmp/pip-req-build-b51mf23f\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/ZanezZephyrs/AutoSurvey.git' /tmp/pip-req-build-b51mf23f\n",
            "  Resolved https://****@github.com/ZanezZephyrs/AutoSurvey.git to commit fbe3c6b849af343e8cb416c18a72638aace808a1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai (from AutoSurvey==0.1)\n",
            "  Downloading openai-1.3.9-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfkit (from AutoSurvey==0.1)\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting py3-wkhtmltopdf==0.4.1 (from AutoSurvey==0.1)\n",
            "  Downloading py3_wkhtmltopdf-0.4.1-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from AutoSurvey==0.1) (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from AutoSurvey==0.1) (4.35.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from AutoSurvey==0.1) (2.1.0+cu118)\n",
            "Collecting sentencepiece (from AutoSurvey==0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from AutoSurvey==0.1) (1.10.13)\n",
            "Collecting PyMuPDF (from AutoSurvey==0.1)\n",
            "  Downloading PyMuPDF-1.23.7-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->AutoSurvey==0.1) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->AutoSurvey==0.1) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->AutoSurvey==0.1)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->AutoSurvey==0.1) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai->AutoSurvey==0.1) (4.5.0)\n",
            "Collecting PyMuPDFb==1.23.7 (from PyMuPDF->AutoSurvey==0.1)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->AutoSurvey==0.1) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->AutoSurvey==0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->AutoSurvey==0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->AutoSurvey==0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->AutoSurvey==0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->AutoSurvey==0.1) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->AutoSurvey==0.1) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->AutoSurvey==0.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->AutoSurvey==0.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->AutoSurvey==0.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->AutoSurvey==0.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->AutoSurvey==0.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->AutoSurvey==0.1) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->AutoSurvey==0.1) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->AutoSurvey==0.1) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->AutoSurvey==0.1) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->AutoSurvey==0.1) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->AutoSurvey==0.1)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->AutoSurvey==0.1)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->AutoSurvey==0.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->AutoSurvey==0.1) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->AutoSurvey==0.1) (2.0.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->AutoSurvey==0.1) (1.3.0)\n",
            "Building wheels for collected packages: AutoSurvey\n",
            "  Building wheel for AutoSurvey (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoSurvey: filename=AutoSurvey-0.1-py3-none-any.whl size=9683 sha256=a416d3639bf93debc887f370ec780a4a6733cad91a26166d674661ebf888370e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b28sxuc4/wheels/87/30/b3/b3754cdcf2aafdd61cd408f0140922e54aa3a63c752a1b7530\n",
            "Successfully built AutoSurvey\n",
            "Installing collected packages: sentencepiece, py3-wkhtmltopdf, pdfkit, PyMuPDFb, h11, PyMuPDF, httpcore, httpx, openai, AutoSurvey\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoSurvey-0.1 PyMuPDF-1.23.7 PyMuPDFb-1.23.7 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.9 pdfkit-1.0.0 py3-wkhtmltopdf-0.4.1 sentencepiece-0.1.99\n",
            "Requirement already satisfied: py3-wkhtmltopdf==0.4.1 in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: pdfkit in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://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@github.com/ZanezZephyrs/AutoSurvey.git\n",
        "!pip install py3-wkhtmltopdf==0.4.1\n",
        "!pip install pdfkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "-cqLAbTVwV7r",
        "outputId": "75596cd8-0a9e-4c30-c905-c2c9d7cf9229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.3.9\n",
            "    Uninstalling openai-1.3.9:\n",
            "      Successfully uninstalled openai-1.3.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNGI37nikMyN",
        "outputId": "3cfd217d-6a6b-4597-9a72-59a5a1b308a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'AutoSurvey' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/ZanezZephyrs/AutoSurvey.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj5k5Dd4j3le"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "sections_path=\"/content/drive/MyDrive/deep_learning_presentations/Geração de Surveys/dataset/2108.05542v2\"\n",
        "mmd_path=\"/content/drive/MyDrive/deep_learning_presentations/Geração de Surveys/dataset/2108.05542v2/cited_papers\"\n",
        "\n",
        "data=[]\n",
        "with open(f\"{sections_path}/sections.json\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for line in f:\n",
        "      data.append(json.loads(line))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohL_RTWRlB0u"
      },
      "outputs": [],
      "source": [
        "docs_for_sections={}\n",
        "for section in data[1:]:\n",
        "    citations=section[\"citations\"]\n",
        "    title=section[\"title\"]\n",
        "    if len(citations) == 0:\n",
        "      continue\n",
        "    docs_for_sections[title]=[]\n",
        "    for citation in citations:\n",
        "        try:\n",
        "          doc_path=f\"{mmd_path}/{citation}.mmd\"\n",
        "\n",
        "          doc=open(doc_path, encoding=\"utf-8\")\n",
        "          content=doc.read()\n",
        "\n",
        "          doc_title=content.split(\"\\n\")[0]\n",
        "          docs_for_sections[title].append({\n",
        "              \"title\": doc_title,\n",
        "              \"content\": content,\n",
        "              \"reference\": citation\n",
        "            })\n",
        "        except FileNotFoundError:\n",
        "          pass\n",
        "\n",
        "    if len(docs_for_sections) == 5:\n",
        "      break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6gFDl1wn90O",
        "outputId": "c0e66c8c-ba4f-4e98-c394-1ec32a3968bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n",
            "8\n",
            "10\n",
            "24\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "for doc in docs_for_sections:\n",
        "  print(len(docs_for_sections[doc]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3rWiZsbiUZF"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "\n",
        "\n",
        "def get_windows(input_text: str, window_size: int = 150, stride: int = 75):\n",
        "    words = input_text.split()\n",
        "    windows = []\n",
        "    for i in range(0, len(words), stride):\n",
        "        prefix = \"...\" if i != 0 else \"\"\n",
        "        window_words = words[i : i + window_size]\n",
        "        windows.append(prefix + \" \".join(window_words))\n",
        "        if window_size > len(window_words):\n",
        "            break\n",
        "    return windows\n",
        "\n",
        "def get_pdf_windows(pdf_path: str, window_size: int = 150, stride: int = 75):\n",
        "    pdf_doc = fitz.open(pdf_path)\n",
        "    num_pages = pdf_doc.page_count\n",
        "    content = \"\"\n",
        "    for page_num in range(num_pages):\n",
        "        page = pdf_doc.load_page(page_num)\n",
        "        text = page.get_text()\n",
        "        content += text + \"\\n\"\n",
        "    windows = get_windows(content, window_size, stride)\n",
        "    return windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d93cf38c290844019a778c6ecb47a4ef",
            "2625ee949f9c457cb0af24a6f56e1c60",
            "d7fbe5f03a3d4f7e8f78ee1e5f4b3b60",
            "3438f15a5e2b412e8fca22116e18edb9",
            "7189d62627464e399f2271fcd51eb2fd",
            "4458ed8acb7d4d0cb474f093b812987c",
            "ebee447b6b8b457d870e64e6e1d4bf39",
            "c026b4938e4d47b7a91d7a3e5d4e81b3",
            "709aa5586c0743e9bf50ff5ce4a9febb",
            "3c7495f901fa48a7a1270f19f42be7e3",
            "89a625de44f84ec78538a78c62c46a21",
            "dbb968b38049414b82c9004e8ef7d928",
            "a01196086dde4c1d9a122a58cd2b59d2",
            "5fc1e69e67094d45a96630e91720ad0f",
            "17baaae9832b40ffba8a73e8a397ab2c",
            "a56fba857f7947dc8d40b82698197b79",
            "f303a4a9a1a8489d9960baf170260911",
            "52e3e56762424d96b4874a41e3ded68e",
            "d7207f1c87af4b4288501f9f1055d85d",
            "48597d09b6a44497a83d7ae8fba40227",
            "11fbc62221b4444a9b8480de19a9cc74",
            "32091e57e4cd4e1d96e843dab603744f",
            "55a30a6889274d839505c784dbcbed4b",
            "e3a738a0b9df41499763bbad9c270a14",
            "4b47688e79d645b882aaa5ab2683f4f4",
            "ca9f120ced8b4f9d81ea8074f39454e2",
            "af5012a208274397857a15b45cb04b7f",
            "b680002fd1a54391a4a2dbb18b0f02a6",
            "c4318244710f4ae18059654fe1199e61",
            "0614480e7dd94d57b2711364d1f85221",
            "f91ca517ee0548c9a96d03ce7a1cb1e0",
            "ed4117e342a042e9a34b445937ac57bf",
            "93a32dc979994e98b12dd817b6d4ed27",
            "748934279bf84046a66e4d86568f0f39",
            "0a7f22795dcb407a8cfd905088e9918a",
            "dba4e9f697eb44cd993eb52ab8850fc8",
            "284c827ba540424dbe5420468beb4ecf",
            "58df4349a7154f6c953f80a573698e10",
            "99bcce740b7c4802bb9c958a34037f94",
            "2bf8b7fe88d34d3f9450c0d720ba7052",
            "8765610536dc44589b4ffab2aa3b13aa",
            "539e3b5db2f044198641dfb0ba5d0e35",
            "6d6484ba330c4983a40717df596ca713",
            "466c92185989458188ca346d5e962c4f",
            "ee567386d95d4994ad64feea31bb57e4",
            "e911e48424e34ef3aae1d9ca3c9ee42e",
            "e32858f32cdb43389afa93cc9d6d910e",
            "afefcdd7013f45b9a5b83e8d5719970e",
            "986c91072b7b43fa8cfa9acbd45cb0ee",
            "3c6ebaf2b61d4f78b06e38d9ec608e25",
            "4d8b181eefc143529284a31307dab39b",
            "6cd109365d73464da7c8fe0a56db762b",
            "92e3b5e939b54f5ca3c99d9a63caef4d",
            "ae55f98eed9f4ff788be06d7060e0a8d",
            "0523ff41d58442c6bcca3f0303bffa8f"
          ]
        },
        "id": "XgzbFMl8hayh",
        "outputId": "38459ae7-8786-4439-80db-fad87caabea7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:debug:survey title -> AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, sections -> ['Self-Supervised Learning (SSL)']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Self-Supervised Learning (SSL)']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d93cf38c290844019a778c6ecb47a4ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing sections:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:debug:reranking documents with query AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, Self-Supervised Learning (SSL)\n",
            "DEBUG:debug:prompt for model - Paper 1\n",
            "Title:# Transformers in Vision: A Survey\n",
            "Content: ...and decoder. example, Vision Transformer model (ViT-L) [11] experiences an absolute \\(13\\%\\) drop in accuracy on ImageNet test set when trained only on ImageNet train set as compared to the case when pretrained on JFT dataset [47] with 300 million images. Since acquiring manual labels at a massive scale is cumbersome, self-supervised learning has been very effectively used in the pre-training stage. The self-supervision based pre-training stage training has played a crucial role in unleashing the scalability and generalization of Transformer networks, enabling training even above a _trillion_ parameter networks (e.g., the latest Switch Transformer [10] from Google). An extensive survey on SSL can be found in [48, 49]. As nicely summarized by Y. LeCun [50], the basic idea of SSL is to _fill in the blanks_, i.e., try to predict the occluded data in images, future or past frames in temporal video sequences or predict a pretext task _e.g._,\n",
            "\n",
            "- Paper 2\n",
            "Title:# Pre-trained Models for Natural Language Processing: A Survey\n",
            "Content: # Pre-trained Models for Natural Language Processing: A Survey Xipeng Qiu Corresponding author (email: xpqiu@fudan.edu.cn) Tianxiang Sun Yige Xu Yunfan Shao Ning Dai & Xuanjing Huang School of Computer Science, Fudan University, Shanghai 200433, China Shanghai Key Laboratory of Intelligent Information Processing, Shanghai 200433, China ###### Abstract Recently, the emergence of pre-trained models (PTMs)* has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks. **Deep Learning, Neural Network, Natural Language\n",
            "\n",
            "- Paper 3\n",
            "Title:# A Survey on Visual Transformer\n",
            "Content: ...milestone in pre-trained language models. Generative Pre-trained Transformer models (_e.g._, GPT [258], GPT-2 [110]) are another type of PTMs based on the transformer decoder architecture, which uses masked self-attention mechanisms. The main difference between the GPT series and BERT is the way in which pre-training is performed. Unlike BERT, GPT models are unidirectional language models pre-trained using Left-to-Right (LTR) language modeling. Furthermore, BERT learns the sentence separator ([SEP]) and classifier token ([CLS]) embeddings during pre-training, whereas these embeddings are involved in only the fine-tuning stage of GPT. Due to its unidirectional pre-training strategy, GPT achieves superior performance in many natural language generation tasks. More recently, a massive transformer-based model called GPT-3, which has an astonishing 175 billion parameters, was developed [11]. By pre-training on 45 TB of compressed plaintext data, GPT-3 can directly process different types of downstream natural language tasks without fine-tuning. As a result, it achieves strong performance\n",
            "\n",
            "- Paper 4\n",
            "Title:# Self-supervised Learning: Generative or Contrastive\n",
            "Content: ...attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analysis on self-supervised learning to provide deeper thoughts on why self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided1. Self-supervised Learning, Generative Model, Contrastive Learning, Deep Learning ###### Contents * 1 Introduction * 2 Motivation of Self-supervised Learning * 3 Generative Self-supervised Learning * 3.1 Auto-regressive (AR) Model * 3.2 Flow-based Model *\n",
            "\n",
            "- Paper 5\n",
            "Title:# Self-Supervised Learning for Personalized Speech Enhancement\n",
            "Content: ...investigate whether alternate choices for \\(\\lambda_{p}\\) and \\(\\lambda_{n}\\) are needed for ConvTasNet. Models pretrained using \\(5\\,\\mathrm{dB}\\) premixtures usually underperforms the 10 dB mixtures, meaning premixture signals' quality matters for a successful SSL feature training. However, the proposed models pretrained on the \\(5\\,\\mathrm{dB}\\) premixtures remain competitive with the multi-speaker-based models. As discussed in Section 1, a model with a small number of parameters achieving comparable performance to a model made up of many more parameters suggests that the smaller model is more advantageous on devices with limited resources. There is an inherent tradeoff between model compression and our overarching FSL objective, i.e., we want to minimize the use of clean speech, while the more of it results in a better performance. But overall, the usage of a self-supervised initialization method allows smaller PSE models to exceed the performance of larger baseline models. For example, a CM-pretrained \\(\\mathrm{GRU}\\colon 128\\times 2\\) model using\n",
            "\n",
            "Paragraph Subject: Self-Supervised Learning (SSL)\n",
            "DEBUG:debug:response from model Self-supervised learning (SSL) has demonstrated impressive results and has grown significantly as an area of active research due to its performance in representation learning across various fields. SSL involves leveraging the input data itself to provide supervision, thus benefiting numerous types of downstream tasks [4]. Given the vast amount of unlabeled data that exists, this method offers a cost-effective strategy for training deep learning models as it reduces the need for expensive labeled data [1,4]. SSL is utilised greatly in training Transformer networks with recent advancements enabling training above a trillion parameter networks [1].\n",
            "\n",
            "Visual transformers (ViT-L) showed a dramatic decrease in precision when trained solely on the ImageNet train set as compared to when pre-trained on an extensive JFT dataset with 300 million images. This demonstrates the beneficial role of SSL in the pre-training stage, substantially enabling the scalability and generalization of Transformer networks [1]. In the realm of natural language processing, Generative Pre-trained Transformer models such as GPT, GPT-2, and GPT-3, have been successful due to pre-training strategies that use self-supervised learning [3]. Particularly, GPT-3, which boasts a colossal 175 billion parameters, was pre-trained on 45 TB of compressed plaintext data, resulting in robust performance in downstream natural language tasks without the necessity for fine-tuning [3]. \n",
            "\n",
            "In the auditory field, ConvTasNet models pre-trained using SSL demonstrated that the quality of premixtures matters for successful SSL feature training. The results indicated that models pre-trained using lower quality premixtures performed competitively with multi-speaker-based models, a pivotal factor for employing self-supervised initialization that allows smaller models to exceed the performance of larger baseline models [5]. This highlights SSL's striking potential across different domains, making it a promising direction for future research. Interestingly, SSL is also grouped into various categories such as generative, contrastive, and generative-contrastive (adversarial), based on their objectives [4]. Further exploration and understanding of these categories will provide profound insights into the effectiveness of SSL and its broader applicability in diverse fields.\n",
            "\n",
            "DEBUG:debug:survey title -> AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, sections -> ['Self-Supervised Learning (SSL)::Types of Self-Supervised Learning']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'content': \"Self-supervised learning (SSL) has demonstrated impressive results and has grown significantly as an area of active research due to its performance in representation learning across various fields. SSL involves leveraging the input data itself to provide supervision, thus benefiting numerous types of downstream tasks [4]. Given the vast amount of unlabeled data that exists, this method offers a cost-effective strategy for training deep learning models as it reduces the need for expensive labeled data [1,4]. SSL is utilised greatly in training Transformer networks with recent advancements enabling training above a trillion parameter networks [1].\\n\\nVisual transformers (ViT-L) showed a dramatic decrease in precision when trained solely on the ImageNet train set as compared to when pre-trained on an extensive JFT dataset with 300 million images. This demonstrates the beneficial role of SSL in the pre-training stage, substantially enabling the scalability and generalization of Transformer networks [1]. In the realm of natural language processing, Generative Pre-trained Transformer models such as GPT, GPT-2, and GPT-3, have been successful due to pre-training strategies that use self-supervised learning [3]. Particularly, GPT-3, which boasts a colossal 175 billion parameters, was pre-trained on 45 TB of compressed plaintext data, resulting in robust performance in downstream natural language tasks without the necessity for fine-tuning [3]. \\n\\nIn the auditory field, ConvTasNet models pre-trained using SSL demonstrated that the quality of premixtures matters for successful SSL feature training. The results indicated that models pre-trained using lower quality premixtures performed competitively with multi-speaker-based models, a pivotal factor for employing self-supervised initialization that allows smaller models to exceed the performance of larger baseline models [5]. This highlights SSL's striking potential across different domains, making it a promising direction for future research. Interestingly, SSL is also grouped into various categories such as generative, contrastive, and generative-contrastive (adversarial), based on their objectives [4]. Further exploration and understanding of these categories will provide profound insights into the effectiveness of SSL and its broader applicability in diverse fields.\\n\", 'references': [], 'title': 'Self-Supervised Learning (SSL)'}]\n",
            "['Self-Supervised Learning (SSL)::Types of Self-Supervised Learning']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbb968b38049414b82c9004e8ef7d928",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing sections:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:debug:reranking documents with query AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, Self-Supervised Learning (SSL)::Types of Self-Supervised Learning\n",
            "DEBUG:debug:prompt for model - Paper 1\n",
            "Title:# Self-supervised Learning: Generative or Contrastive\n",
            "Content: ...attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further collect related theoretical analysis on self-supervised learning to provide deeper thoughts on why self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided1. Self-supervised Learning, Generative Model, Contrastive Learning, Deep Learning ###### Contents * 1 Introduction * 2 Motivation of Self-supervised Learning * 3 Generative Self-supervised Learning * 3.1 Auto-regressive (AR) Model * 3.2 Flow-based Model *\n",
            "\n",
            "- Paper 2\n",
            "Title:# Improving Language Understanding\n",
            "Content: ...us to fine-tune effectively with minimal changes to the architecture of the pre-trained model. We evaluate our approach on four types of language understanding tasks - natural language inference, question answering, semantic similarity, and text classification. Our general task-agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66] and 5.5% on the recently introduced GLUE multi-task benchmark [64]. We also analyzed zero-shot behaviors of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks. ## 2 Related Work Semi-supervised learning for NLPOur work broadly falls under the category of semi-supervised learning for natural language. This\n",
            "\n",
            "- Paper 3\n",
            "Title:# Efficient pre-training objectives for Transformers\n",
            "Content: ...the SLM-all approach compared with the \\(86.9\\) of ELECTRA). These results point out the quality of SLM-all applied to an ELECTRA model, but also its limits. Indeed, ELECTRA-SLM-all demonstrates that the advantages of transforming the ELECTRA discriminator into a generative network is expensive and does not ensure better performance for the majority of the tasks, but could be still a valuable choice for transfer learning. ## 6 Conclusion and Future Works In this work, we studied several methods to efficiently pre-train Transformer models. Our approaches aim to match the results of well known pre-training objectives such as MLM but consuming less computational resources. These research directions have several benefits, since a lower computational effort leads to a shorter training and to lower memory usage. We evaluate our approach using several benchmark dataset such as GLUE, SQUAD, WikiQA and ASNQ-R to have a clear understanding of the behaviours of our models.\n",
            "\n",
            "- Paper 4\n",
            "Title:# Albert: A Lite BERT for Self-supervised Learning of Language Representations\n",
            "Content: # Albert: A Lite BERT for Self-supervised Learning of Language Representations Zhenzhong Lan\\({}^{1}\\) Mingda Chen\\({}^{2}\\)1 Sebastian Goodman\\({}^{1}\\) Kevin Gimpel\\({}^{2}\\) Piyush Sharma\\({}^{1}\\) Radu Soricut\\({}^{1}\\) \\({}^{1}\\)Google Research \\({}^{2}\\)Toyota Technological Institute at Chicago {lanzhzh, seabass, piyushsharma, rsoricut}@google.com {mchen, kgimpel}@ttic.edu Footnote 1: Work done as an intern at Google Research, driving data processing and downstream task evaluations. ###### Abstract Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps\n",
            "\n",
            "- Paper 5\n",
            "Title:# Shuffled-token Detection for Refining Pre-trained RoBERTa\n",
            "Content: ...Footnote 1: The code is available at [https://github.com/subhadarship/learning-to-unjumble](https://github.com/subhadarship/learning-to-unjumble). ## 1 Introduction The method of pre-training natural language models has been shown to greatly improve model performance on a wide range of NLP tasks Peters et al. (2018); Radford et al. (2018); Howard and Ruder (2018). State-of-the-art models that utilize transformers and deep bi-directional representations of text such as BERT, RoBERTa, and ALBERT Devlin et al. (2019); Liu et al. (2019); Lan et al. (2020) have achieved superior results by pre-training on general, large corpora to learn rich representations from unlabeled data. Particularly helpful in low training data resource scenarios, unsupervised pre-training has become the first step for many language models to build powerful linguistic representations before fine tuning for downstream target tasks. BERT style models use masked language modeling (MLM) and sometimes next sentence prediction, as pre-training tasks. While these tasks have been shown to produce transferable sentence representations for\n",
            "\n",
            "Paragraph Subject: Self-Supervised Learning (SSL)::Types of Self-Supervised Learning\n",
            "DEBUG:debug:response from model Self-Supervised Learning has shown significant progress in representation learning across different domains such as computer vision, natural language processing, and graph learning [1]. Generative, contrastive, and generative-contrastive (adversarial) are the three main categories into which self-supervised learning methods can be classified based on their objectives [1]. While generative models consider the distribution of the input data itself, contrastive methods differentiate between positive and negative examples in the data [1]. In many natural language processing tasks, generative pretraining has been successful in enhancing model performance [2]. For instance, BERT, RoBERTa, and ALBERT are preeminent models that use transformers and deep bi-directional text representations [5]. These models have shown their superiority by pretraining on large general corpora to learn rich representations from unlabeled data, particularly beneficial in low training data resource scenarios. \n",
            "\n",
            "Moreover, numerous studies have been devoted to improving the efficiency of pre-training methods for Transformer models. ELECTRA-SLM-all demonstrates the value of converting the ELECTRA discriminator into a generative network and how it can be a good choice for transfer learning, although it does not guarantee a better performance [3]. Memory consumption and the speed of BERT training can also be reduced through parameter-reduction techniques, which allows for more extensive model scaling as compared to the original BERT. This method also employs a self-supervised loss that concentrates on modelling inter-sentence coherence to consistently improve the results [4]. Despite the progress and potential demonstrated to date, the exploration of SSL's capabilities and limits, especially in terms of generative and contrastive approaches, remains an open area for future research [1].\n",
            "DEBUG:debug:survey title -> AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, sections -> ['T-PTLM Core Concepts::Pretraining']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'content': \"Self-Supervised Learning has shown significant progress in representation learning across different domains such as computer vision, natural language processing, and graph learning [1]. Generative, contrastive, and generative-contrastive (adversarial) are the three main categories into which self-supervised learning methods can be classified based on their objectives [1]. While generative models consider the distribution of the input data itself, contrastive methods differentiate between positive and negative examples in the data [1]. In many natural language processing tasks, generative pretraining has been successful in enhancing model performance [2]. For instance, BERT, RoBERTa, and ALBERT are preeminent models that use transformers and deep bi-directional text representations [5]. These models have shown their superiority by pretraining on large general corpora to learn rich representations from unlabeled data, particularly beneficial in low training data resource scenarios. \\n\\nMoreover, numerous studies have been devoted to improving the efficiency of pre-training methods for Transformer models. ELECTRA-SLM-all demonstrates the value of converting the ELECTRA discriminator into a generative network and how it can be a good choice for transfer learning, although it does not guarantee a better performance [3]. Memory consumption and the speed of BERT training can also be reduced through parameter-reduction techniques, which allows for more extensive model scaling as compared to the original BERT. This method also employs a self-supervised loss that concentrates on modelling inter-sentence coherence to consistently improve the results [4]. Despite the progress and potential demonstrated to date, the exploration of SSL's capabilities and limits, especially in terms of generative and contrastive approaches, remains an open area for future research [1].\", 'references': [], 'title': 'Self-Supervised Learning (SSL)::Types of Self-Supervised Learning'}]\n",
            "['T-PTLM Core Concepts::Pretraining']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55a30a6889274d839505c784dbcbed4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing sections:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:debug:reranking documents with query AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, T-PTLM Core Concepts::Pretraining\n",
            "DEBUG:debug:prompt for model - Paper 1\n",
            "Title:# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "Content: ...### Cb **Original input:**: **Hypothesis:**: Valence was helping **Premise:**: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? **Processed input:**: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? **Original target:**: 1 **Processed target:**: contradiction ### Copa **Original input:** **Question:** effect **Premise:** Political violence broke out in the nation. **Choice 1:** Many citizens relocated to the capitol. **Choice 2:** Many citizens took refuge in other territories. **Processed input:** copa choice1:** Many citizens relocated to the capitol. choice2: Many citizens took refuge in other territories. premise: Political violence broke out in the nation. question: effect **Original target:** 1 **Processed target:** True ### MultiRC **Original input:** **Answer:** There was only pie to\n",
            "\n",
            "- Paper 2\n",
            "Title:# ELECTRA: Pre-training Text Encoders\n",
            "Content: ...pre-train Transformer text encoders (Vaswani et al., 2017) that can be fine-tuned on downstream tasks. Through a series of ablations, we show that learning from all input positions causes ELECTRA to train much faster than BERT. We also show ELECTRA achieves higher accuracy on downstream tasks when fully trained. Footnote 1: Code and pre-trained weights will be released at [https://github.com/google-research/electra](https://github.com/google-research/electra) Most current pre-training methods require large amounts of compute to be effective, raising concerns about their cost and accessibility. Since pre-training with more compute almost always results in better downstream accuracies, we argue an important consideration for pre-training methods should be compute efficiency as well as absolute downstream performance. From this viewpoint, we train ELECTRA models of various sizes and evaluate their downstream performance vs. their compute requirement. In particular, we run experiments on the GLUE natural language understanding benchmark (Wang et al., 2019) and SQuAD question answering benchmark (Rajpurkar\n",
            "\n",
            "- Paper 3\n",
            "Title:# XLNet: Generalized Autoregressive Pretraining\n",
            "Content: ...that BERT is subject to. Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining. * Inspired by the latest advancements in AR language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence. * Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity. Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD\n",
            "\n",
            "- Paper 4\n",
            "Title:# RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
            "Content: ...review in detail. We use a transformer architecture with \\(L\\) layers. Each block uses \\(A\\) self-attention heads and hidden dimension \\(H\\). ### Training Objectives During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction. Masked Language Model (MLM)A random sample of the tokens in the input sequence is selected and replaced with the special token \\([\\mathit{MASK}]\\). The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with \\([\\mathit{MASK}]\\), 10% are left unchanged, and 10% are replaced by a randomly selected vocabulary token. In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1). Next Sentence Prediction\n",
            "\n",
            "- Paper 5\n",
            "Title:Deep Transfer Learning Application for Automated Ischemic Classification in Posterior Fossa CT Images\n",
            "Content: ...This is particularly important for small training dataset cases. The pre-trained models appear to possess relatively good performance with previous knowledge of large scale data of the existing model. This section describes a background on three popular pre-trained models (VGG-16, GoogleNet and ResNet-50) which have been proven to be excellent in medical image classification [28][29]. ### _Vgg-16_ VGG-16 is the CNN model developed by Simonyan and Zisserman [25] with the best performance of 92.7% top-5 test accuracy in ImageNet. Fig. 2 illustrates the architecture model of VGG-16. This was developed and tested on a number of 1000 classes. The input required for the convolution layer is RGB with a fixed size of 224x224. The input image is passed via a stack of convolutional layers with ReLU activations in which filters are applied with very small receptive fields of 3x3. Meanwhile, the stride of convolution is fixed to 1. In the\n",
            "\n",
            "Paragraph Subject: T-PTLM Core Concepts::Pretraining\n",
            "DEBUG:debug:response from model Pretraining forms a crucial part of many transformer language models strategies targeted at enhancing the effectiveness of these systems [1-4]. The pretraining procedures, often reliant on Transformer architectures, are optimized to train the systems to perform a variety of self-supervisory pseudo-tasks [1,2,4]. Notably, the Transformer architecture, containing self-attention heads and hidden dimensions, are utilized in accomplishing the end goals of these models [4]. Several objectives are set for pretraining these deep learning systems. For instance, the BERT model uses two objectives: masked language modeling and next sentence prediction [4]. These pseudo-tasks help to train the model by predicting the masked tokens from the input sequence or determining the relationship between sentences [4]. \n",
            "\n",
            "Another widely used model, ELECTRA, pre-trains Transformer text encoders that helps in improving its computation efficiency and downstream accuracies [2]. Transformer-based models such XLNet integrate segment recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining procedures, focusing on tasks involving longer text sequences [3]. Such advancements are seen to improve the performance of the models in natural language understanding, reading comprehension tasks such as the SQuAD benchmark. Indeed, these models prove to perform excellent in classification tasks [5] [1,2,3,4].\n",
            "DEBUG:debug:survey title -> AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, sections -> ['T-PTLM Core Concepts::Pretraining::Pretraining Steps']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'content': 'Pretraining forms a crucial part of many transformer language models strategies targeted at enhancing the effectiveness of these systems [1-4]. The pretraining procedures, often reliant on Transformer architectures, are optimized to train the systems to perform a variety of self-supervisory pseudo-tasks [1,2,4]. Notably, the Transformer architecture, containing self-attention heads and hidden dimensions, are utilized in accomplishing the end goals of these models [4]. Several objectives are set for pretraining these deep learning systems. For instance, the BERT model uses two objectives: masked language modeling and next sentence prediction [4]. These pseudo-tasks help to train the model by predicting the masked tokens from the input sequence or determining the relationship between sentences [4]. \\n\\nAnother widely used model, ELECTRA, pre-trains Transformer text encoders that helps in improving its computation efficiency and downstream accuracies [2]. Transformer-based models such XLNet integrate segment recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining procedures, focusing on tasks involving longer text sequences [3]. Such advancements are seen to improve the performance of the models in natural language understanding, reading comprehension tasks such as the SQuAD benchmark. Indeed, these models prove to perform excellent in classification tasks [5] [1,2,3,4].', 'references': [], 'title': 'T-PTLM Core Concepts::Pretraining'}]\n",
            "['T-PTLM Core Concepts::Pretraining::Pretraining Steps']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "748934279bf84046a66e4d86568f0f39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing sections:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:debug:reranking documents with query AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, T-PTLM Core Concepts::Pretraining::Pretraining Steps\n",
            "DEBUG:debug:prompt for model - Paper 1\n",
            "Title:CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters\n",
            "Content: ...pre-training phase consists of two steps: Footnote 9: More specifically, we adapt these scripts to our needs. **Step 1**: 3,519 updates with a batch size10 of 8,192 and a learning rate of \\(6.10^{-3}\\) on sequences of size 128. **Step 2**: 782 updates with a batch size of 4,096 and a learning rate of \\(4.10^{-3}\\) on sequences of size 512. Footnote 10: We use gradient accumulation for larger batch sizes. All models are optimized using LAMB (You et al., 2019) with a warm-up rate and weight decay of \\(0.01\\). ### Evaluation Phase #### 4.3.1 Tasks All models are evaluated on five medical tasks after adding task-specific layers (Devlin et al., 2019). **Medical Entity Recognition**: We evaluate our models on the i2b2/VA 2010 (Uzuner et al., 2011) clinical concept extraction task which aims to extract three types of medical concepts: problem (e.g. \"headache\"), treatment (e.g. \"oxycodone\"), and test (e.g. \"MRI\"). **Natural Language\n",
            "\n",
            "- Paper 2\n",
            "Title:# Pre-training technique to localize medical BERT and enhance biomedical BERT\n",
            "Content: ...using bidirectional transformers [11]. The BERT framework consists of two steps: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled large corpora. For fine-tuning, the BERT model is first initialized with pre-trained weights, and all the weights are fine-tuned using labeled data from Figure 1: Overview of pre-training BERT. Figure 2: Pre-training procedures for BERT (adapted from Devlin 2019). the downstream tasks. We applied minimal architectural modifications to the task-specific inputs and outputs into BERT and fine-tuned all the parameters in an end-to-end manner. #### 2.1.1 Pre-training BERT pre-training is optimized for two unsupervised classification tasks (Figure 2). The first is MLM. One training instance of MLM is a single modified sentence. Each token in the sentence has a 15% chance of being replaced by a special token [MASK]. The chosen token is replaced with [MASK] 80% of the time, 10% with another random token, and the remaining\n",
            "\n",
            "- Paper 3\n",
            "Title:# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "Content: ...soon afterwards, and Jebe died on the road back to Samarkand. **Processed input:**: qnli question: Where did Jebe die? sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. **Original target:**: 0 **Processed target:**: entailment ### Qqp **Original input:**: **Question 1:**: What attributes would have made you highly desirable in ancient Rome? **Question 2:**: How I GET OPPETINUTY TO JOIN IT COMPANY AS A FRESHER? **Processed input:**: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPETINUTY TO JOIN IT COMPANY AS A FRESHER? **Original target:**: 0 **Processed target:**: not_duplicate ### Sst2 **Original input:**: **Sentence:**: it confirms fincher's status as a film maker who artfully bends technical know-how to the service of psychological insight. **Processed input:**: sst2 sentence: it confirms fincher's status as a film maker who artfully bends technical know-how to the\n",
            "\n",
            "- Paper 4\n",
            "Title:# Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language\n",
            "Content: ...as at 250 thousand step of multilingual initialization. It takes about two days to train for 250 thousand steps (on Tesla P100 x 8), so it helped us to save six days of computational time. Proposed averaging of new subtokens in vocabulary also has positive effect on the rate of convergence (instead of averaging we could take random initialization for new subtokens). ## 6 Conclusion In this work, we have shown that Transformer network pre-trained on the multilingual Masked Language Modelling task significantly improves performance on a number of Russian NLP tasks compared to existing solutions. Furthermore, language-specific unsupervised training with multilingual initialization results in even better improvements. Pre-trained models for the Russian language are open sourced, as well as code to reproduce our results as part of DeepPavlov library. ## 7 Acknowledgments This work was supported by National Technology Initiative and PAO Sberbank project ID 0000000007417F630002. Figure 2: Models\n",
            "\n",
            "- Paper 5\n",
            "Title:# Knowledge Inheritance for Pre-trained Language Models\n",
            "Content: ...vary RoBERTaMEDIUM's pre-training steps in \\(\\{62.5\\)k, \\(125\\)k, \\(250\\)k, \\(500\\)k\\(\\}\\), and keep all other settings the same. After pre-training, these teacher models achieve the final validation PPL of \\(\\{5.25,4.92,4.72,4.51\\}\\), respectively. Then we compare the performances when RoBERTaBASE learn from these teacher models and visualize the results in Figure 4, from which we can conclude that, inheriting knowledge from teachers with longer pre-training time (steps) helps \\(\\mathcal{M}_{L}\\) converge faster. However, such a benefit is less and less obvious as \\(\\mathcal{M}_{S}\\)'s pre-training steps increase, which means after enough training computations are invested, the teacher model enters a plateau of convergence in validation PPL, and digging deeper in knowledge becomes even harder. The bottleneck lies in other factors, e.g., the size and diversity of pre-training data, which hinder \\(\\mathcal{M}_{S}\\) from becoming more knowledgeable. We also found empirically that, after being pre-trained for \\(125\\)k steps on the corpus with a batch size of \\(2,048\\), all\n",
            "\n",
            "Paragraph Subject: T-PTLM Core Concepts::Pretraining::Pretraining Steps\n",
            "DEBUG:debug:response from model The first stage of developing Transformer-based Pretrained Language Models (T-PTLMs) is pretraining [1,2,5]. This phase essentially involves unsupervised learning on large-scale text corpora, with the intent of obtaining general language understanding capabilities. The number of pretraining steps employed has a significant impact on the learning process and the resultant performance of the model [1,5]. For instance, in the development of CharacterBERT, a pretraining phase comprised of 3,519 updates with a batch size of 8,192 was followed by 782 updates with a batch size of 4,096 [1]. Similarly, pretraining for BERT also uses two steps - initial pretraining and fine-tuning using labeled data [2]. In contrast, for the development of RoBERTaMEDIUM, variations in the pretraining steps were observed, suggesting that the number of steps has a crucial influence on the model’s performance. Specifically, final validation Perplexity (PPL) performance differed when pretraining steps varied in the range of {62.5k,125k,250k,500k} [5].\n",
            "\n",
            "The steps involved in each pretraining phase are also variant. For example, CharacterBERT employs updates with batch sizes and learning rates on sequences of different sizes [1], while BERT uses two unsupervised classification tasks for its pretraining- Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) [2]. Regardless of the diversity in step implementation and settings, pretraining uniformly aims to equip the models with a foundational understanding of language semantics and structure, serving as a significant determinant of the model's performance in downstream tasks [1,2,3,4,5].\n",
            "DEBUG:debug:survey title -> AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, sections -> ['T-PTLM Core Concepts::Pretraining::Pretraining Corpus']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'content': \"The first stage of developing Transformer-based Pretrained Language Models (T-PTLMs) is pretraining [1,2,5]. This phase essentially involves unsupervised learning on large-scale text corpora, with the intent of obtaining general language understanding capabilities. The number of pretraining steps employed has a significant impact on the learning process and the resultant performance of the model [1,5]. For instance, in the development of CharacterBERT, a pretraining phase comprised of 3,519 updates with a batch size of 8,192 was followed by 782 updates with a batch size of 4,096 [1]. Similarly, pretraining for BERT also uses two steps - initial pretraining and fine-tuning using labeled data [2]. In contrast, for the development of RoBERTaMEDIUM, variations in the pretraining steps were observed, suggesting that the number of steps has a crucial influence on the model’s performance. Specifically, final validation Perplexity (PPL) performance differed when pretraining steps varied in the range of {62.5k,125k,250k,500k} [5].\\n\\nThe steps involved in each pretraining phase are also variant. For example, CharacterBERT employs updates with batch sizes and learning rates on sequences of different sizes [1], while BERT uses two unsupervised classification tasks for its pretraining- Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) [2]. Regardless of the diversity in step implementation and settings, pretraining uniformly aims to equip the models with a foundational understanding of language semantics and structure, serving as a significant determinant of the model's performance in downstream tasks [1,2,3,4,5].\", 'references': [], 'title': 'T-PTLM Core Concepts::Pretraining::Pretraining Steps'}]\n",
            "['T-PTLM Core Concepts::Pretraining::Pretraining Corpus']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee567386d95d4994ad64feea31bb57e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing sections:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:debug:reranking documents with query AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, T-PTLM Core Concepts::Pretraining::Pretraining Corpus\n",
            "DEBUG:debug:prompt for model - Paper 1\n",
            "Title:# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "Content: ...soon afterwards, and Jebe died on the road back to Samarkand. **Processed input:**: qnli question: Where did Jebe die? sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. **Original target:**: 0 **Processed target:**: entailment ### Qqp **Original input:**: **Question 1:**: What attributes would have made you highly desirable in ancient Rome? **Question 2:**: How I GET OPPETINUTY TO JOIN IT COMPANY AS A FRESHER? **Processed input:**: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPETINUTY TO JOIN IT COMPANY AS A FRESHER? **Original target:**: 0 **Processed target:**: not_duplicate ### Sst2 **Original input:**: **Sentence:**: it confirms fincher's status as a film maker who artfully bends technical know-how to the service of psychological insight. **Processed input:**: sst2 sentence: it confirms fincher's status as a film maker who artfully bends technical know-how to the\n",
            "\n",
            "- Paper 2\n",
            "Title:# InfoXLM: An Information-Theoretic Framework for\n",
            "Content: ...(2020) for MMLM, which remains \\(94\\) languages by filtering the language code larger than 0.1GB. Following Conneau and Lample (2019), for the TLM and XlCo tasks, we employ \\(14\\) language pairs of parallel data that involves English. We collect translation pairs from MultiUN Ziemski et al. (2016), IIT Bombay Kunchukuttan et al. (2018), OPUS Tiedemann (2012), and WikiMatrix Schwenk et al. (2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model SizeWe follow the model configurations of XLM-R Conneau et al. (2020). For the Transformer Vaswani et al. (2017) architecture, we use 12 layers and 768 hidden states for InfoXLM (i.e., base size), and 24 layers and 1,024 hidden states for InfoXLM\\({}_{\\text{LARGE}}\\) (i.e., large size). HyperparametersWe initialize the parameters of InfoXLM with XLM-R. We optimize the model with Adam Kingma and Ba (2015) using a batch size of \\(2048\\)\n",
            "\n",
            "- Paper 3\n",
            "Title:# mT6: Multilingual Pretrained Text-to-Text Transformer\n",
            "Content: ...(Wenzek et al., 2019) in 94 languages for monolingual text-to-text tasks. For cross-lingual text-to-text tasks, we use parallel corpora of 14 English-centric language pairs, collected from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., Figure 3: Partially non-autoregressive objective. 2019). Details of the pre-training data are described in Appendix. Training DetailsIn the experiments, we consider the small-size Transformer model Xue et al. (2020), with \\(d_{\\text{model}}=512\\), \\(d_{\\text{ff}}=1,024\\), 6 attention heads, and \\(8\\) layers for both the encoder and the decoder1. We use the vocabulary provided by XLM-R Conneau et al. (2020), and extend it with \\(100\\) unique mask tokens for the span corruption tasks. We pretrain our MT6 for \\(0.5\\)M steps with batches of \\(256\\) length-\\(512\\) input sequences. The model is optimized by the Adam optimizer Kingma and Ba (2015) with a linear learning rate scheduler. The pre-training procedure takes\n",
            "\n",
            "- Paper 4\n",
            "Title:# Unsupervised Cross-lingual Representation Learning at Scale\n",
            "Content: ...tasks by jointly pretraining large Transformer models Vaswani et al. (2017) on many languages. These models allow for effective cross-lingual transfer, as seen in a number of benchmarks including cross-lingual natural language inference Bowman et al. (2015); Williams et al. (2017); Conneau et al. (2018), question answering Rajpurkar et al. (2016); Lewis et al. (2019), and named entity recognition Pires et al. (2019); Wu and Dredze (2019). However, all of these studies pre-train on Wikipedia, which provides a relatively limited scale especially for lower resource languages. In this paper, we first present a comprehensive analysis of the trade-offs and limitations of multilingual language models at scale, inspired by recent monolingual scaling efforts Liu et al. (2019). We measure the trade-off between high-resource and low-resource languages and the impact of language sampling and vocabulary size. The experiments expose a trade-off as we scale the number of languages for a fixed model\n",
            "\n",
            "- Paper 5\n",
            "Title:# IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation\n",
            "Content: ...a large and clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets called Indo4B-Plus, which is used to pretrain our NLG models, IndoBART and IndoGPT. We evaluate the effectiveness and efficiency of our models by conducting extensive automatic and human evaluations on the IndoNLG tasks. Based on the evaluation, our IndoBART and IndoGPT models achieve a competitive (albeit slightly lower) performance compared to the largest multilingual model in our benchmark, mBART\\({}_{\\text{LARGE}}\\), despite only using \\(\\sim\\)20% of the number of parameters, and an almost 4x and 2.5x faster inference time on a CPU and a GPU, respectively. To help with the reproducibility of the benchmark, we release the pretrained models, including the collected data and code. In order to accelerate community engagement and benchmark transparency, we have set up a leaderboard website for the NLP community. We publish all of our resources including IndoBART, IndoGPT, and IndoNLG tasks at [https://github.com/indobenchmark/indonlg](https://github.com/indobenchmark/indonlg). ##\n",
            "\n",
            "Paragraph Subject: T-PTLM Core Concepts::Pretraining::Pretraining Corpus\n",
            "DEBUG:debug:response from model The concept of pretraining text-to-text language models entails using large and often multilingual corpora of text data for the initial stages of training the model. The pretraining corpora ideally encompass diverse topics and languages in order to build a versatile model that is competent in handling a wide range of tasks and languages [1,2,3,5]. For instance, the foundational architecture of the Unified Text-to-Text Transformer (T5) was pretrained on a corpus that contains questions from the QNLI task and sentence pairs from the QQP task, among others [1]. \n",
            "\n",
            "On a similar note, InfoXLM, a framework grounded on information-theoretic principles for cross-lingual understanding, pretraining involves 94 languages for Masked Machine Language Modeling (MMLM), while translation pairs from MultiUN, IIT Bombay, OPUS, and WikiMatrix were used for the translation language modeling task (TLM) and the cross-lingual contrastive task (XLCo) [2]. Likewise, the mulitilingual pretraining of mT6 called for the use of a number of languages including English-centric pairs, and these varied corpora were compiled from MultiUN, IIT Bombay, OPUS, and WikiMatrix [3]. This trend of using large-scale and diverse text sources for pretraining is also evident in cross-lingual representation learning, where large Transformer models are pretrained on many languages, enhancing their proficiency in a myriad of languages and tasks [4]. \n",
            "\n",
            "In essence, the core corpus for pretraining not only influences the effectiveness and efficiency of a given model, but also defines its generalization capabilities to a considerable extent. This is best exemplified by the case of IndoNLG, where a specific pretraining corpus engineered for Indonesian, Sundanese, and Javanese datasets was used for IndoBART and IndoGPT models, making them a competitive option for the Indonesian natural language generation tasks, despite their comparatively faster inference time and lower parameter requirements [5]. This spotlight on the importance of the pretraining corpus is indicative of the larger emphasis on data diversity and relevance in transformer-based language model training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'content': 'The concept of pretraining text-to-text language models entails using large and often multilingual corpora of text data for the initial stages of training the model. The pretraining corpora ideally encompass diverse topics and languages in order to build a versatile model that is competent in handling a wide range of tasks and languages [1,2,3,5]. For instance, the foundational architecture of the Unified Text-to-Text Transformer (T5) was pretrained on a corpus that contains questions from the QNLI task and sentence pairs from the QQP task, among others [1]. \\n\\nOn a similar note, InfoXLM, a framework grounded on information-theoretic principles for cross-lingual understanding, pretraining involves 94 languages for Masked Machine Language Modeling (MMLM), while translation pairs from MultiUN, IIT Bombay, OPUS, and WikiMatrix were used for the translation language modeling task (TLM) and the cross-lingual contrastive task (XLCo) [2]. Likewise, the mulitilingual pretraining of mT6 called for the use of a number of languages including English-centric pairs, and these varied corpora were compiled from MultiUN, IIT Bombay, OPUS, and WikiMatrix [3]. This trend of using large-scale and diverse text sources for pretraining is also evident in cross-lingual representation learning, where large Transformer models are pretrained on many languages, enhancing their proficiency in a myriad of languages and tasks [4]. \\n\\nIn essence, the core corpus for pretraining not only influences the effectiveness and efficiency of a given model, but also defines its generalization capabilities to a considerable extent. This is best exemplified by the case of IndoNLG, where a specific pretraining corpus engineered for Indonesian, Sundanese, and Javanese datasets was used for IndoBART and IndoGPT models, making them a competitive option for the Indonesian natural language generation tasks, despite their comparatively faster inference time and lower parameter requirements [5]. This spotlight on the importance of the pretraining corpus is indicative of the larger emphasis on data diversity and relevance in transformer-based language model training.', 'references': [], 'title': 'T-PTLM Core Concepts::Pretraining::Pretraining Corpus'}]\n"
          ]
        }
      ],
      "source": [
        "from AutoSurvey.llm_inference.openai_inference import OpenAIInference\n",
        "from AutoSurvey.searchers.semantic_scholar import SemanticScholarSearcher\n",
        "from AutoSurvey.filters.field_in_list_filter import FieldInListFilter\n",
        "from AutoSurvey.filters.field_numeric_filter import FieldNumericFilter\n",
        "from AutoSurvey.pdf_generation.pdf_generator import PDFGenerator\n",
        "from AutoSurvey.llm_inference.query_augmentor import QueryAugmentor\n",
        "from AutoSurvey.ranker import MonoT5\n",
        "# from AutoSurvey.pdf_extraction.pdf_extractor import get_pdf_windows\n",
        "from tqdm.notebook import tqdm\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "paper_template=\"\"\"- Paper {I}\n",
        "Title:{TITLE}\n",
        "Content: {CONTENT}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "template_messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You Are an expert in scientific literature review, your job is, given a series of papers and theirs summaries, write a paragraph with a given title citing relevant information in the traditional format (e.g [1,2,3] [1]) from the provided papers, you must write as much as you can, and must write at least 2 paragraphs\"},\n",
        "    {\"role\": \"user\", \"content\": \"Paper 1\\nTitle:Evaluating GPT-3.5 and GPT-4 Models on Brazilian - University Admission Exams\\nSummary: he present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino Médio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on experiments are available at https://github.com/piresramon/gpt-4-enem \\n\\n- Paper 2\\nTitle:BLUEX: a benchmark based on Brazilian Leading Universities Entrance eXams\\nSummary: o enable future comparisons, we evaluated our dataset using several language models, ranging from 6B to 66B parameters, as well as the OpenAI’s GPT-4 and GPT-3.5-Turbo models. Our experiments focused on a simple scenario where the model had to answer a question in a 1-shot setting, based on a random example from an exam of the same university but from a different year. We excluded all questions containing images since the language models we used can only process text. This resulted in a total of 638 questions being used, which corresponds to approximately 60% of the dataset. Table 3 summarizes our experimental findings, including the mean score achieved by exam-taking students, as well as the mean cutoff score of the most competitive major, which is medicine in both universities.1 The BLUEX column shows the accuracy of the whole subset used in the evaluation, while the UNICAMP and USP columns account for only the questions from the respective universities. The MR and BK columns account only for questions that include those categories. Among the language models tested in the 7B-parameter range, Sabi ́a [17], a model further pre- trained in Portuguese, consistently outperformed all other models, coming close to matching the average human score. Among the open-source models in the 60B-parameter range, LLaMA 65B [25] significantly outperformed OPT 66B [30] and achieved similar performance to GPT-3.5-Turbo. Sabi ́a 65B achieved better performance than GPT-3.5-Turbo but still lagged behind GPT-4 by ten points. GPT-4 was by far the best model in our evaluations but did not achieve an average score high enough to pass in medicine, the most competitive major. It is worth noting that the average and cutoff scores provided in Table 3 are computed taking into account the whole exam, including questions with images, while the scores obtained by the language models utilize only the subset of questions with no images. \\n\\n- Paper 3\\nTitle:Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential - Benefits of ChatGPT in Promoting Teaching and Learning\\nSummary: Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators as this advancement in AI seems to revolutionize existing educational praxis. This review article synthesizes recent extant literature to offer some potential benefits of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students’ learning.\\n\\nParagraph Subject: The capacity and impact of ChatGPT in brazillian education\"},\n",
        "    {\"role\": \"system\", \"content\": \"Since it's introduction, ChatGPT proved that it was capable of handling numerous tasks, [1,2] showed that GPT4 in particular is quite skilled at solving some of the most challeging high school level standarized tests in Brazil, namely ENEM [1] ,Unicamp and USP [2]. Since GPT4 is able to answer such high level tests, it is reasonable to ask the impact it could have in education. Numerous discussions exist regarding if the use of IA such as gpt3.5 and gpt4 is beneficial or harmful to students and teeachers, in particular [3] lists some benefits such as personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning, but also lists some limitations such as generating wrong information, biases in data training which may augment existing biases, privacy issues etc.\"}\n",
        "]\n",
        "\n",
        "survey_title=\"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing\"\n",
        "out_path=\"/content/test.json\"\n",
        "reranker=True\n",
        "reranker_model=\"castorini/monot5-3b-msmarco-10k\"\n",
        "# reranker_model=\"castorini/monot5-large-msmarco\"\n",
        "# reranker_model=\"castorini/monot5-base-msmarco-10k\"\n",
        "number_of_docs_to_gpt=5\n",
        "\n",
        "#create debug logger that writes to file\n",
        "debug_logger = logging.getLogger('debug')\n",
        "debug_logger.setLevel(logging.DEBUG)\n",
        "fh = logging.FileHandler('debug.log')\n",
        "fh.setLevel(logging.DEBUG)\n",
        "debug_logger.addHandler(fh)\n",
        "\n",
        "\n",
        "searcher=SemanticScholarSearcher()\n",
        "\n",
        "agent=OpenAIInference(api_key=\"\", engine=\"gpt-3.5\")\n",
        "agent_augment=OpenAIInference(api_key=\"\", engine=\"gpt-4\")\n",
        "\n",
        "\n",
        "query_augmentor=QueryAugmentor(agent=agent_augment)\n",
        "\n",
        "def search_one_query(query, filters=None, rerank=False, model=None):\n",
        "  params={\n",
        "      \"query\": query,\n",
        "      \"limit\": 50, # max 100\n",
        "      \"fields\": \"title,authors,citationCount,year,tldr,url,abstract,influentialCitationCount,citationStyles\"\n",
        "  }\n",
        "\n",
        "  # compose as many filters as you like, they are applied in sequence\n",
        "\n",
        "  if filters is None:\n",
        "    filters=[\n",
        "        FieldNumericFilter(\"citationCount\", lower_bound=15), # at least 10 citations\n",
        "        #FieldNumericFilter(\"authors\", lower_bound=2), # at least 2  authors\n",
        "        #FieldNumericFilter(\"year\", lower_bound=2020, upper_bound=2023), # between 2020 and 2023\n",
        "    ]\n",
        "\n",
        "  docs = searcher.search(params, filters=filters)\n",
        "  if len(docs) == 0:\n",
        "    debug_logger.debug(f\"query: {query} returned 0 results with filters {filters}\")\n",
        "    return []\n",
        "  augmented_docs = []\n",
        "  tot=0\n",
        "  for doc in docs:\n",
        "    if doc[\"citationStyles\"] and doc[\"citationStyles\"].get(\"bibtex\"):\n",
        "      ref=doc[\"citationStyles\"][\"bibtex\"]\n",
        "    else:\n",
        "       ref=\"\"\n",
        "    if rerank:\n",
        "      pdf = searcher.download_pdf(doc[\"paperId\"])\n",
        "      if pdf:\n",
        "        tot+=1\n",
        "        temp_file = tempfile.NamedTemporaryFile()\n",
        "        temp_file_path = temp_file.name\n",
        "        temp_file.write(pdf)\n",
        "        windows = get_pdf_windows(temp_file_path)\n",
        "        temp_file.close()\n",
        "\n",
        "        augmented_docs.extend([{\"title\": doc[\"title\"], \"content\": window, \"reference\": ref} for window in windows])\n",
        "        # debug_logger.debug(f\"extracted {len(windows)} windows from pdf {doc['title']}\\n\")\n",
        "        continue\n",
        "    if doc[\"abstract\"]:\n",
        "      augmented_docs.append({\"title\": doc[\"title\"], \"content\": doc[\"abstract\"], \"reference\": ref})\n",
        "    elif doc[\"tldr\"] and doc[\"tldr\"].get(\"text\"):\n",
        "      augmented_docs.append({\"title\": doc[\"title\"], \"content\": doc[\"tldr\"][\"text\"], \"reference\": ref})\n",
        "    else:\n",
        "      continue\n",
        "  print(\"number of documents with pdf extracted\", tot)\n",
        "\n",
        "  debug_logger.debug(f\"query: {query} returned {len(docs)} results with filters {filters}\")\n",
        "\n",
        "  return augmented_docs\n",
        "\n",
        "\n",
        "def rerank_documents(query,documents,model, survey_title=None):\n",
        "  query_for_rerank=query\n",
        "  if survey_title:\n",
        "    query_for_rerank=survey_title + \", \" + query\n",
        "  debug_logger.debug(f\"reranking documents with query {query_for_rerank}\")\n",
        "  scores = model.rescore(query_for_rerank, [doc[\"content\"] for doc in documents])\n",
        "  assert len(scores) == len(documents), \"The number of scores should be the same as the number of documents\"\n",
        "  final_results = [x for _, x in sorted(zip(scores, documents), key=lambda x: x[0], reverse=True)]\n",
        "\n",
        "  return final_results\n",
        "\n",
        "def query_to_documents(query, survey_title=None, filters=None, augment_query=True, rerank=False, model=None):\n",
        "\n",
        "  query_for_search=query\n",
        "  if survey_title:\n",
        "    query_for_search=survey_title + \" \" + query\n",
        "  if augment_query:\n",
        "    queries= query_augmentor.augment_queries(query_for_search)\n",
        "    queries.append(query_for_search)\n",
        "    queries=queries[-3:]\n",
        "    print(\"augmentation resulted in the following queries\", queries)\n",
        "    debug_logger.debug(f\"augmentation resulted in the following queries {queries}\")\n",
        "\n",
        "    results=[]\n",
        "    for query in queries:\n",
        "      results.extend(search_one_query(query, filters=filters, rerank=rerank, model=model))\n",
        "\n",
        "\n",
        "\n",
        "  else:\n",
        "    results=search_one_query(query, filters=filters, rerank=rerank, model=model)\n",
        "\n",
        "  if rerank:\n",
        "    final_results=rerank_documents(query,results,model, survey_title=survey_title)\n",
        "\n",
        "  return final_results\n",
        "\n",
        "\n",
        "def documents_to_section(results, query):\n",
        "\n",
        "  user_prompt=\"\"\n",
        "  current_paper_number=0\n",
        "  used_refs=[]\n",
        "  used_papers=set()\n",
        "  for i,result in enumerate(results):\n",
        "\n",
        "    if current_paper_number>=number_of_docs_to_gpt:\n",
        "      break\n",
        "\n",
        "    if result[\"title\"] in used_papers:\n",
        "       continue\n",
        "    current_paper_number+=1\n",
        "\n",
        "    paper_text=paper_template.format(I=current_paper_number, CONTENT=result[\"content\"], TITLE=result[\"title\"])\n",
        "    user_prompt+=paper_text + \"\\n\"\n",
        "    used_papers.add(result[\"title\"])\n",
        "\n",
        "    if result[\"reference\"]:\n",
        "      used_refs.append(result[\"reference\"])\n",
        "\n",
        "  user_prompt+= \"Paragraph Subject: \" + query\n",
        "\n",
        "  current_msgs=template_messages.copy()\n",
        "\n",
        "  current_msgs.append({\n",
        "      \"role\": \"user\",\n",
        "      \"content\": user_prompt\n",
        "  })\n",
        "\n",
        "  debug_logger.debug(f\"prompt for model {user_prompt}\")\n",
        "  response=agent.complete(current_msgs)\n",
        "  debug_logger.debug(f\"response from model {response}\", )\n",
        "\n",
        "  return response, used_refs\n",
        "\n",
        "\n",
        "# python -m test.full_test --survey_data C:\\Users\\Thiago\\Documents\\projeto_final_ia368\\AutoSurvey\\data\\dataset\\survey_1.json --out_path proc5/proc5.json\n",
        "\n",
        "# /content/AutoSurvey/data/dataset/survey_3.json\n",
        "\n",
        "\n",
        "def write_sections(survey_title, sections, provided_docs, search=True):\n",
        "\n",
        "    print(sections)\n",
        "\n",
        "    sections_data=[]\n",
        "\n",
        "    debug_logger.debug(f\"survey title -> {survey_title}, sections -> {sections}\")\n",
        "\n",
        "    model = None\n",
        "    if reranker:\n",
        "      model = MonoT5(reranker_model, fp16=True)\n",
        "\n",
        "    for section in tqdm(sections, desc=\"Processing sections\"):\n",
        "      if search:\n",
        "        documents=query_to_documents(section, survey_title=survey_title,filters=None, rerank=reranker, model=model)\n",
        "      else:\n",
        "        documents=provided_docs\n",
        "        windowed_documents=[]\n",
        "        for doc_id,doc in enumerate(documents):\n",
        "          windows=get_windows(doc[\"content\"])\n",
        "          for i,window in enumerate(windows):\n",
        "            # giving unique title to prevent deduplication for windows of same paper\n",
        "            windowed_documents.append({\n",
        "                \"content\": window,\n",
        "                \"title\": doc[\"title\"],\n",
        "                \"reference\": doc[\"reference\"]\n",
        "                })\n",
        "\n",
        "        documents=rerank_documents(section,windowed_documents, model, survey_title=survey_title)\n",
        "\n",
        "      if len(documents)==0:\n",
        "          print(\"skipping section, no relevant documents found for query\", section)\n",
        "          continue\n",
        "\n",
        "      if not reranker:\n",
        "          documents=sorted(documents, key=lambda x: x[\"influentialCitationCount\"], reverse=True)\n",
        "\n",
        "\n",
        "      section_text, used_refs=documents_to_section(documents, section)\n",
        "      sections_data.append({\n",
        "          \"content\": section_text,\n",
        "          \"references\": used_refs,\n",
        "          \"title\": section,\n",
        "      })\n",
        "      print(sections_data)\n",
        "\n",
        "\n",
        "    # with open(out_path, \"w\") as f:\n",
        "    #   json.dump(sections_data, f, indent=4)\n",
        "\n",
        "    return sections_data\n",
        "\n",
        "\n",
        "if not os.path.exists(out_path):\n",
        "  from pathlib import Path\n",
        "  os.makedirs(Path(out_path).parent, exist_ok=True)\n",
        "\n",
        "all_survey_data={\n",
        "    survey_title: {}\n",
        "}\n",
        "\n",
        "for section in docs_for_sections:\n",
        "  provided_docs=docs_for_sections[section]\n",
        "  section_content=write_sections(survey_title, [section], provided_docs, search=False)[0]\n",
        "\n",
        "  if section not in all_survey_data[survey_title]:\n",
        "    all_survey_data[survey_title][section]={}\n",
        "  all_survey_data[survey_title][section][\"content\"]= section_content[\"content\"]\n",
        "  all_survey_data[survey_title][section][\"references\"]= section_content[\"references\"]\n",
        "\n",
        "with open(out_path, \"w\") as f:\n",
        "  json.dump(all_survey_data, f, indent=4)\n",
        "\n",
        "# pdf_path=args.out_path.replace(\".json\", \".pdf\")\n",
        "# PDFGenerator.generate_pdf(survey_title, sections_data, output_file=pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en06-0KJhcTI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeQCFjhRhcCq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9dUB2rrfVJO"
      },
      "outputs": [],
      "source": [
        "from AutoSurvey.searchers.semantic_scholar import SemanticScholarSearcher\n",
        "from AutoSurvey.filters.field_in_list_filter import FieldInListFilter\n",
        "from AutoSurvey.filters.field_numeric_filter import FieldNumericFilter\n",
        "import json\n",
        "\n",
        "searcher=SemanticScholarSearcher()\n",
        "\n",
        "\n",
        "\n",
        "def query_to_documents(query):\n",
        "  # search for papers with \"deep learning\" in the title\n",
        "  params={\n",
        "      \"query\": query,\n",
        "      \"limit\": 100, # max 100\n",
        "      \"fields\": \"title,authors,citationCount,year,tldr,url,abstract\"\n",
        "  }\n",
        "\n",
        "  # compose as many filters as you like, they are applied in sequence\n",
        "  filters=[\n",
        "      FieldNumericFilter(\"citationCount\", lower_bound=2), # at least 10 citations\n",
        "      FieldNumericFilter(\"authors\", lower_bound=2), # at least 2  authors\n",
        "      FieldNumericFilter(\"year\", lower_bound=2020, upper_bound=2023), # between 2020 and 2023\n",
        "  ]\n",
        "\n",
        "  results=searcher.search(params, filters=filters)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ype14d13ff28"
      },
      "outputs": [],
      "source": [
        "from AutoSurvey.llm_inference.openai_inference import OpenAIInference\n",
        "\n",
        "\n",
        "paper_template=\"\"\"- Paper {I}\n",
        "Title:{TITLE}\n",
        "Summary: {SUMMARY}\n",
        "\"\"\"\n",
        "\n",
        "template_messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You Are an expert in scientific literature review, your job is, given a series of papers and theirs summaries, write a paragraph with a given title citing relevant information in the traditional format (e.g [1,2,3] [1]) from the provided papers\"},\n",
        "]\n",
        "\n",
        "agent=OpenAIInference(api_key=\"\", engine=\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "def documents_to_section(results, query):\n",
        "\n",
        "  user_prompt=\"\"\n",
        "  current_paper_number=0\n",
        "  for i,result in enumerate(results):\n",
        "\n",
        "    if result[\"abstract\"]:\n",
        "      paper_summary=result[\"abstract\"]\n",
        "\n",
        "    elif result[\"tldr\"]:\n",
        "      paper_summary=result[\"tldr\"][\"text\"]\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "    current_paper_number+=1\n",
        "\n",
        "    if current_paper_number>=6:\n",
        "      break\n",
        "\n",
        "\n",
        "    paper_text=paper_template.format(I=i+1,SUMMARY=paper_summary, TITLE=result[\"title\"])\n",
        "    user_prompt+=paper_text + \"\\n\"\n",
        "\n",
        "  user_prompt+= \"Paragraph Subject: \" + query\n",
        "\n",
        "  current_msgs=template_messages.copy()\n",
        "\n",
        "  current_msgs.append({\n",
        "      \"role\": \"user\",\n",
        "      \"content\": user_prompt\n",
        "  })\n",
        "\n",
        "  return agent.complete(current_msgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKn8lrkojp12",
        "outputId": "6a86ebbc-9e7f-41bd-b936-4103f70be695"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [01:36<00:00, 24.18s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "theme=\"deep learning\"\n",
        "sections=[\n",
        "    \"applications\",\n",
        "    \"latest developments\",\n",
        "    \"neural networks\",\n",
        "    \"large language models\"\n",
        "]\n",
        "\n",
        "sections_data={}\n",
        "\n",
        "for section in tqdm(sections):\n",
        "  query=theme + \" - \"+ section\n",
        "  documents=query_to_documents(query)\n",
        "  section_text=documents_to_section(documents, query)\n",
        "  sections_data[section]=section_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVpXDlrxnCkH",
        "outputId": "97843807-a89e-48b4-8a20-418ea32140b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deep learning\n",
            "\n",
            "applications\n",
            "\n",
            "Deep learning has been utilized in various domains, including COVID-19 research, medical image analysis, genomic selection, and structural damage detection. In their survey of Deep Learning applications for COVID-19, the authors explore how Deep Learning has battled the pandemic and provide directions for future research with a critical evaluation of the current state of Deep Learning, which concludes with key limitations for COVID-19 applications. Research in artificial intelligence for medical imaging has relied more on deep learning-based algorithms, as revealed by a review of medical image data augmentation techniques. Data augmentation has become a popular method for increasing the size of a training dataset, and this approach has become commonplace in medical images, particularly in fields where large datasets aren't typically available. Another area where deep learning has shown great promise is in genomic selection. Research has shown that deep learning algorithms capture nonlinear patterns more efficiently than conventional genome-based prediction models, making it possible to improve prediction accuracy for large plant breeding data. Furthermore, machine learning and deep learning have been utilized in various domains, as described in a review survey that showcases some applications and outlines the new normal COVID-19 blues. Finally, in a comprehensive review of vibration-based structural damage detection in civil structures, the traditional methods are highlighted, while the most recent applications of ML and DL algorithms utilized in identifying the damage are outlined. In conclusion, Deep learning shows promise in various fields, including COVID-19 research, medical image analysis, genomic selection, and structural damage detection.\n",
            "\n",
            "latest developments\n",
            "\n",
            "Deep learning has been a significant breakthrough in various fields of study, including community detection, coding, forecasting, and epidemiology. The first paper presents a comprehensive survey on community detection with deep learning techniques. It highlights state-of-the-art methods using deep neural networks, deep non-negative matrix factorization, and deep sparse filtering. In the second paper, an adaptive deep learning-based point cloud geometry coding is presented, which efficiently adapts to the content's characteristics. The third paper explores the use of machine learning and deep learning algorithms for forecasting and analyzing COVID-19 data, highlighting CNN and SVM as the most used algorithms. In the fourth paper, the authors present a survey of device-free wireless sensing for human detection using deep learning, which is relatively new in this field. Finally, the fifth paper proposes an automated deep learning model for cardiovascular disease diagnosis using ECG signals, which shows promising results in terms of accuracy, sensitivity, specificity, kappa, Mathew correlation coefficient, and Hamming loss. These papers demonstrate how deep learning is advancing different areas of research and suggest promising future research directions.\n",
            "\n",
            "neural networks\n",
            "\n",
            "The use of deep learning neural networks in medical research, specifically in skin disease classification, has shown promising results. One study proposed a computerized process using MobileNet V2 and Long Short Term Memory (LSTM) models, which proved to be efficient with a high accuracy of more than 85% on the HAM10000 dataset. The proposed model can work on lightweight computational devices, and its robustness in recognizing the affected region much faster with almost 2× lesser computations than the conventional MobileNet model results in minimal computational efforts. Moreover, a mobile application is designed for instant and proper action, which helps the patient and dermatologists to identify the type of disease from the affected region's image at the initial stage of the skin disease. However, deep learning models' failures are proposed to be a consequence of shortcut learning, a common characteristic across biological and artificial systems in which strategies that appear to have solved a problem fail unexpectedly under different circumstances. Some researchers have worked towards mitigating the growing energy and performance costs of deep learning, and sparsity is proposed as one such solution to reduce the memory footprint of regular networks to fit mobile devices as well as shorten training time for ever-growing networks. Several approaches to achieve model sparsity, including the removal and addition of elements of neural networks and different training strategies, are investigated. Therefore, while deep learning neural networks have the potential to revolutionize medical research, the challenges and limitations of these methods should also be considered.\n",
            "\n",
            "large language models\n",
            "\n",
            "Large language models (LLMs) are a promising approach for improving various deep learning (DL) tasks. The use of LLMs has been explored to address challenges in different DL domains such as DL libraries and NLP tasks. In the first paper, TitanFuzz is proposed to leverage LLMs to automatically generate diverse input programs for fuzzing DL libraries. Similarly, in the second paper, LLMFuzz is proposed to address the limitations of existing fuzzing or testing techniques for DL libraries. These approaches demonstrated improved coverage and bug detection capabilities in popular DL libraries such as TensorFlow and PyTorch. Another application of LLMs in DL is in predicting protein structures and their functions, as shown in NetSurfP-3.0. This update exploits recent advances in pre-trained protein language models to significantly reduce the runtime while maintaining similar prediction performance. The use of LLMs is not limited to DL libraries and protein structure prediction but also extends to the few-shot classification of tabular data, as demonstrated in TabLLM. These studies highlight the potential of LLMs as a generalizable tool for improving DL in various domains.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def to_basic_html(theme, sections, sections_data):\n",
        "  html=\"<html>\\n\"\n",
        "  html+=f\"<h1>{theme}</h1>\"\n",
        "\n",
        "  for section in sections:\n",
        "    html+=f\"<h2>{section}</h2>\\n\"\n",
        "\n",
        "    html+=f\"<p>{sections_data[section]}</p>\\n\"\n",
        "\n",
        "  html+=\"</html>\"\n",
        "  return html\n",
        "\n",
        "print(theme)\n",
        "print()\n",
        "\n",
        "for section in sections:\n",
        "  print(section)\n",
        "  print()\n",
        "  print(sections_data[section])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OZg_0bcqWsi",
        "outputId": "4982ad15-9d33-44f8-8441-bec616f6bc2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting py3-wkhtmltopdf==0.4.1\n",
            "  Downloading py3_wkhtmltopdf-0.4.1-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: py3-wkhtmltopdf\n",
            "Successfully installed py3-wkhtmltopdf-0.4.1\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Va_eKPgswL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y131Es4-qhE1",
        "outputId": "f9ba0009-06a2-42b2-ce04-05cf6d540e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfkit\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: pdfkit\n",
            "Successfully installed pdfkit-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2) (2.1.2)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6WnQ8n2rWuC"
      },
      "outputs": [],
      "source": [
        "html=to_basic_html(theme, sections, sections_data)\n",
        "with open(\"file.html\", \"w\") as fout:\n",
        "  fout.write(html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vELt2Xjqnor",
        "outputId": "ce4d16a6-94e8-4b04-cdff-5636cd1de7aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pdfkit\n",
        "options = {\n",
        "    'page-size': 'A4',\n",
        "    'margin-top': '0.75in',\n",
        "    'margin-right': '0.75in',\n",
        "    'margin-bottom': '0.75in',\n",
        "    'margin-left': '0.75in',\n",
        "    'encoding': \"UTF-8\",\n",
        "    'custom-header': [\n",
        "        ('Accept-Encoding', 'gzip')\n",
        "    ],\n",
        "    'cookie': [\n",
        "        ('cookie-empty-value', '\"\"'),\n",
        "        ('cookie-name1', 'cookie-value1'),\n",
        "        ('cookie-name2', 'cookie-value2'),\n",
        "    ],\n",
        "    'no-outline': None\n",
        "}\n",
        "# calling the from file method to convert file to pdf\n",
        "pdfkit.from_file('file.html', 'file.pdf', options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUk-5Ge8tDhI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0523ff41d58442c6bcca3f0303bffa8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0614480e7dd94d57b2711364d1f85221": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a7f22795dcb407a8cfd905088e9918a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99bcce740b7c4802bb9c958a34037f94",
            "placeholder": "​",
            "style": "IPY_MODEL_2bf8b7fe88d34d3f9450c0d720ba7052",
            "value": "Processing sections: 100%"
          }
        },
        "11fbc62221b4444a9b8480de19a9cc74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17baaae9832b40ffba8a73e8a397ab2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11fbc62221b4444a9b8480de19a9cc74",
            "placeholder": "​",
            "style": "IPY_MODEL_32091e57e4cd4e1d96e843dab603744f",
            "value": " 1/1 [00:28&lt;00:00, 28.59s/it]"
          }
        },
        "2625ee949f9c457cb0af24a6f56e1c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4458ed8acb7d4d0cb474f093b812987c",
            "placeholder": "​",
            "style": "IPY_MODEL_ebee447b6b8b457d870e64e6e1d4bf39",
            "value": "Processing sections: 100%"
          }
        },
        "284c827ba540424dbe5420468beb4ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d6484ba330c4983a40717df596ca713",
            "placeholder": "​",
            "style": "IPY_MODEL_466c92185989458188ca346d5e962c4f",
            "value": " 1/1 [00:39&lt;00:00, 39.04s/it]"
          }
        },
        "2bf8b7fe88d34d3f9450c0d720ba7052": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32091e57e4cd4e1d96e843dab603744f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3438f15a5e2b412e8fca22116e18edb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c7495f901fa48a7a1270f19f42be7e3",
            "placeholder": "​",
            "style": "IPY_MODEL_89a625de44f84ec78538a78c62c46a21",
            "value": " 1/1 [00:40&lt;00:00, 40.98s/it]"
          }
        },
        "3c6ebaf2b61d4f78b06e38d9ec608e25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c7495f901fa48a7a1270f19f42be7e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4458ed8acb7d4d0cb474f093b812987c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466c92185989458188ca346d5e962c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48597d09b6a44497a83d7ae8fba40227": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b47688e79d645b882aaa5ab2683f4f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0614480e7dd94d57b2711364d1f85221",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f91ca517ee0548c9a96d03ce7a1cb1e0",
            "value": 1
          }
        },
        "4d8b181eefc143529284a31307dab39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52e3e56762424d96b4874a41e3ded68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "539e3b5db2f044198641dfb0ba5d0e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55a30a6889274d839505c784dbcbed4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3a738a0b9df41499763bbad9c270a14",
              "IPY_MODEL_4b47688e79d645b882aaa5ab2683f4f4",
              "IPY_MODEL_ca9f120ced8b4f9d81ea8074f39454e2"
            ],
            "layout": "IPY_MODEL_af5012a208274397857a15b45cb04b7f"
          }
        },
        "58df4349a7154f6c953f80a573698e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fc1e69e67094d45a96630e91720ad0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7207f1c87af4b4288501f9f1055d85d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48597d09b6a44497a83d7ae8fba40227",
            "value": 1
          }
        },
        "6cd109365d73464da7c8fe0a56db762b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d6484ba330c4983a40717df596ca713": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "709aa5586c0743e9bf50ff5ce4a9febb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7189d62627464e399f2271fcd51eb2fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "748934279bf84046a66e4d86568f0f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a7f22795dcb407a8cfd905088e9918a",
              "IPY_MODEL_dba4e9f697eb44cd993eb52ab8850fc8",
              "IPY_MODEL_284c827ba540424dbe5420468beb4ecf"
            ],
            "layout": "IPY_MODEL_58df4349a7154f6c953f80a573698e10"
          }
        },
        "8765610536dc44589b4ffab2aa3b13aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89a625de44f84ec78538a78c62c46a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92e3b5e939b54f5ca3c99d9a63caef4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93a32dc979994e98b12dd817b6d4ed27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "986c91072b7b43fa8cfa9acbd45cb0ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99bcce740b7c4802bb9c958a34037f94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a01196086dde4c1d9a122a58cd2b59d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f303a4a9a1a8489d9960baf170260911",
            "placeholder": "​",
            "style": "IPY_MODEL_52e3e56762424d96b4874a41e3ded68e",
            "value": "Processing sections: 100%"
          }
        },
        "a56fba857f7947dc8d40b82698197b79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae55f98eed9f4ff788be06d7060e0a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5012a208274397857a15b45cb04b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afefcdd7013f45b9a5b83e8d5719970e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae55f98eed9f4ff788be06d7060e0a8d",
            "placeholder": "​",
            "style": "IPY_MODEL_0523ff41d58442c6bcca3f0303bffa8f",
            "value": " 1/1 [00:35&lt;00:00, 35.91s/it]"
          }
        },
        "b680002fd1a54391a4a2dbb18b0f02a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c026b4938e4d47b7a91d7a3e5d4e81b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4318244710f4ae18059654fe1199e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca9f120ced8b4f9d81ea8074f39454e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed4117e342a042e9a34b445937ac57bf",
            "placeholder": "​",
            "style": "IPY_MODEL_93a32dc979994e98b12dd817b6d4ed27",
            "value": " 1/1 [00:26&lt;00:00, 26.64s/it]"
          }
        },
        "d7207f1c87af4b4288501f9f1055d85d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7fbe5f03a3d4f7e8f78ee1e5f4b3b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c026b4938e4d47b7a91d7a3e5d4e81b3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_709aa5586c0743e9bf50ff5ce4a9febb",
            "value": 1
          }
        },
        "d93cf38c290844019a778c6ecb47a4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2625ee949f9c457cb0af24a6f56e1c60",
              "IPY_MODEL_d7fbe5f03a3d4f7e8f78ee1e5f4b3b60",
              "IPY_MODEL_3438f15a5e2b412e8fca22116e18edb9"
            ],
            "layout": "IPY_MODEL_7189d62627464e399f2271fcd51eb2fd"
          }
        },
        "dba4e9f697eb44cd993eb52ab8850fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8765610536dc44589b4ffab2aa3b13aa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_539e3b5db2f044198641dfb0ba5d0e35",
            "value": 1
          }
        },
        "dbb968b38049414b82c9004e8ef7d928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a01196086dde4c1d9a122a58cd2b59d2",
              "IPY_MODEL_5fc1e69e67094d45a96630e91720ad0f",
              "IPY_MODEL_17baaae9832b40ffba8a73e8a397ab2c"
            ],
            "layout": "IPY_MODEL_a56fba857f7947dc8d40b82698197b79"
          }
        },
        "e32858f32cdb43389afa93cc9d6d910e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cd109365d73464da7c8fe0a56db762b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92e3b5e939b54f5ca3c99d9a63caef4d",
            "value": 1
          }
        },
        "e3a738a0b9df41499763bbad9c270a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b680002fd1a54391a4a2dbb18b0f02a6",
            "placeholder": "​",
            "style": "IPY_MODEL_c4318244710f4ae18059654fe1199e61",
            "value": "Processing sections: 100%"
          }
        },
        "e911e48424e34ef3aae1d9ca3c9ee42e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c6ebaf2b61d4f78b06e38d9ec608e25",
            "placeholder": "​",
            "style": "IPY_MODEL_4d8b181eefc143529284a31307dab39b",
            "value": "Processing sections: 100%"
          }
        },
        "ebee447b6b8b457d870e64e6e1d4bf39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed4117e342a042e9a34b445937ac57bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee567386d95d4994ad64feea31bb57e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e911e48424e34ef3aae1d9ca3c9ee42e",
              "IPY_MODEL_e32858f32cdb43389afa93cc9d6d910e",
              "IPY_MODEL_afefcdd7013f45b9a5b83e8d5719970e"
            ],
            "layout": "IPY_MODEL_986c91072b7b43fa8cfa9acbd45cb0ee"
          }
        },
        "f303a4a9a1a8489d9960baf170260911": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91ca517ee0548c9a96d03ce7a1cb1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
